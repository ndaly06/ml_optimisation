dataset,extradata,global_rank,metric_name,metric_value,model,remove,task,paper_title,paper_path,paper_url
IC15,,# 10,F-Measure,75.61%,SegLink,-,Scene Text Detection,Detecting Oriented Text in Natural Images by Linking Segments,/paper/detecting-oriented-text-in-natural-images-by,https://arxiv.org/pdf/1703.06520v3.pdf
SCUT-CTW1500,,# 5,F-Measure,40.8%,SegLink,-,Curved Text Detection,Detecting Oriented Text in Natural Images by Linking Segments,/paper/detecting-oriented-text-in-natural-images-by,https://arxiv.org/pdf/1703.06520v3.pdf
SNLI,,# 36,% Test Accuracy,84.6,300D NSE encoders,-,Natural Language Inference,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
SNLI,,# 44,% Train Accuracy,86.2,300D NSE encoders,-,Natural Language Inference,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
SNLI,,# 1,Parameters,3.0m,300D NSE encoders,-,Natural Language Inference,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
SNLI,,# 33,% Test Accuracy,85.4,300D MMA-NSE encoders with attention,-,Natural Language Inference,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
SNLI,,# 41,% Train Accuracy,86.9,300D MMA-NSE encoders with attention,-,Natural Language Inference,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
SNLI,,# 1,Parameters,3.2m,300D MMA-NSE encoders with attention,-,Natural Language Inference,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
SST-2 Binary classification,,# 10,Accuracy,89.7,Neural Semantic Encoder,-,Sentiment Analysis,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
WikiQA,,# 8,MAP,0.6811,MMA-NSE attention,-,Question Answering,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
WikiQA,,# 7,MRR,0.6993,MMA-NSE attention,-,Question Answering,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
WMT2014 English-German,,# 21,BLEU score,17.93,NSE-NSE,-,Machine Translation,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://arxiv.org/pdf/1607.04315v3.pdf
VQA v2,,# 2,Accuracy,70.24%,Pythia v0.1,-,Visual Question Answering,Pythia v0.1: the Winning Entry to the VQA Challenge 2018,/paper/pythia-v01-the-winning-entry-to-the-vqa,https://arxiv.org/pdf/1807.09956v2.pdf
ImageNet 128x128,,# 3,FID,9.6,BigGAN,-,Conditional Image Generation,Large Scale GAN Training for High Fidelity Natural Image Synthesis,/paper/large-scale-gan-training-for-high-fidelity,https://arxiv.org/pdf/1809.11096v2.pdf
ImageNet 128x128,,# 2,Inception score,166.3,BigGAN,-,Conditional Image Generation,Large Scale GAN Training for High Fidelity Natural Image Synthesis,/paper/large-scale-gan-training-for-high-fidelity,https://arxiv.org/pdf/1809.11096v2.pdf
ImageNet 128x128,,# 1,FID,7.4,BigGAN-deep,-,Conditional Image Generation,Large Scale GAN Training for High Fidelity Natural Image Synthesis,/paper/large-scale-gan-training-for-high-fidelity,https://arxiv.org/pdf/1809.11096v2.pdf
ImageNet 128x128,,# 1,Inception score,166.5,BigGAN-deep,-,Conditional Image Generation,Large Scale GAN Training for High Fidelity Natural Image Synthesis,/paper/large-scale-gan-training-for-high-fidelity,https://arxiv.org/pdf/1809.11096v2.pdf
BUCC French-to-English,,# 2,F1 score,92.89,Multilingual Sentence Embeddings,-,Cross-Lingual Bitext Mining,Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings,/paper/margin-based-parallel-corpus-mining-with,https://arxiv.org/pdf/1811.01136v1.pdf
BUCC German-to-English,,# 2,F1 score,95.58,Multilingual Sentence Embeddings,-,Cross-Lingual Bitext Mining,Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings,/paper/margin-based-parallel-corpus-mining-with,https://arxiv.org/pdf/1811.01136v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 5,Accuracy,50.13%,PLATIPUS,-,Few-Shot Image Classification,Probabilistic Model-Agnostic Meta-Learning,/paper/probabilistic-model-agnostic-meta-learning,https://arxiv.org/pdf/1806.02817v1.pdf
Multi-Domain Sentiment Dataset,,# 2,DVD,78.14,Multi-task tri-training,-,Sentiment Analysis,Strong Baselines for Neural Semi-supervised Learning under Domain Shift,/paper/strong-baselines-for-neural-semi-supervised,https://arxiv.org/pdf/1804.09530v1.pdf
Multi-Domain Sentiment Dataset,,# 2,Books,74.86,Multi-task tri-training,-,Sentiment Analysis,Strong Baselines for Neural Semi-supervised Learning under Domain Shift,/paper/strong-baselines-for-neural-semi-supervised,https://arxiv.org/pdf/1804.09530v1.pdf
Multi-Domain Sentiment Dataset,,# 1,Electronics,81.45,Multi-task tri-training,-,Sentiment Analysis,Strong Baselines for Neural Semi-supervised Learning under Domain Shift,/paper/strong-baselines-for-neural-semi-supervised,https://arxiv.org/pdf/1804.09530v1.pdf
Multi-Domain Sentiment Dataset,,# 4,Kitchen,82.14,Multi-task tri-training,-,Sentiment Analysis,Strong Baselines for Neural Semi-supervised Learning under Domain Shift,/paper/strong-baselines-for-neural-semi-supervised,https://arxiv.org/pdf/1804.09530v1.pdf
Multi-Domain Sentiment Dataset,,# 2,Average,79.15,Multi-task tri-training,-,Sentiment Analysis,Strong Baselines for Neural Semi-supervised Learning under Domain Shift,/paper/strong-baselines-for-neural-semi-supervised,https://arxiv.org/pdf/1804.09530v1.pdf
BSD100 - 4x upscaling,,# 1,PSNR,27.85,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
BSD100 - 4x upscaling,,# 3,SSIM,0.7455,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Manga109 - 4x upscaling,,# 8,PSNR,24.89,bicubic,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Manga109 - 4x upscaling,,# 9,SSIM,0.7866,bicubic,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Manga109 - 4x upscaling,,# 1,PSNR,31.66,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Manga109 - 4x upscaling,,# 1,SSIM,0.9196,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Set14 - 4x upscaling,,# 1,PSNR,28.99,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Set14 - 4x upscaling,,# 4,SSIM,0.7917,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Set5 - 4x upscaling,,# 2,PSNR,32.73,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Set5 - 4x upscaling,,# 3,SSIM,0.9011,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Urban100 - 4x upscaling,,# 1,PSNR,27.03,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Urban100 - 4x upscaling,,# 1,SSIM,0.8153,SRGAN + Residual-in-Residual Dense Block,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Urban100 - 4x upscaling,,# 23,PSNR,23.14,bicubic,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
Urban100 - 4x upscaling,,# 22,SSIM,0.6577,bicubic,-,Image Super-Resolution,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,/paper/esrgan-enhanced-super-resolution-generative,https://arxiv.org/pdf/1809.00219v2.pdf
CCGBank,,# 1,Accuracy,96.1,Clark et al.,-,CCG Supertagging,Semi-Supervised Sequence Modeling with Cross-View Training,/paper/semi-supervised-sequence-modeling-with-cross,https://arxiv.org/pdf/1809.08370v1.pdf
CoNLL 2003 (English),,# 4,F1,92.61,CVT + Multi-Task,-,Named Entity Recognition (NER),Semi-Supervised Sequence Modeling with Cross-View Training,/paper/semi-supervised-sequence-modeling-with-cross,https://arxiv.org/pdf/1809.08370v1.pdf
Ontonotes v5 (English),,# 2,F1,88.81,CVT + Multi-Task,-,Named Entity Recognition (NER),Semi-Supervised Sequence Modeling with Cross-View Training,/paper/semi-supervised-sequence-modeling-with-cross,https://arxiv.org/pdf/1809.08370v1.pdf
Penn Treebank,,# 4,POS,---,CVT + Multi-Task,-,Dependency Parsing,Semi-Supervised Sequence Modeling with Cross-View Training,/paper/semi-supervised-sequence-modeling-with-cross,https://arxiv.org/pdf/1809.08370v1.pdf
Penn Treebank,,# 1,UAS,96.61,CVT + Multi-Task,-,Dependency Parsing,Semi-Supervised Sequence Modeling with Cross-View Training,/paper/semi-supervised-sequence-modeling-with-cross,https://arxiv.org/pdf/1809.08370v1.pdf
Penn Treebank,,# 1,LAS,95.02,CVT + Multi-Task,-,Dependency Parsing,Semi-Supervised Sequence Modeling with Cross-View Training,/paper/semi-supervised-sequence-modeling-with-cross,https://arxiv.org/pdf/1809.08370v1.pdf
WebQSP-WD,,# 1,F1,0.73,VCG,-,Entity Linking,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,/paper/mixing-context-granularities-for-improved,https://arxiv.org/pdf/1804.08460v1.pdf
Douban,,# 5,RMSE,0.8009999999999999,sRGCNN,-,Collaborative Filtering,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,/paper/geometric-matrix-completion-with-recurrent,https://arxiv.org/pdf/1704.06803v1.pdf
Flixster,,# 3,RMSE,0.9259999999999999,sRGCNN,-,Collaborative Filtering,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,/paper/geometric-matrix-completion-with-recurrent,https://arxiv.org/pdf/1704.06803v1.pdf
MovieLens 100K,,# 3,RMSE,0.929,sRGCNN,-,Collaborative Filtering,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,/paper/geometric-matrix-completion-with-recurrent,https://arxiv.org/pdf/1704.06803v1.pdf
Caltech-101,,# 1,Top-1 Error Rate,13.07%,AutoAugment,-,Fine-Grained Image Classification,AutoAugment: Learning Augmentation Policies from Data,/paper/autoaugment-learning-augmentation-policies,https://arxiv.org/pdf/1805.09501v3.pdf
FGVC Aircraft,,# 1,Top-1 Error Rate,7.33%,AutoAugment,-,Fine-Grained Image Classification,AutoAugment: Learning Augmentation Policies from Data,/paper/autoaugment-learning-augmentation-policies,https://arxiv.org/pdf/1805.09501v3.pdf
Oxford 102 Flowers,,# 1,Top-1 Error Rate,4.64%,AutoAugment,-,Fine-Grained Image Classification,AutoAugment: Learning Augmentation Policies from Data,/paper/autoaugment-learning-augmentation-policies,https://arxiv.org/pdf/1805.09501v3.pdf
Oxford-IIIT Pets,,# 1,Top-1 Error Rate,11.02%,AutoAugment,-,Fine-Grained Image Classification,AutoAugment: Learning Augmentation Policies from Data,/paper/autoaugment-learning-augmentation-policies,https://arxiv.org/pdf/1805.09501v3.pdf
Stanford Cars,,# 1,Top-1 Error Rate,5.19%,AutoAugment,-,Fine-Grained Image Classification,AutoAugment: Learning Augmentation Policies from Data,/paper/autoaugment-learning-augmentation-policies,https://arxiv.org/pdf/1805.09501v3.pdf
ImageNet,,# 20,Top 1 Accuracy,70.9%,ShuffleNet,-,Image Classification,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,/paper/shufflenet-an-extremely-efficient,https://arxiv.org/pdf/1707.01083v2.pdf
ImageNet,,# 16,Top 5 Accuracy,89.8%,ShuffleNet,-,Image Classification,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,/paper/shufflenet-an-extremely-efficient,https://arxiv.org/pdf/1707.01083v2.pdf
Caltech,,# 18,Reasonable Miss Rate,23.3,AlexNet,-,Pedestrian Detection,Taking a Deeper Look at Pedestrians,/paper/taking-a-deeper-look-at-pedestrians,https://arxiv.org/pdf/1501.05790v1.pdf
CoNLL 2005,,# 1,F1,98.4,LISA,-,Predicate Detection,Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
CoNLL 2005,,# 1,F1,86.9,LISA + ELMo,-,Semantic Role Labeling (predicted predicates),Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
CoNLL 2005,,# 4,F1,86.04,LISA,-,Semantic Role Labeling,Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
CoNLL 2005,,# 3,F1,84.99,LISA,-,Semantic Role Labeling (predicted predicates),Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
CoNLL 2012,,# 3,F1,82.33,LISA,-,Semantic Role Labeling (predicted predicates),Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
CoNLL 2012,,# 1,F1,83.38,LISA + ELMo,-,Semantic Role Labeling (predicted predicates),Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
CoNLL 2012,,# 1,F1,97.2,LISA,-,Predicate Detection,Linguistically-Informed Self-Attention for Semantic Role Labeling,/paper/linguistically-informed-self-attention-for,https://arxiv.org/pdf/1804.08199v3.pdf
Cityscapes,,# 1,Mean IoU,82.1%,DeepLabv3+ (Xception-JFT),-,Semantic Segmentation,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,/paper/encoder-decoder-with-atrous-separable,https://arxiv.org/pdf/1802.02611v3.pdf
PASCAL VOC 2012,,# 1,Mean IoU,89.0%,DeepLabv3+ (Xception-JFT),-,Semantic Segmentation,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,/paper/encoder-decoder-with-atrous-separable,https://arxiv.org/pdf/1802.02611v3.pdf
PASCAL Context,,# 7,mIoU,44.5,VeryDeep,-,Semantic Segmentation,Bridging Category-level and Instance-level Semantic Image Segmentation,/paper/bridging-category-level-and-instance-level,https://arxiv.org/pdf/1605.06885v1.pdf
SUBJ,,# 1,Accuracy,95.5,AdaSent,-,Subjectivity Analysis,Self-Adaptive Hierarchical Sentence Model,/paper/self-adaptive-hierarchical-sentence-model,https://arxiv.org/pdf/1504.05070v2.pdf
CIFAR-10,,# 6,NLL Test,3.35,GLOW,-,Image Generation,Glow: Generative Flow with Invertible 1x1 Convolutions,/paper/glow-generative-flow-with-invertible-1x1,https://arxiv.org/pdf/1807.03039v2.pdf
ImageNet 64x64,,# 5,Bits per byte,3.81,GLOW,-,Image Generation,Glow: Generative Flow with Invertible 1x1 Convolutions,/paper/glow-generative-flow-with-invertible-1x1,https://arxiv.org/pdf/1807.03039v2.pdf
Penn Treebank,,# 2,POS,97.44,Weiss et al.,-,Dependency Parsing,Structured Training for Neural Network Transition-Based Parsing,/paper/structured-training-for-neural-network,https://arxiv.org/pdf/1506.06158v1.pdf
Penn Treebank,,# 6,UAS,93.99,Weiss et al.,-,Dependency Parsing,Structured Training for Neural Network Transition-Based Parsing,/paper/structured-training-for-neural-network,https://arxiv.org/pdf/1506.06158v1.pdf
Penn Treebank,,# 6,LAS,92.05,Weiss et al.,-,Dependency Parsing,Structured Training for Neural Network Transition-Based Parsing,/paper/structured-training-for-neural-network,https://arxiv.org/pdf/1506.06158v1.pdf
CACDVS,,# 5,Accuracy,97.95%,MFM-CNN,-,Age-Invariant Face Recognition,A Light CNN for Deep Face Representation with Noisy Labels,/paper/a-light-cnn-for-deep-face-representation-with,https://arxiv.org/pdf/1511.02683v4.pdf
CAFR,,# 2,Accuracy,73.56%,Light CNN,-,Age-Invariant Face Recognition,A Light CNN for Deep Face Representation with Noisy Labels,/paper/a-light-cnn-for-deep-face-representation-with,https://arxiv.org/pdf/1511.02683v4.pdf
Labeled Faces in the Wild,,# 7,Accuracy,99.33%,Light CNN-29,-,Face Verification,A Light CNN for Deep Face Representation with Noisy Labels,/paper/a-light-cnn-for-deep-face-representation-with,https://arxiv.org/pdf/1511.02683v4.pdf
MegaFace,,# 6,Accuracy,85.133%,Light CNN-29,-,Face Verification,A Light CNN for Deep Face Representation with Noisy Labels,/paper/a-light-cnn-for-deep-face-representation-with,https://arxiv.org/pdf/1511.02683v4.pdf
MegaFace,,# 4,Accuracy,73.749%,Light CNN-29,-,Face Identification,A Light CNN for Deep Face Representation with Noisy Labels,/paper/a-light-cnn-for-deep-face-representation-with,https://arxiv.org/pdf/1511.02683v4.pdf
YouTube Faces DB,,# 5,Accuracy,95.54%,Light CNN-29,-,Face Verification,A Light CNN for Deep Face Representation with Noisy Labels,/paper/a-light-cnn-for-deep-face-representation-with,https://arxiv.org/pdf/1511.02683v4.pdf
BSD100 - 4x upscaling,,# 9,PSNR,27.57,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
BSD100 - 4x upscaling,,# 13,SSIM,0.7353,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
Set14 - 4x upscaling,,# 9,PSNR,28.56,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
Set14 - 4x upscaling,,# 15,SSIM,0.7803,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
Set5 - 4x upscaling,,# 7,PSNR,32.14,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
Set5 - 4x upscaling,,# 12,SSIM,0.8937,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
Urban100 - 4x upscaling,,# 11,PSNR,26.03,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
Urban100 - 4x upscaling,,# 10,SSIM,0.7835,BSRN,-,Image Super-Resolution,Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network,/paper/lightweight-and-efficient-image-super,https://arxiv.org/pdf/1811.12546v1.pdf
enwiki8,,# 9,Bit per Character (BPC),1.27,Recurrent highway networks,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
enwiki8,,# 1,Number of params,46M,Recurrent highway networks,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Hutter Prize,,# 9,Bit per Character (BPC),1.27,Large RHN,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Hutter Prize,,# 1,Number of params,46M,Large RHN,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Penn Treebank (Word Level),,# 17,Validation perplexity,67.9,Recurrent highway networks,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Penn Treebank (Word Level),,# 20,Test perplexity,65.4,Recurrent highway networks,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Penn Treebank (Word Level),,# 1,Params,23M,Recurrent highway networks,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Text8,,# 7,Bit per Character (BPC),1.27,Large RHN,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
Text8,,# 1,Number of params,46M,Large RHN,-,Language Modelling,Recurrent Highway Networks,/paper/recurrent-highway-networks,https://arxiv.org/pdf/1607.03474v5.pdf
SNLI,,# 39,% Test Accuracy,83.5,100D LSTMs w/ word-by-word attention,-,Natural Language Inference,Reasoning about Entailment with Neural Attention,/paper/reasoning-about-entailment-with-neural,https://arxiv.org/pdf/1509.06664v4.pdf
SNLI,,# 47,% Train Accuracy,85.3,100D LSTMs w/ word-by-word attention,-,Natural Language Inference,Reasoning about Entailment with Neural Attention,/paper/reasoning-about-entailment-with-neural,https://arxiv.org/pdf/1509.06664v4.pdf
SNLI,,# 1,Parameters,250k,100D LSTMs w/ word-by-word attention,-,Natural Language Inference,Reasoning about Entailment with Neural Attention,/paper/reasoning-about-entailment-with-neural,https://arxiv.org/pdf/1509.06664v4.pdf
AG News,,# 11,Error,7.61,CCCapsNet,-,Text Classification,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
Amazon Review Full,,# 7,Accuracy,60.95,CCCapsNet,-,Sentiment Analysis,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
Amazon Review Polarity,,# 7,Accuracy,94.96,CCCapsNet,-,Sentiment Analysis,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
DBpedia,,# 12,Error,1.28,CCCapsNet,-,Text Classification,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
Sogou News,,# 1,Accuracy,97.25,CCCapsNet,-,Text Classification,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
Yahoo! Answers,,# 3,Accuracy,73.85,CCCapsNet,-,Text Classification,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
Yelp Binary classification,,# 8,Error,3.52,CCCapsNet,-,Sentiment Analysis,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
Yelp Fine-grained classification,,# 8,Error,34.15,CCCapsNet,-,Sentiment Analysis,Compositional coding capsule network with k-means routing for text classification,/paper/compositional-coding-capsule-network-with-k,https://arxiv.org/pdf/1810.09177v3.pdf
TREC Robust04,,# 5,MAP,0.2846,NPRF-KNRM,-,Ad-Hoc Information Retrieval,NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval,/paper/nprf-a-neural-pseudo-relevance-feedback,https://arxiv.org/pdf/1810.12936v1.pdf
TREC Robust04,,# 12,MAP,0.2464,KNRM,-,Ad-Hoc Information Retrieval,NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval,/paper/nprf-a-neural-pseudo-relevance-feedback,https://arxiv.org/pdf/1810.12936v1.pdf
TREC Robust04,,# 3,MAP,0.2904,NPRF-DRMM,-,Ad-Hoc Information Retrieval,NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval,/paper/nprf-a-neural-pseudo-relevance-feedback,https://arxiv.org/pdf/1810.12936v1.pdf
TREC Robust04,,# 1,MAP,0.302,Anserini BM25+RM3,-,Ad-Hoc Information Retrieval,The Neural Hype and Comparisons Against Weak Baselines,/paper/the-neural-hype-and-comparisons-against-weak,https://sigir.org/wp-content/uploads/2019/01/p040.pdf
SNLI,,# 48,% Test Accuracy,50.4,Unlexicalized features,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 54,% Train Accuracy,49.4,Unlexicalized features,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 1,Parameters,,Unlexicalized features,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 46,% Test Accuracy,78.2,+ Unigram and bigram features,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 1,% Train Accuracy,99.7,+ Unigram and bigram features,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 1,Parameters,,+ Unigram and bigram features,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 47,% Test Accuracy,77.6,100D LSTM encoders,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 49,% Train Accuracy,84.8,100D LSTM encoders,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
SNLI,,# 1,Parameters,220k,100D LSTM encoders,-,Natural Language Inference,A large annotated corpus for learning natural language inference,/paper/a-large-annotated-corpus-for-learning-natural,https://arxiv.org/pdf/1508.05326v1.pdf
WIDER Face (Easy),,# 13,AP,0.695,ACF-WIDER,-,Face Detection,Aggregate channel features for multi-view face detection,/paper/aggregate-channel-features-for-multi-view,https://arxiv.org/pdf/1407.4023v2.pdf
WIDER Face (Hard),,# 16,AP,0.29,ACF-WIDER,-,Face Detection,Aggregate channel features for multi-view face detection,/paper/aggregate-channel-features-for-multi-view,https://arxiv.org/pdf/1407.4023v2.pdf
WIDER Face (Medium),,# 14,AP,0.588,ACF-WIDER,-,Face Detection,Aggregate channel features for multi-view face detection,/paper/aggregate-channel-features-for-multi-view,https://arxiv.org/pdf/1407.4023v2.pdf
IJB-A,,# 7,TAR @ FAR=0.01,92.20%,All-in-one CNN,-,Face Verification,An All-In-One Convolutional Neural Network for Face Analysis,/paper/an-all-in-one-convolutional-neural-network,https://arxiv.org/pdf/1611.00851v1.pdf
Citeseer,,# 3,Accuracy,71.8%,DGI,-,Node Classification,Deep Graph Infomax,/paper/deep-graph-infomax,https://arxiv.org/pdf/1809.10341v2.pdf
Cora,,# 3,Accuracy,82.3%,DGI,-,Node Classification,Deep Graph Infomax,/paper/deep-graph-infomax,https://arxiv.org/pdf/1809.10341v2.pdf
Pubmed,,# 6,Accuracy,76.80%,DGI,-,Node Classification,Deep Graph Infomax,/paper/deep-graph-infomax,https://arxiv.org/pdf/1809.10341v2.pdf
CoQA,,# 6,In-domain,69.4,BiDAF++ (single model),-,Question Answering,"A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC",/paper/a-qualitative-comparison-of-coqa-squad-20-and,https://arxiv.org/pdf/1809.10735v1.pdf
CoQA,,# 6,Out-of-domain,63.8,BiDAF++ (single model),-,Question Answering,"A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC",/paper/a-qualitative-comparison-of-coqa-squad-20-and,https://arxiv.org/pdf/1809.10735v1.pdf
CoQA,,# 6,Overall,67.8,BiDAF++ (single model),-,Question Answering,"A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC",/paper/a-qualitative-comparison-of-coqa-squad-20-and,https://arxiv.org/pdf/1809.10735v1.pdf
enwiki8,,# 10,Bit per Character (BPC),1.32,LN HM-LSTM,-,Language Modelling,Hierarchical Multiscale Recurrent Neural Networks,/paper/hierarchical-multiscale-recurrent-neural,https://arxiv.org/pdf/1609.01704v7.pdf
enwiki8,,# 1,Number of params,35M,LN HM-LSTM,-,Language Modelling,Hierarchical Multiscale Recurrent Neural Networks,/paper/hierarchical-multiscale-recurrent-neural,https://arxiv.org/pdf/1609.01704v7.pdf
Text8,,# 8,Bit per Character (BPC),1.29,LayerNorm HM-LSTM,-,Language Modelling,Hierarchical Multiscale Recurrent Neural Networks,/paper/hierarchical-multiscale-recurrent-neural,https://arxiv.org/pdf/1609.01704v7.pdf
Text8,,# 1,Number of params,35M,LayerNorm HM-LSTM,-,Language Modelling,Hierarchical Multiscale Recurrent Neural Networks,/paper/hierarchical-multiscale-recurrent-neural,https://arxiv.org/pdf/1609.01704v7.pdf
PASCAL VOC 2012,,# 9,Mean IoU,83.1%,TuSimple,-,Semantic Segmentation,Understanding Convolution for Semantic Segmentation,/paper/understanding-convolution-for-semantic,https://arxiv.org/pdf/1702.08502v3.pdf
Caltech,,# 14,Reasonable Miss Rate,11.75,CompACT-Deep,-,Pedestrian Detection,Learning Complexity-Aware Cascades for Deep Pedestrian Detection,/paper/learning-complexity-aware-cascades-for-deep,https://arxiv.org/pdf/1507.05348v1.pdf
LDC2014T12:,,# 3,F1 Newswire,0.70,Imitation learning,-,Amr Parsing,Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing,/paper/noise-reduction-and-targeted-exploration-in,https://aclweb.org/anthology/P16-1001
LDC2014T12:,,# 3,F1 Full,--,Imitation learning,-,Amr Parsing,Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing,/paper/noise-reduction-and-targeted-exploration-in,https://aclweb.org/anthology/P16-1001
SST-2 Binary classification,,# 11,Accuracy,89.5,BLSTM-2DCNN,-,Sentiment Analysis,Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling,/paper/text-classification-improved-by-integrating,https://arxiv.org/pdf/1611.06639v1.pdf
TREC-6,,# 3,Error,3.9,LSTM-CNN,-,Text Classification,Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling,/paper/text-classification-improved-by-integrating,https://arxiv.org/pdf/1611.06639v1.pdf
Atari 2600 Freeway,,# 16,Score,27.0,MP-EB,-,Atari Games,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,/paper/incentivizing-exploration-in-reinforcement,https://arxiv.org/pdf/1507.00814v3.pdf
Atari 2600 Frostbite,,# 17,Score,507.0,MP-EB,-,Atari Games,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,/paper/incentivizing-exploration-in-reinforcement,https://arxiv.org/pdf/1507.00814v3.pdf
Atari 2600 Montezuma's Revenge,,# 10,Score,142.0,MP-EB,-,Atari Games,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,/paper/incentivizing-exploration-in-reinforcement,https://arxiv.org/pdf/1507.00814v3.pdf
Atari 2600 Q*Bert,,# 7,Score,15805.0,MP-EB,-,Atari Games,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,/paper/incentivizing-exploration-in-reinforcement,https://arxiv.org/pdf/1507.00814v3.pdf
Atari 2600 Venture,,# 27,Score,0.0,MP-EB,-,Atari Games,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,/paper/incentivizing-exploration-in-reinforcement,https://arxiv.org/pdf/1507.00814v3.pdf
COCO,,# 1,AP,0.770,HRNet-48,-,Multi-Person Pose Estimation,Deep High-Resolution Representation Learning for Human Pose Estimation,/paper/deep-high-resolution-representation-learning,https://arxiv.org/pdf/1902.09212v1.pdf
COCO,,# 1,Mean mAP,77.0,HRNet-48,-,Pose Estimation,Deep High-Resolution Representation Learning for Human Pose Estimation,/paper/deep-high-resolution-representation-learning,https://arxiv.org/pdf/1902.09212v1.pdf
COCO,,# 1,Validation AP,76.3,HRNet-48,-,Keypoint Detection,Deep High-Resolution Representation Learning for Human Pose Estimation,/paper/deep-high-resolution-representation-learning,https://arxiv.org/pdf/1902.09212v1.pdf
COCO,,# 1,Test AP,75.5,HRNet-48,-,Keypoint Detection,Deep High-Resolution Representation Learning for Human Pose Estimation,/paper/deep-high-resolution-representation-learning,https://arxiv.org/pdf/1902.09212v1.pdf
COCO,,# 2,Validation AP,75.8,HRNet-32,-,Keypoint Detection,Deep High-Resolution Representation Learning for Human Pose Estimation,/paper/deep-high-resolution-representation-learning,https://arxiv.org/pdf/1902.09212v1.pdf
MPII Human Pose,,# 1,PCKh-0.5,92.3%,HRNet-32,-,Pose Estimation,Deep High-Resolution Representation Learning for Human Pose Estimation,/paper/deep-high-resolution-representation-learning,https://arxiv.org/pdf/1902.09212v1.pdf
Caltech,,# 19,Reasonable Miss Rate,24.8,LDCF,-,Pedestrian Detection,Local Decorrelation For Improved Pedestrian Detection,/paper/local-decorrelation-for-improved-pedestrian,https://papers.nips.cc/paper/5419-local-decorrelation-for-improved-pedestrian-detection.pdf
Chinese Poems,,# 3,BLEU-2,0.738,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
COCO Captions,,# 3,BLEU-2,0.831,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
COCO Captions,,# 3,BLEU-3,0.642,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
COCO Captions,,# 3,BLEU-4,0.521,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
COCO Captions,,# 3,BLEU-5,0.42700000000000005,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
EMNLP2017 WMT,,# 2,BLEU-2,0.8590000000000001,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
EMNLP2017 WMT,,# 2,BLEU-3,0.6015,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
EMNLP2017 WMT,,# 2,BLEU-4,0.4541,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
EMNLP2017 WMT,,# 3,BLEU-5,0.4498,SeqGAN,-,Text Generation,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,/paper/seqgan-sequence-generative-adversarial-nets,https://arxiv.org/pdf/1609.05473v6.pdf
VOT2017/18,,# 1,Expected Average Overlap (EAO),0.414,SiamRPN++,-,Visual Object Tracking,SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks,/paper/siamrpn-evolution-of-siamese-visual-tracking,https://arxiv.org/pdf/1812.11703v1.pdf
SK-LARGE,,# 1,F-Measure,0.732,DeepFlux,-,Object Skeleton Detection,DeepFlux for Skeletons in the Wild,/paper/deepflux-for-skeletons-in-the-wild,https://arxiv.org/pdf/1811.12608v1.pdf
CUB-200-2011,,# 10,Accuracy,76.4%,Part RCNN,-,Fine-Grained Image Classification,Part-based R-CNNs for Fine-grained Category Detection,/paper/part-based-r-cnns-for-fine-grained-category,https://arxiv.org/pdf/1407.3867v1.pdf
PASCAL VOC 2007,,# 7,MAP,78.9%,OHEM,-,Object Detection,Training Region-based Object Detectors with Online Hard Example Mining,/paper/training-region-based-object-detectors-with,https://arxiv.org/pdf/1604.03540v1.pdf
bAbi,,# 6,Accuracy (trained on 1k),60%,GORU,-,Question Answering,Gated Orthogonal Recurrent Units: On Learning to Forget,/paper/gated-orthogonal-recurrent-units-on-learning,https://arxiv.org/pdf/1706.02761v3.pdf
AG News,,# 7,Error,7.34,SWEM-concat,-,Text Classification,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
CoNLL 2003 (English),,# 19,F1,86.28,SWEM-CRF,-,Named Entity Recognition (NER),Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
DBpedia,,# 15,Error,1.43,SWEM-concat,-,Text Classification,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
MR,,# 7,Accuracy,78.2,SWEM-concat,-,Sentiment Analysis,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
SST-2 Binary classification,,# 19,Accuracy,84.3,SWEM-concat,-,Sentiment Analysis,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
SST-5 Fine-grained classification,,# 12,Accuracy,46.1,SWEM-concat,-,Sentiment Analysis,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
SUBJ,,# 7,Accuracy,93.0,SWEM-concat,-,Subjectivity Analysis,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
TREC-6,,# 9,Error,7.8,SWEM-aver,-,Text Classification,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
Yahoo! Answers,,# 4,Accuracy,73.53,SWEM-concat,-,Text Classification,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
Yelp Binary classification,,# 11,Error,4.19,SWEM-hier,-,Sentiment Analysis,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
Yelp Fine-grained classification,,# 11,Error,36.21,SWEM-hier,-,Sentiment Analysis,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word,https://arxiv.org/pdf/1805.09843v1.pdf
SemEval,,# 1,F1-score,0.685,LSTMs+CNNs ensemble with multiple conv. ops,-,Sentiment Analysis,BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs,/paper/bb_twtr-at-semeval-2017-task-4-twitter,https://arxiv.org/pdf/1704.06125v1.pdf
AFLW2000-3D,,# 4,Mean NME,4.50%,DeFA,-,Face Alignment,Dense Face Alignment,/paper/dense-face-alignment,https://arxiv.org/pdf/1709.01442v1.pdf
AFLW2000-3D,,# 3,Mean NME,5.6454%,DeFA,-,3D Face Reconstruction,Dense Face Alignment,/paper/dense-face-alignment,https://arxiv.org/pdf/1709.01442v1.pdf
AFLW-LFPA,,# 2,Mean NME,3.86%,DeFA,-,Face Alignment,Dense Face Alignment,/paper/dense-face-alignment,https://arxiv.org/pdf/1709.01442v1.pdf
IWSLT2015 German-English,,# 16,BLEU score,19.41,QRNN,-,Machine Translation,Quasi-Recurrent Neural Networks,/paper/quasi-recurrent-neural-networks,https://arxiv.org/pdf/1611.01576v2.pdf
WMT2014 English-French,,# 22,BLEU score,34.54,CSLM + RNN + WP,-,Machine Translation,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,/paper/learning-phrase-representations-using-rnn,https://arxiv.org/pdf/1406.1078v3.pdf
Citeseer,,# 4,Accuracy,71.60%,alpha-LoNGAE,-,Node Classification,Learning to Make Predictions on Graphs with Autoencoders,/paper/learning-to-make-predictions-on-graphs-with,https://arxiv.org/pdf/1802.08352v2.pdf
Cora,,# 7,Accuracy,78.30%,alpha-LoNGAE,-,Node Classification,Learning to Make Predictions on Graphs with Autoencoders,/paper/learning-to-make-predictions-on-graphs-with,https://arxiv.org/pdf/1802.08352v2.pdf
Pubmed,,# 3,Accuracy,79.40%,alpha-LoNGAE,-,Node Classification,Learning to Make Predictions on Graphs with Autoencoders,/paper/learning-to-make-predictions-on-graphs-with,https://arxiv.org/pdf/1802.08352v2.pdf
MPII Multi-Person,,# 6,AP,62.2%,Local Joint-to-Person Association,-,Multi-Person Pose Estimation,Multi-Person Pose Estimation with Local Joint-to-Person Associations,/paper/multi-person-pose-estimation-with-local-joint,https://arxiv.org/pdf/1608.08526v2.pdf
IJB-A,,# 5,TAR @ FAR=0.01,94.10%,NAN,-,Face Verification,Neural Aggregation Network for Video Face Recognition,/paper/neural-aggregation-network-for-video-face,https://arxiv.org/pdf/1603.05474v4.pdf
IWSLT2015 German-English,,# 5,BLEU score,32.93,ConvS2S+Risk,-,Machine Translation,Classical Structured Prediction Losses for Sequence to Sequence Learning,/paper/classical-structured-prediction-losses-for,https://arxiv.org/pdf/1711.04956v5.pdf
IWSLT2015 German-English,,# 6,BLEU score,32.84,ConvS2S (MLE+SLE),-,Machine Translation,Classical Structured Prediction Losses for Sequence to Sequence Learning,/paper/classical-structured-prediction-losses-for,https://arxiv.org/pdf/1711.04956v5.pdf
Barrettâs Esophagus,,# 1,Mean Accuracy,81%,Attention-based model,-,Medical Object Detection,Finding a Needle in the Haystack: Attention-Based Classification of High Resolution Microscopy Images,/paper/finding-a-needle-in-the-haystack-attention,https://arxiv.org/pdf/1811.08513v1.pdf
Atari 2600 Freeway,,# 10,Score,30.48,A3C-CTS,-,Atari Games,Unifying Count-Based Exploration and Intrinsic Motivation,/paper/unifying-count-based-exploration-and,https://arxiv.org/pdf/1606.01868v2.pdf
Atari 2600 Gravitar,,# 21,Score,238.68,A3C-CTS,-,Atari Games,Unifying Count-Based Exploration and Intrinsic Motivation,/paper/unifying-count-based-exploration-and,https://arxiv.org/pdf/1606.01868v2.pdf
Atari 2600 Montezuma's Revenge,,# 4,Score,3459.0,DDQN-PC,-,Atari Games,Unifying Count-Based Exploration and Intrinsic Motivation,/paper/unifying-count-based-exploration-and,https://arxiv.org/pdf/1606.01868v2.pdf
Atari 2600 Montezuma's Revenge,,# 8,Score,273.7,A3C-CTS,-,Atari Games,Unifying Count-Based Exploration and Intrinsic Motivation,/paper/unifying-count-based-exploration-and,https://arxiv.org/pdf/1606.01868v2.pdf
Atari 2600 Private Eye,,# 22,Score,99.32,A3C-CTS,-,Atari Games,Unifying Count-Based Exploration and Intrinsic Motivation,/paper/unifying-count-based-exploration-and,https://arxiv.org/pdf/1606.01868v2.pdf
Atari 2600 Venture,,# 27,Score,0.0,A3C-CTS,-,Atari Games,Unifying Count-Based Exploration and Intrinsic Motivation,/paper/unifying-count-based-exploration-and,https://arxiv.org/pdf/1606.01868v2.pdf
WikiText-103,,# 8,Validation perplexity,-,Temporal CNN,-,Language Modelling,Convolutional Sequence Modeling Revisited,/paper/convolutional-sequence-modeling-revisited,https://openreview.net/pdf?id=rk8wKk-R-
WikiText-103,,# 12,Test perplexity,45.2,Temporal CNN,-,Language Modelling,Convolutional Sequence Modeling Revisited,/paper/convolutional-sequence-modeling-revisited,https://openreview.net/pdf?id=rk8wKk-R-
Switchboard + Hub500,,# 5,Percentage error,6.6,CNN-LSTM,-,Speech Recognition,Achieving Human Parity in Conversational Speech Recognition,/paper/achieving-human-parity-in-conversational,https://arxiv.org/pdf/1610.05256v2.pdf
Switchboard + Hub500,,# 2,Percentage error,5.8,Microsoft 2016b,-,Speech Recognition,Achieving Human Parity in Conversational Speech Recognition,/paper/achieving-human-parity-in-conversational,https://arxiv.org/pdf/1610.05256v2.pdf
CoNLL 2003 (English),,# 14,F1,91.35,NCRF++,-,Named Entity Recognition (NER),NCRF++: An Open-source Neural Sequence Labeling Toolkit,/paper/ncrf-an-open-source-neural-sequence-labeling,https://arxiv.org/pdf/1806.05626v2.pdf
Penn Treebank,,# 7,Accuracy,97.49,NCRF++,-,Part-Of-Speech Tagging,NCRF++: An Open-source Neural Sequence Labeling Toolkit,/paper/ncrf-an-open-source-neural-sequence-labeling,https://arxiv.org/pdf/1806.05626v2.pdf
Penn Treebank,,# 5,F1 score,95.06,NCRF++,-,Chunking,NCRF++: An Open-source Neural Sequence Labeling Toolkit,/paper/ncrf-an-open-source-neural-sequence-labeling,https://arxiv.org/pdf/1806.05626v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 18,Restaurant (Acc),79.2,PF-CNN,-,Aspect-Based Sentiment Analysis,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,/paper/parameterized-convolutional-neural-networks,https://aclweb.org/anthology/D18-1136
SemEval 2014 Task 4 Sub Task 2,,# 4,Laptop (Acc),70.06,PF-CNN,-,Aspect-Based Sentiment Analysis,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,/paper/parameterized-convolutional-neural-networks,https://aclweb.org/anthology/D18-1136
Penn Treebank,,# 8,F1 score,93.6,Stack-only RNNG,-,Constituency Parsing,What Do Recurrent Neural Network Grammars Learn About Syntax?,/paper/what-do-recurrent-neural-network-grammars,https://arxiv.org/pdf/1611.05774v2.pdf
KITTI Cars Easy,,# 1,AP,84.33%,PC-CNN-V2,-,3D Object Detection,A General Pipeline for 3D Detection of Vehicles,/paper/a-general-pipeline-for-3d-detection-of,https://arxiv.org/pdf/1803.00387v1.pdf
KITTI Cars Hard,,# 4,AP,64.83%,PC-CNN-V2,-,3D Object Detection,A General Pipeline for 3D Detection of Vehicles,/paper/a-general-pipeline-for-3d-detection-of,https://arxiv.org/pdf/1803.00387v1.pdf
KITTI Cars Moderate,,# 3,AP,73.80%,PC-CNN-V2,-,3D Object Detection,A General Pipeline for 3D Detection of Vehicles,/paper/a-general-pipeline-for-3d-detection-of,https://arxiv.org/pdf/1803.00387v1.pdf
CIFAR-10,,# 3,Percentage correct,97.88,SENet + ShakeShake + Cutout,-,Image Classification,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks,https://arxiv.org/pdf/1709.01507v3.pdf
CIFAR-10,,# 3,Percentage error,2.12,SENet + ShakeShake + Cutout,-,Image Classification,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks,https://arxiv.org/pdf/1709.01507v3.pdf
CIFAR-100,,# 2,Percentage correct,84.59,SENet + ShakeEven + Cutout,-,Image Classification,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks,https://arxiv.org/pdf/1709.01507v3.pdf
CIFAR-100,,# 1,Percentage error,15.41,SENet + ShakeEven + Cutout,-,Image Classification,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks,https://arxiv.org/pdf/1709.01507v3.pdf
ImageNet,,# 4,Top 1 Accuracy,82.7%,SENet,-,Image Classification,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks,https://arxiv.org/pdf/1709.01507v3.pdf
ImageNet,,# 3,Top 5 Accuracy,96.2%,SENet,-,Image Classification,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks,https://arxiv.org/pdf/1709.01507v3.pdf
PASCAL VOC 2007,,# 13,MAP,74.2%,FRCN,-,Object Detection,A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection,/paper/a-fast-rcnn-hard-positive-generation-via,https://arxiv.org/pdf/1704.03414v1.pdf
Cityscapes Labels-to-Photo,,# 3,Class IOU,0.06,CoGAN,-,Image-to-Image Translation,Coupled Generative Adversarial Networks,/paper/coupled-generative-adversarial-networks,https://arxiv.org/pdf/1606.07536v2.pdf
Cityscapes Labels-to-Photo,,# 3,Per-class Accuracy,10%,CoGAN,-,Image-to-Image Translation,Coupled Generative Adversarial Networks,/paper/coupled-generative-adversarial-networks,https://arxiv.org/pdf/1606.07536v2.pdf
Cityscapes Labels-to-Photo,,# 7,Per-pixel Accuracy,40%,CoGAN,-,Image-to-Image Translation,Coupled Generative Adversarial Networks,/paper/coupled-generative-adversarial-networks,https://arxiv.org/pdf/1606.07536v2.pdf
Cityscapes Photo-to-Labels,,# 4,Per-pixel Accuracy,45%,CoGAN,-,Image-to-Image Translation,Coupled Generative Adversarial Networks,/paper/coupled-generative-adversarial-networks,https://arxiv.org/pdf/1606.07536v2.pdf
Cityscapes Photo-to-Labels,,# 4,Per-class Accuracy,11%,CoGAN,-,Image-to-Image Translation,Coupled Generative Adversarial Networks,/paper/coupled-generative-adversarial-networks,https://arxiv.org/pdf/1606.07536v2.pdf
Cityscapes Photo-to-Labels,,# 3,Class IOU,0.08,CoGAN,-,Image-to-Image Translation,Coupled Generative Adversarial Networks,/paper/coupled-generative-adversarial-networks,https://arxiv.org/pdf/1606.07536v2.pdf
COCO,,# 20,Bounding Box AP,41.6,GHM-C + GHM-R,-,Object Detection,Gradient Harmonized Single-stage Detector,/paper/gradient-harmonized-single-stage-detector,https://arxiv.org/pdf/1811.05181v1.pdf
GigaWord,,# 8,ROUGE-1,35.47,Struct+2Way+Word,-,Text Summarization,Structure-Infused Copy Mechanisms for Abstractive Summarization,/paper/structure-infused-copy-mechanisms-for-1,https://aclweb.org/anthology/C18-1146
GigaWord,,# 5,ROUGE-2,17.66,Struct+2Way+Word,-,Text Summarization,Structure-Infused Copy Mechanisms for Abstractive Summarization,/paper/structure-infused-copy-mechanisms-for-1,https://aclweb.org/anthology/C18-1146
GigaWord,,# 9,ROUGE-L,33.52,Struct+2Way+Word,-,Text Summarization,Structure-Infused Copy Mechanisms for Abstractive Summarization,/paper/structure-infused-copy-mechanisms-for-1,https://aclweb.org/anthology/C18-1146
Penn Treebank,,# 12,Accuracy,97.22,Bi-LSTM,-,Part-Of-Speech Tagging,Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss,/paper/multilingual-part-of-speech-tagging-with,https://arxiv.org/pdf/1604.05529v3.pdf
UD,,# 2,Avg accuracy,96.4,Bi-LSTM,-,Part-Of-Speech Tagging,Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss,/paper/multilingual-part-of-speech-tagging-with,https://arxiv.org/pdf/1604.05529v3.pdf
ImageNet,,# 2,MAP,16.3,WCCN,-,Weakly Supervised Object Detection,Weakly Supervised Cascaded Convolutional Networks,/paper/weakly-supervised-cascaded-convolutional,https://arxiv.org/pdf/1611.08258v1.pdf
PASCAL VOC 2007,,# 10,MAP,42.8,WCCN,-,Weakly Supervised Object Detection,Weakly Supervised Cascaded Convolutional Networks,/paper/weakly-supervised-cascaded-convolutional,https://arxiv.org/pdf/1611.08258v1.pdf
PASCAL VOC 2012,,# 8,MAP,37.9,WCCN,-,Weakly Supervised Object Detection,Weakly Supervised Cascaded Convolutional Networks,/paper/weakly-supervised-cascaded-convolutional,https://arxiv.org/pdf/1611.08258v1.pdf
CIFAR-10,,# 6,Inception score,6.58,DCGAN,-,Conditional Image Generation,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,/paper/unsupervised-representation-learning-with-1,https://arxiv.org/pdf/1511.06434v2.pdf
CIFAR-10,,# 9,Percentage correct,96.54,DenseNet,-,Image Classification,Densely Connected Convolutional Networks,/paper/densely-connected-convolutional-networks,https://arxiv.org/pdf/1608.06993v5.pdf
CIFAR-10,,# 9,Percentage error,3.46,DenseNet,-,Image Classification,Densely Connected Convolutional Networks,/paper/densely-connected-convolutional-networks,https://arxiv.org/pdf/1608.06993v5.pdf
CIFAR-100,,# 5,Percentage correct,82.62,DenseNet,-,Image Classification,Densely Connected Convolutional Networks,/paper/densely-connected-convolutional-networks,https://arxiv.org/pdf/1608.06993v5.pdf
CIFAR-100,,# 3,Percentage error,17.18,DenseNet,-,Image Classification,Densely Connected Convolutional Networks,/paper/densely-connected-convolutional-networks,https://arxiv.org/pdf/1608.06993v5.pdf
SVHN,,# 4,Percentage error,1.59,DenseNet,-,Image Classification,Densely Connected Convolutional Networks,/paper/densely-connected-convolutional-networks,https://arxiv.org/pdf/1608.06993v5.pdf
COCO,,# 14,Bounding Box AP,43.2,FCOS_ResNeXt-101-64x4d-FPN,-,Object Detection,FCOS: Fully Convolutional One-Stage Object Detection,/paper/fcos-fully-convolutional-one-stage-object,https://arxiv.org/pdf/1904.01355v3.pdf
CHASE_DB1,,# 4,F1 score,0.7783,U-Net,-,Retinal Vessel Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
CHASE_DB1,,# 4,AUC,0.9772,U-Net,-,Retinal Vessel Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
CT-150,,# 2,Dice Score,0.8140000000000001,U-Net,-,Pancreas Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
CT-150,,# 2,Precision,0.848,U-Net,-,Pancreas Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
CT-150,,# 2,Recall,0.8059999999999999,U-Net,-,Pancreas Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
DIC-HeLa,,# 1,Mean IoU,0.7756,U-Net,-,Cell Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
DRIVE,,# 4,F1 score,0.8142,U-Net,-,Retinal Vessel Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
DRIVE,,# 4,AUC,0.9755,U-Net,-,Retinal Vessel Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
ISBI 2012 EM Segmentation,,# 1,Warping Error,0.00035299999999999996,U-Net,-,Medical Image Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
Kaggle Skin Lesion Segmentation,,# 3,F1 score,0.8682,U-Net,-,Skin Cancer Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
Kaggle Skin Lesion Segmentation,,# 3,AUC,0.9371,U-Net,-,Skin Cancer Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
LUNA,,# 3,F1 score,0.9658,U-Net,-,Lung Nodule Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
LUNA,,# 3,AUC,0.9784,U-Net,-,Lung Nodule Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
PhC-U373,,# 1,Mean IoU,0.9203,U-Net,-,Cell Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
STARE,,# 3,F1 score,0.8373,U-Net,-,Retinal Vessel Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
STARE,,# 3,AUC,0.9898,U-Net,-,Retinal Vessel Segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation,/paper/u-net-convolutional-networks-for-biomedical,https://arxiv.org/pdf/1505.04597v1.pdf
PolyAI AmazonQA,,# 1,1-of-100 Accuracy,84.2%,PolyAI Encoder,-,Conversational Response Selection,A Repository of Conversational Datasets,/paper/190406472,https://arxiv.org/pdf/1904.06472v1.pdf
PolyAI OpenSubtitles,,# 1,1-of-100 Accuracy,30.6%,PolyAI Encoder,-,Conversational Response Selection,A Repository of Conversational Datasets,/paper/190406472,https://arxiv.org/pdf/1904.06472v1.pdf
PolyAI Reddit,,# 1,1-of-100 Accuracy,61.3%,PolyAI Encoder,-,Conversational Response Selection,A Repository of Conversational Datasets,/paper/190406472,https://arxiv.org/pdf/1904.06472v1.pdf
SST-5 Fine-grained classification,,# 1,Accuracy,64.4,EDD-LG (shared),-,Sentiment Analysis,Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator,/paper/learning-semantic-sentence-embeddings-using,https://aclweb.org/anthology/C18-1230
COCO,,# 2,Mean mAP,73.7,ResNet152,-,Pose Estimation,Simple Baselines for Human Pose Estimation and Tracking,/paper/simple-baselines-for-human-pose-estimation,https://arxiv.org/pdf/1804.06208v2.pdf
COCO,,# 3,Validation AP,72.2,ResNet-50,-,Keypoint Detection,Simple Baselines for Human Pose Estimation and Tracking,/paper/simple-baselines-for-human-pose-estimation,https://arxiv.org/pdf/1804.06208v2.pdf
Visual7W,,# 1,Percentage correct,72.53,CMN,-,Visual Question Answering,Modeling Relationships in Referential Expressions with Compositional Modular Networks,/paper/modeling-relationships-in-referential,https://arxiv.org/pdf/1611.09978v1.pdf
Visual Genome (pairs),,# 1,Percentage correct,28.52,CMN,-,Visual Question Answering,Modeling Relationships in Referential Expressions with Compositional Modular Networks,/paper/modeling-relationships-in-referential,https://arxiv.org/pdf/1611.09978v1.pdf
Visual Genome (subjects),,# 1,Percentage correct,44.24,CMN,-,Visual Question Answering,Modeling Relationships in Referential Expressions with Compositional Modular Networks,/paper/modeling-relationships-in-referential,https://arxiv.org/pdf/1611.09978v1.pdf
Market-1501,,# 8,Rank-1,88.2,PartLoss,-,Person Re-Identification,Deep Representation Learning with Part Loss for Person Re-Identification,/paper/deep-representation-learning-with-part-loss,https://arxiv.org/pdf/1707.00798v2.pdf
Market-1501,,# 8,MAP,69.3,PartLoss,-,Person Re-Identification,Deep Representation Learning with Part Loss for Person Re-Identification,/paper/deep-representation-learning-with-part-loss,https://arxiv.org/pdf/1707.00798v2.pdf
SQuAD1.1,,# 132,EM,62.446000000000005,Fine-Grained Gating,-,Question Answering,Words or Characters? Fine-grained Gating for Reading Comprehension,/paper/words-or-characters-fine-grained-gating-for,https://arxiv.org/pdf/1611.01724v2.pdf
SQuAD1.1,,# 130,F1,73.327,Fine-Grained Gating,-,Question Answering,Words or Characters? Fine-grained Gating for Reading Comprehension,/paper/words-or-characters-fine-grained-gating-for,https://arxiv.org/pdf/1611.01724v2.pdf
MNIST,,# 2,Accuracy,96.61,Bidirectional InfoGAN,-,Unsupervised MNIST,Inferencing Based on Unsupervised Learning of Disentangled Representations,/paper/inferencing-based-on-unsupervised-learning-of,https://arxiv.org/pdf/1803.02627v1.pdf
MNIST,,# 2,Accuracy,96.61,Bidirectional InfoGAN,-,Unsupervised image classification,Inferencing Based on Unsupervised Learning of Disentangled Representations,/paper/inferencing-based-on-unsupervised-learning-of,https://arxiv.org/pdf/1803.02627v1.pdf
TIMIT,,# 11,Percentage error,17.6,Bi-RNN + Attention,-,Speech Recognition,Attention-Based Models for Speech Recognition,/paper/attention-based-models-for-speech-recognition,https://arxiv.org/pdf/1506.07503v1.pdf
CNN / Daily Mail,,# 10,CNN,72.9,Dynamic Entity Repres. + w2v,-,Question Answering,Dynamic Entity Representation with Max-pooling Improves Machine Reading,/paper/dynamic-entity-representation-with-max,https://aclweb.org/anthology/N16-1099
CIFAR-10,,# 11,Inception score,6.0,GMAN,-,Image Generation,Generative Multi-Adversarial Networks,/paper/generative-multi-adversarial-networks,https://arxiv.org/pdf/1611.01673v3.pdf
One Billion Word,,# 2,PPL,23.02,Adaptive Input Very Large,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
One Billion Word,,# 1,Number of params,1.0B,Adaptive Input Very Large,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
One Billion Word,,# 1,Validation perplexity,22.92,Adaptive Input Very Large,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
One Billion Word,,# 4,PPL,23.91,Adaptive Input Large,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
One Billion Word,,# 1,Number of params,0.46B,Adaptive Input Large,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
One Billion Word,,# 2,Validation perplexity,23.83,Adaptive Input Large,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
WikiText-103,,# 1,Validation perplexity,17.97,Transformer with tied adaptive embeddings,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
WikiText-103,,# 3,Test perplexity,18.70,Transformer with tied adaptive embeddings,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
WikiText-103,,# 1,Number of params,247M,Transformer with tied adaptive embeddings,-,Language Modelling,Adaptive Input Representations for Neural Language Modeling,/paper/adaptive-input-representations-for-neural,https://arxiv.org/pdf/1809.10853v3.pdf
MPII Gaze,,# 2,Angular Error,4.6,RT-GENE 2 model ensemble,-,Gaze Estimation,RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments,/paper/rt-gene-real-time-eye-gaze-estimation-in,https://openaccess.thecvf.com/content_ECCV_2018/papers/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.pdf
MPII Gaze,,# 3,Angular Error,4.8,RT-GENE single model,-,Gaze Estimation,RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments,/paper/rt-gene-real-time-eye-gaze-estimation-in,https://openaccess.thecvf.com/content_ECCV_2018/papers/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.pdf
MPII Gaze,,# 1,Angular Error,4.3,RT-GENE 4 model ensemble,-,Gaze Estimation,RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments,/paper/rt-gene-real-time-eye-gaze-estimation-in,https://openaccess.thecvf.com/content_ECCV_2018/papers/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.pdf
RT-GENE,,# 1,Angular Error,7.7,RT-GENE 4 model ensemble,-,Gaze Estimation,RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments,/paper/rt-gene-real-time-eye-gaze-estimation-in,https://openaccess.thecvf.com/content_ECCV_2018/papers/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.pdf
UT Multi-view,,# 1,Angular Error,5.1,RT-GENE 4 model ensemble,-,Gaze Estimation,RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments,/paper/rt-gene-real-time-eye-gaze-estimation-in,https://openaccess.thecvf.com/content_ECCV_2018/papers/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.pdf
SciTail,,# 4,Accuracy,86.0,Hierarchical BiLSTM Max Pooling,-,Natural Language Inference,Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture,/paper/natural-language-inference-with-hierarchical,https://arxiv.org/pdf/1808.08762v1.pdf
SNLI,,# 24,% Test Accuracy,86.6,"600D Hierarchical BiLSTM with Max Pooling (HBMP, code)",-,Natural Language Inference,Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture,/paper/natural-language-inference-with-hierarchical,https://arxiv.org/pdf/1808.08762v1.pdf
SNLI,,# 32,% Train Accuracy,89.9,"600D Hierarchical BiLSTM with Max Pooling (HBMP, code)",-,Natural Language Inference,Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture,/paper/natural-language-inference-with-hierarchical,https://arxiv.org/pdf/1808.08762v1.pdf
SNLI,,# 1,Parameters,22m,"600D Hierarchical BiLSTM with Max Pooling (HBMP, code)",-,Natural Language Inference,Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture,/paper/natural-language-inference-with-hierarchical,https://arxiv.org/pdf/1808.08762v1.pdf
IMDb,,# 8,Accuracy,91.8,BCN+Char+CoVe,-,Sentiment Analysis,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SNLI,,# 15,% Test Accuracy,88.1,Biattentive Classification Network + CoVe + Char,-,Natural Language Inference,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SNLI,,# 39,% Train Accuracy,88.5,Biattentive Classification Network + CoVe + Char,-,Natural Language Inference,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SNLI,,# 1,Parameters,22m,Biattentive Classification Network + CoVe + Char,-,Natural Language Inference,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SQuAD1.1,,# 101,EM,71.3,DCN + Char + CoVe,-,Question Answering,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SQuAD1.1,,# 103,F1,79.9,DCN + Char + CoVe,-,Question Answering,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SST-2 Binary classification,,# 8,Accuracy,90.3,BCN+Char+CoVe,-,Sentiment Analysis,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
SST-5 Fine-grained classification,,# 4,Accuracy,53.7,BCN+Char+CoVe,-,Sentiment Analysis,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
TREC-6,,# 5,Error,4.2,CoVe,-,Text Classification,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word,https://arxiv.org/pdf/1708.00107v2.pdf
BUCC French-to-English,,# 3,F1 score,75.8,Monolingual training data,-,Cross-Lingual Bitext Mining,Improving Neural Machine Translation Models with Monolingual Data,/paper/improving-neural-machine-translation-models,https://arxiv.org/pdf/1511.06709v4.pdf
BUCC German-to-English,,# 3,F1 score,76.9,Monolingual training data,-,Cross-Lingual Bitext Mining,Improving Neural Machine Translation Models with Monolingual Data,/paper/improving-neural-machine-translation-models,https://arxiv.org/pdf/1511.06709v4.pdf
Sentihood,,# 5,Aspect,69.3,LSTM-LOC,-,Aspect-Based Sentiment Analysis,SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,/paper/sentihood-targeted-aspect-based-sentiment,https://arxiv.org/pdf/1610.03771v1.pdf
Sentihood,,# 5,Sentiment,81.9,LSTM-LOC,-,Aspect-Based Sentiment Analysis,SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,/paper/sentihood-targeted-aspect-based-sentiment,https://arxiv.org/pdf/1610.03771v1.pdf
CUB-200-2011,,# 1,Accuracy,89.6,Inception-V3,-,Fine-Grained Image Classification,Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning,/paper/large-scale-fine-grained-categorization-and,https://arxiv.org/pdf/1806.06193v1.pdf
CIFAR-10,,# 13,Percentage correct,96.11,Wide ResNet,-,Image Classification,Wide Residual Networks,/paper/wide-residual-networks,https://arxiv.org/pdf/1605.07146v4.pdf
CIFAR-10,,# 10,Percentage error,3.89,Wide ResNet,-,Image Classification,Wide Residual Networks,/paper/wide-residual-networks,https://arxiv.org/pdf/1605.07146v4.pdf
CIFAR-100,,# 8,Percentage correct,81.15,Wide ResNet,-,Image Classification,Wide Residual Networks,/paper/wide-residual-networks,https://arxiv.org/pdf/1605.07146v4.pdf
CIFAR-100,,# 4,Percentage error,18.85,Wide ResNet,-,Image Classification,Wide Residual Networks,/paper/wide-residual-networks,https://arxiv.org/pdf/1605.07146v4.pdf
SVHN,,# 6,Percentage error,1.7,Wide ResNet,-,Image Classification,Wide Residual Networks,/paper/wide-residual-networks,https://arxiv.org/pdf/1605.07146v4.pdf
Atari 2600 Alien,,# 15,Score,945.3,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Alien,,# 20,Score,518.4,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Alien,,# 21,Score,182.1,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Amidar,,# 17,Score,173.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Amidar,,# 12,Score,263.9,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Amidar,,# 11,Score,283.9,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Assault,,# 16,Score,3746.1,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Assault,,# 11,Score,5474.9,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Assault,,# 2,Score,14497.9,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Asterix,,# 8,Score,22140.5,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Asterix,,# 12,Score,17244.5,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Asterix,,# 15,Score,6723.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Asteroids,,# 2,Score,4474.5,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Asteroids,,# 1,Score,5093.1,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Asteroids,,# 3,Score,3009.4,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Atlantis,,# 7,Score,772392.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Atlantis,,# 4,Score,911091.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Atlantis,,# 5,Score,875822.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Bank Heist,,# 13,Score,932.8,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Bank Heist,,# 11,Score,970.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Bank Heist,,# 12,Score,946.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Battle Zone,,# 15,Score,20760.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Battle Zone,,# 19,Score,12950.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Battle Zone,,# 20,Score,11340.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Beam Rider,,# 5,Score,24622.2,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Beam Rider,,# 8,Score,22707.9,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Beam Rider,,# 13,Score,13235.9,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Berzerk,,# 5,Score,1433.4,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Berzerk,,# 14,Score,817.9,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Berzerk,,# 13,Score,862.2,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Bowling,,# 20,Score,35.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Bowling,,# 17,Score,41.8,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Bowling,,# 19,Score,36.2,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Boxing,,# 20,Score,37.3,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Boxing,,# 17,Score,59.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Boxing,,# 21,Score,33.7,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Breakout,,# 2,Score,766.8,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Breakout,,# 5,Score,681.9,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Breakout,,# 6,Score,551.6,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Centipede,,# 21,Score,3306.5,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Centipede,,# 19,Score,3755.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Centipede,,# 22,Score,1997.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Chopper Command,,# 8,Score,7021.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Chopper Command,,# 5,Score,10150.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Chopper Command,,# 13,Score,4669.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Crazy Climber,,# 6,Score,138518.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Crazy Climber,,# 17,Score,101624.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Crazy Climber,,# 15,Score,112646.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Demon Attack,,# 4,Score,113308.4,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Demon Attack,,# 3,Score,115201.9,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Demon Attack,,# 5,Score,84997.5,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Double Dunk,,# 7,Score,0.1,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Double Dunk,,# 8,Score,-0.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Double Dunk,,# 7,Score,0.1,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Enduro,,# 21,Score,-82.2,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Enduro,,# 22,Score,-82.5,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Enduro,,# 22,Score,-82.5,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Fishing Derby,,# 11,Score,13.6,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Fishing Derby,,# 7,Score,22.6,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Fishing Derby,,# 8,Score,18.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Freeway,,# 22,Score,0.1,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Freeway,,# 22,Score,0.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Freeway,,# 22,Score,0.1,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Frostbite,,# 23,Score,197.6,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Frostbite,,# 26,Score,180.1,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Frostbite,,# 24,Score,190.5,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Gopher,,# 14,Score,10022.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Gopher,,# 17,Score,8442.8,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Gopher,,# 10,Score,17106.8,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Gravitar,,# 14,Score,320.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Gravitar,,# 20,Score,269.5,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Gravitar,,# 16,Score,303.5,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 HERO,,# 4,Score,28765.8,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 HERO,,# 2,Score,32464.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 HERO,,# 3,Score,28889.5,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Ice Hockey,,# 12,Score,-2.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Ice Hockey,,# 8,Score,-1.7,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Ice Hockey,,# 16,Score,-4.7,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 James Bond,,# 12,Score,613.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 James Bond,,# 20,Score,351.5,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 James Bond,,# 16,Score,541.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Kangaroo,,# 21,Score,94.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Kangaroo,,# 19,Score,125.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Kangaroo,,# 20,Score,106.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Krull,,# 10,Score,8066.6,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Krull,,# 19,Score,5560.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Krull,,# 18,Score,5911.4,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Kung-Fu Master,,# 21,Score,3046.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Kung-Fu Master,,# 4,Score,40835.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Kung-Fu Master,,# 14,Score,28819.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Montezuma's Revenge,,# 15,Score,53.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Montezuma's Revenge,,# 19,Score,41.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Montezuma's Revenge,,# 14,Score,67.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Ms. Pacman,,# 19,Score,850.7,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Ms. Pacman,,# 21,Score,594.4,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Ms. Pacman,,# 20,Score,653.7,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Name This Game,,# 7,Score,12093.7,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Name This Game,,# 13,Score,10476.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Name This Game,,# 19,Score,5614.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Pong,,# 11,Score,11.4,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Pong,,# 13,Score,5.6,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Pong,,# 12,Score,10.7,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Private Eye,,# 17,Score,194.4,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Private Eye,,# 10,Score,421.1,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Private Eye,,# 14,Score,206.9,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Q*Bert,,# 3,Score,21307.5,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Q*Bert,,# 8,Score,15148.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Q*Bert,,# 13,Score,13752.3,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 River Raid,,# 14,Score,10001.2,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 River Raid,,# 17,Score,6591.9,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 River Raid,,# 11,Score,12201.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Road Runner,,# 18,Score,31769.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Road Runner,,# 17,Score,34216.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Road Runner,,# 1,Score,73949.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Robotank,,# 21,Score,2.6,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Robotank,,# 15,Score,32.8,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Robotank,,# 22,Score,2.3,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Seaquest,,# 15,Score,2355.4,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Seaquest,,# 16,Score,2300.2,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Seaquest,,# 20,Score,1326.1,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Space Invaders,,# 15,Score,2214.7,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Space Invaders,,# 3,Score,15730.5,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Space Invaders,,# 2,Score,23846.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Star Gunner,,# 1,Score,164766.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Star Gunner,,# 2,Score,138218.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Star Gunner,,# 8,Score,64393.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Tennis,,# 14,Score,-6.3,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Tennis,,# 15,Score,-6.4,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Tennis,,# 17,Score,-10.2,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Time Pilot,,# 1,Score,27202.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Time Pilot,,# 2,Score,12679.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Time Pilot,,# 15,Score,5825.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Tutankham,,# 11,Score,144.2,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Tutankham,,# 22,Score,26.1,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Tutankham,,# 10,Score,156.3,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Up and Down,,# 1,Score,105728.7,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Up and Down,,# 3,Score,74705.7,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Up and Down,,# 5,Score,54525.4,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Venture,,# 23,Score,23.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Venture,,# 22,Score,25.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Venture,,# 25,Score,19.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Video Pinball,,# 13,Score,185852.6,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Video Pinball,,# 8,Score,331628.1,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Video Pinball,,# 5,Score,470310.5,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Wizard of Wor,,# 3,Score,17244.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Wizard of Wor,,# 2,Score,18082.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Wizard of Wor,,# 14,Score,5278.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Zaxxon,,# 21,Score,2659.0,A3C FF (1 day) hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Zaxxon,,# 2,Score,23519.0,A3C LSTM hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
Atari 2600 Zaxxon,,# 1,Score,24622.0,A3C FF hs,-,Atari Games,Asynchronous Methods for Deep Reinforcement Learning,/paper/asynchronous-methods-for-deep-reinforcement,https://arxiv.org/pdf/1602.01783v2.pdf
MAFA,,# 2,MAP,77.3%,AOFD,-,Occluded Face Detection,Adversarial Occlusion-aware Face Detection,/paper/adversarial-occlusion-aware-face-detection,https://arxiv.org/pdf/1709.05188v6.pdf
SemEval 2014 Task 4 Sub Task 2,,# 14,Restaurant (Acc),80.79,TNet,-,Aspect-Based Sentiment Analysis,Transformation Networks for Target-Oriented Sentiment Classification,/paper/transformation-networks-for-target-oriented,https://arxiv.org/pdf/1805.01086v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 21,Laptop (Acc),76.01,TNet,-,Aspect-Based Sentiment Analysis,Transformation Networks for Target-Oriented Sentiment Classification,/paper/transformation-networks-for-target-oriented,https://arxiv.org/pdf/1805.01086v1.pdf
MSRA,,# 1,F1,97.4,Pre-trained+bigram+ LSTM+CRF,-,Chinese Word Segmentation,Long Short-Term Memory Neural Networks for Chinese Word Segmentation,/paper/long-short-term-memory-neural-networks-for,https://aclweb.org/anthology/D15-1141
CIFAR-10,,# 34,Percentage correct,91.7,BinaryConnect,-,Image Classification,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,/paper/binaryconnect-training-deep-neural-networks-1,https://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf
MNIST,,# 10,Percentage error,1.0,BinaryConnect,-,Image Classification,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,/paper/binaryconnect-training-deep-neural-networks-1,https://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf
SVHN,,# 15,Percentage error,2.15,BinaryConnect,-,Image Classification,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,/paper/binaryconnect-training-deep-neural-networks-1,https://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf
Switchboard + Hub500,,# 8,Percentage error,8.0,IBM 2015,-,Speech Recognition,The IBM 2015 English Conversational Telephone Speech Recognition System,/paper/the-ibm-2015-english-conversational-telephone,https://arxiv.org/pdf/1505.05899v1.pdf
Twitter Dialogue (Noun),,# 1,Precision,4.82,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Twitter Dialogue (Noun),,# 1,Recall,5.22,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Twitter Dialogue (Noun),,# 1,F1,4.63,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Twitter Dialogue (Tense),,# 1,Accuracy,34.48%,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Activity),,# 1,Precision,16.84,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Activity),,# 1,Recall,9.72,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Activity),,# 1,F1,11.43,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Cmd),,# 1,Accuracy,95.04%,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Entity),,# 1,Precision,4.91,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Entity),,# 1,Recall,3.36,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Entity),,# 1,F1,3.72,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
Ubuntu Dialogue (Tense),,# 1,Accuracy,29.01%,MrRNN Act.-Ent.,-,Dialogue Generation,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation,/paper/multiresolution-recurrent-neural-networks-an,https://arxiv.org/pdf/1606.00776v2.pdf
ImageNet,,# 18,Top 1 Accuracy,75.7%,LR-Net-26,-,Image Classification,Local Relation Networks for Image Recognition,/paper/local-relation-networks-for-image-recognition,https://arxiv.org/pdf/1904.11491.pdf
ImageNet,,# 13,Top 5 Accuracy,92.6%,LR-Net-26,-,Image Classification,Local Relation Networks for Image Recognition,/paper/local-relation-networks-for-image-recognition,https://arxiv.org/pdf/1904.11491.pdf
DukeMTMC-reID,,# 16,Rank-1,67.68,GAN,-,Person Re-Identification,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,/paper/unlabeled-samples-generated-by-gan-improve,https://arxiv.org/pdf/1701.07717v5.pdf
DukeMTMC-reID,,# 16,MAP,47.13,GAN,-,Person Re-Identification,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,/paper/unlabeled-samples-generated-by-gan-improve,https://arxiv.org/pdf/1701.07717v5.pdf
Market-1501,,# 15,Rank-1,83.97,GAN,-,Person Re-Identification,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,/paper/unlabeled-samples-generated-by-gan-improve,https://arxiv.org/pdf/1701.07717v5.pdf
Market-1501,,# 13,MAP,66.07,GAN,-,Person Re-Identification,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,/paper/unlabeled-samples-generated-by-gan-improve,https://arxiv.org/pdf/1701.07717v5.pdf
E2E NLG Challenge,,# 5,BLEU,65.45,Zhang,-,Data-to-Text Generation,Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge,/paper/attention-regularized-sequence-to-sequence,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Zhang.pdf
E2E NLG Challenge,,# 6,NIST,8.1804,Zhang,-,Data-to-Text Generation,Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge,/paper/attention-regularized-sequence-to-sequence,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Zhang.pdf
E2E NLG Challenge,,# 7,METEOR,43.92,Zhang,-,Data-to-Text Generation,Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge,/paper/attention-regularized-sequence-to-sequence,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Zhang.pdf
E2E NLG Challenge,,# 1,ROUGE-L,70.83,Zhang,-,Data-to-Text Generation,Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge,/paper/attention-regularized-sequence-to-sequence,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Zhang.pdf
E2E NLG Challenge,,# 5,CIDEr,2.1012,Zhang,-,Data-to-Text Generation,Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge,/paper/attention-regularized-sequence-to-sequence,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Zhang.pdf
BSD100 - 4x upscaling,,# 21,PSNR,27.21,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
BSD100 - 4x upscaling,,# 2,SSIM,0.7493,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
BSD100 - 4x upscaling,,# 3,MOS,2.12,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
Set14 - 4x upscaling,,# 23,PSNR,28.02,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
Set14 - 4x upscaling,,# 2,SSIM,0.8074,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
Set14 - 4x upscaling,,# 3,MOS,2.84,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
Set5 - 4x upscaling,,# 16,PSNR,31.52,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
Set5 - 4x upscaling,,# 11,SSIM,0.8938,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
Set5 - 4x upscaling,,# 3,MOS,3.26,DRCN,-,Image Super-Resolution,Deeply-Recursive Convolutional Network for Image Super-Resolution,/paper/deeply-recursive-convolutional-network-for,https://arxiv.org/pdf/1511.04491v2.pdf
GTAV-to-Cityscapes Labels,,# 6,mIoU,28.9,CDA,-,Synthetic-to-Real Translation,Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes,/paper/curriculum-domain-adaptation-for-semantic,https://arxiv.org/pdf/1707.09465v5.pdf
CIFAR-10,,# 34,Percentage correct,91.7,CLS-GAN,-,Image Classification,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,/paper/loss-sensitive-generative-adversarial,https://arxiv.org/pdf/1701.06264v6.pdf
SVHN,,# 24,Percentage error,5.98,CLS-GAN,-,Image Classification,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,/paper/loss-sensitive-generative-adversarial,https://arxiv.org/pdf/1701.06264v6.pdf
MNIST,,# 4,Accuracy,95.73,CatGAN,-,Unsupervised MNIST,Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks,/paper/unsupervised-and-semi-supervised-learning,https://arxiv.org/pdf/1511.06390v2.pdf
MNIST,,# 4,Accuracy,95.73,CatGAN,-,Unsupervised image classification,Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks,/paper/unsupervised-and-semi-supervised-learning,https://arxiv.org/pdf/1511.06390v2.pdf
20NEWS,,# 1,Accuracy,88.5,SGCN,-,Text Classification,Simplifying Graph Convolutional Networks,/paper/simplifying-graph-convolutional-networks,https://arxiv.org/pdf/1902.07153v1.pdf
MR,,# 10,Accuracy,75.9,SGCN,-,Sentiment Analysis,Simplifying Graph Convolutional Networks,/paper/simplifying-graph-convolutional-networks,https://arxiv.org/pdf/1902.07153v1.pdf
Ohsumed,,# 1,Accuracy,68.5,SGCN,-,Text Classification,Simplifying Graph Convolutional Networks,/paper/simplifying-graph-convolutional-networks,https://arxiv.org/pdf/1902.07153v1.pdf
R52,,# 1,Accuracy,94.0,SGCN,-,Text Classification,Simplifying Graph Convolutional Networks,/paper/simplifying-graph-convolutional-networks,https://arxiv.org/pdf/1902.07153v1.pdf
R8,,# 1,Accuracy,97.2,SGCN,-,Text Classification,Simplifying Graph Convolutional Networks,/paper/simplifying-graph-convolutional-networks,https://arxiv.org/pdf/1902.07153v1.pdf
TACRED,,# 2,F1,67.0,C-SGC,-,Relation Extraction,Simplifying Graph Convolutional Networks,/paper/simplifying-graph-convolutional-networks,https://arxiv.org/pdf/1902.07153v1.pdf
CAT 256x256,,# 2,FID,155.46,WGAN-GP,-,Image Generation,Improved Training of Wasserstein GANs,/paper/improved-training-of-wasserstein-gans,https://arxiv.org/pdf/1704.00028v3.pdf
CIFAR-10,,# 6,Inception score,7.86,WGAN-GP,-,Image Generation,Improved Training of Wasserstein GANs,/paper/improved-training-of-wasserstein-gans,https://arxiv.org/pdf/1704.00028v3.pdf
CIFAR-10,,# 8,FID,29.3,WGAN-GP,-,Image Generation,Improved Training of Wasserstein GANs,/paper/improved-training-of-wasserstein-gans,https://arxiv.org/pdf/1704.00028v3.pdf
CIFAR-10,,# 2,Inception score,8.67,WGAN-GP,-,Conditional Image Generation,Improved Training of Wasserstein GANs,/paper/improved-training-of-wasserstein-gans,https://arxiv.org/pdf/1704.00028v3.pdf
FB15k,,# 1,MRR,0.86,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
FB15k,,# 1,[emailÂ protected],0.91,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
FB15k-237,,# 1,MRR,0.37,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
FB15k-237,,# 1,[emailÂ protected],0.56,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
WN18,,# 4,MRR,0.95,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
WN18,,# 2,[emailÂ protected],0.96,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
WN18RR,,# 2,MRR,0.48,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
WN18RR,,# 2,[emailÂ protected],0.57,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
YAGO3-10,,# 1,MRR,0.58,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
YAGO3-10,,# 1,[emailÂ protected],0.71,ComplEx-N3 (reciprocal),-,Link Prediction,Canonical Tensor Decomposition for Knowledge Base Completion,/paper/canonical-tensor-decomposition-for-knowledge,https://arxiv.org/pdf/1806.07297v1.pdf
MultiNLI,,# 4,Matched,72.2,Multi-task BiLSTM + Attn,-,Natural Language Inference,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,/paper/glue-a-multi-task-benchmark-and-analysis,https://arxiv.org/pdf/1804.07461v3.pdf
MultiNLI,,# 4,Mismatched,72.1,Multi-task BiLSTM + Attn,-,Natural Language Inference,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,/paper/glue-a-multi-task-benchmark-and-analysis,https://arxiv.org/pdf/1804.07461v3.pdf
PASCAL VOC 2007,,# 14,MAP,73.2%,Faster R-CNN,-,Object Detection,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,/paper/faster-r-cnn-towards-real-time-object,https://arxiv.org/pdf/1506.01497v3.pdf
PASCAL VOC 2007,,# 5,MAP,73.2%,Faster R-CNN,-,Real-Time Object Detection,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,/paper/faster-r-cnn-towards-real-time-object,https://arxiv.org/pdf/1506.01497v3.pdf
PASCAL VOC 2007,,# 6,FPS,7,Faster R-CNN,-,Real-Time Object Detection,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,/paper/faster-r-cnn-towards-real-time-object,https://arxiv.org/pdf/1506.01497v3.pdf
AG News,,# 5,Error,6.87,DPCNN,-,Text Classification,Deep Pyramid Convolutional Neural Networks for Text Categorization,/paper/deep-pyramid-convolutional-neural-networks,https://aclweb.org/anthology/P17-1052
Amazon Review Full,,# 2,Accuracy,65.19,DPCNN,-,Sentiment Analysis,Deep Pyramid Convolutional Neural Networks for Text Categorization,/paper/deep-pyramid-convolutional-neural-networks,https://aclweb.org/anthology/P17-1052
Amazon Review Polarity,,# 2,Accuracy,96.68,DPCNN,-,Sentiment Analysis,Deep Pyramid Convolutional Neural Networks for Text Categorization,/paper/deep-pyramid-convolutional-neural-networks,https://aclweb.org/anthology/P17-1052
DBpedia,,# 6,Error,0.88,DPCNN,-,Text Classification,Deep Pyramid Convolutional Neural Networks for Text Categorization,/paper/deep-pyramid-convolutional-neural-networks,https://aclweb.org/anthology/P17-1052
Yelp Binary classification,,# 4,Error,2.64,DPCNN,-,Sentiment Analysis,Deep Pyramid Convolutional Neural Networks for Text Categorization,/paper/deep-pyramid-convolutional-neural-networks,https://aclweb.org/anthology/P17-1052
Yelp Fine-grained classification,,# 3,Error,30.58,DPCNN,-,Sentiment Analysis,Deep Pyramid Convolutional Neural Networks for Text Categorization,/paper/deep-pyramid-convolutional-neural-networks,https://aclweb.org/anthology/P17-1052
CamVid,,# 2,Global Accuracy,88.7%,ReSeg,-,Semantic Segmentation,ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation,/paper/reseg-a-recurrent-neural-network-based-model,https://arxiv.org/pdf/1511.07053v3.pdf
CamVid,,# 6,Mean IoU,58.8%,ReSeg,-,Semantic Segmentation,ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation,/paper/reseg-a-recurrent-neural-network-based-model,https://arxiv.org/pdf/1511.07053v3.pdf
SNLI,,# 25,% Test Accuracy,86.5,Densely-Connected Recurrent and Co-Attentive Network (encoder),-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 25,% Train Accuracy,91.4,Densely-Connected Recurrent and Co-Attentive Network (encoder),-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 1,Parameters,5.6m,Densely-Connected Recurrent and Co-Attentive Network (encoder),-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 9,% Test Accuracy,88.9,Densely-Connected Recurrent and Co-Attentive Network,-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 18,% Train Accuracy,93.1,Densely-Connected Recurrent and Co-Attentive Network,-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 1,Parameters,6.7m,Densely-Connected Recurrent and Co-Attentive Network,-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 3,% Test Accuracy,90.1,Densely-Connected Recurrent and Co-Attentive Network Ensemble,-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 9,% Train Accuracy,95.0,Densely-Connected Recurrent and Co-Attentive Network Ensemble,-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 1,Parameters,53.3m,Densely-Connected Recurrent and Co-Attentive Network Ensemble,-,Natural Language Inference,Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information,/paper/semantic-sentence-matching-with-densely,https://arxiv.org/pdf/1805.11360v2.pdf
SNLI,,# 4,% Test Accuracy,89.9,Fine-Tuned LM-Pretrained Transformer,-,Natural Language Inference,Improving Language Understanding by Generative Pre-Training,/paper/improving-language-understanding-by,https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
SNLI,,# 4,% Train Accuracy,96.6,Fine-Tuned LM-Pretrained Transformer,-,Natural Language Inference,Improving Language Understanding by Generative Pre-Training,/paper/improving-language-understanding-by,https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
SNLI,,# 1,Parameters,85m,Fine-Tuned LM-Pretrained Transformer,-,Natural Language Inference,Improving Language Understanding by Generative Pre-Training,/paper/improving-language-understanding-by,https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
YouTube Faces DB,,# 4,Accuracy,96.17%,QAN,-,Face Verification,Quality Aware Network for Set to Set Recognition,/paper/quality-aware-network-for-set-to-set,https://arxiv.org/pdf/1704.03373v1.pdf
Mini-ImageNet - 5-Shot Learning,,# 6,Accuracy,66.43%,Baseline++,-,Few-Shot Image Classification,A Closer Look at Few-shot Classification,/paper/a-closer-look-at-few-shot-classification,https://openreview.net/pdf?id=HkxLXnAcFQ
ADE20K Labels-to-Photos,,# 3,mIoU,20.3,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
ADE20K Labels-to-Photos,,# 2,Accuracy,69.2%,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
ADE20K Labels-to-Photos,,# 3,FID,81.8,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
ADE20K-Outdoor Labels-to-Photos,,# 2,mIoU,17.4,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
ADE20K-Outdoor Labels-to-Photos,,# 3,Accuracy,71.6%,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
ADE20K-Outdoor Labels-to-Photos,,# 3,FID,97.8,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
Cityscapes Labels-to-Photo,,# 6,Class IOU,,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
Cityscapes Labels-to-Photo,,# 5,Per-class Accuracy,,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
Cityscapes Labels-to-Photo,,# 2,Per-pixel Accuracy,81.4%,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
Cityscapes Labels-to-Photo,,# 2,mIoU,58.3,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
Cityscapes Labels-to-Photo,,# 3,FID,95,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
COCO-Stuff Labels-to-Photos,,# 3,mIoU,14.6,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
COCO-Stuff Labels-to-Photos,,# 2,Accuracy,45.8%,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
COCO-Stuff Labels-to-Photos,,# 3,FID,111.5,pix2pixHD,-,Image-to-Image Translation,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,/paper/high-resolution-image-synthesis-and-semantic,https://arxiv.org/pdf/1711.11585v2.pdf
WMT2014 English-French,,# 12,BLEU score,39.92,GNMT+RL,-,Machine Translation,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,/paper/googles-neural-machine-translation-system,https://arxiv.org/pdf/1609.08144v2.pdf
WMT2014 English-German,,# 10,BLEU score,26.3,GNMT+RL,-,Machine Translation,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,/paper/googles-neural-machine-translation-system,https://arxiv.org/pdf/1609.08144v2.pdf
MPII Multi-Person,,# 1,AP,82.1%,Regional Multi-Person Pose Estimation,-,Multi-Person Pose Estimation,RMPE: Regional Multi-person Pose Estimation,/paper/rmpe-regional-multi-person-pose-estimation,https://arxiv.org/pdf/1612.00137v5.pdf
SQuAD1.1,,# 15,EM,82.28299999999999,Reinforced Mnemonic Reader (ensemble model),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 16,F1,88.53299999999999,Reinforced Mnemonic Reader (ensemble model),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 34,EM,79.545,Reinforced Mnemonic Reader (single model),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 32,F1,86.654,Reinforced Mnemonic Reader (single model),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 103,EM,70.995,Mnemonic Reader (single model),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 100,F1,80.146,Mnemonic Reader (single model),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 78,EM,74.268,Mnemonic Reader (ensemble),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
SQuAD1.1,,# 79,F1,82.37100000000001,Mnemonic Reader (ensemble),-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
TriviaQA,,# 4,EM,46.94,Mnemonic Reader,-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
TriviaQA,,# 4,F1,52.85,Mnemonic Reader,-,Question Answering,Reinforced Mnemonic Reader for Machine Reading Comprehension,/paper/reinforced-mnemonic-reader-for-machine,https://arxiv.org/pdf/1705.02798v6.pdf
CUB,,# 3,FID,67.22,GAWWN,-,Text-to-Image Generation,Learning What and Where to Draw,/paper/learning-what-and-where-to-draw,https://arxiv.org/pdf/1610.02454v1.pdf
CUB,,# 4,Inception score,3.62,GAWWN,-,Text-to-Image Generation,Learning What and Where to Draw,/paper/learning-what-and-where-to-draw,https://arxiv.org/pdf/1610.02454v1.pdf
Hutter Prize,,# 4,Bit per Character (BPC),1.08,mLSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
Hutter Prize,,# 1,Number of params,46M,mLSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
Penn Treebank (Word Level),,# 5,Validation perplexity,51.6,AWD-LSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
Penn Treebank (Word Level),,# 6,Test perplexity,51.1,AWD-LSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
Penn Treebank (Word Level),,# 1,Params,24M,AWD-LSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
Text8,,# 6,Bit per Character (BPC),1.19,mLSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
Text8,,# 1,Number of params,45M,mLSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
WikiText-2,,# 4,Validation perplexity,46.4,AWD-LSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
WikiText-2,,# 5,Test perplexity,44.3,AWD-LSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
WikiText-2,,# 1,Number of params,33M,AWD-LSTM + dynamic eval,-,Language Modelling,Dynamic Evaluation of Neural Sequence Models,/paper/dynamic-evaluation-of-neural-sequence-models,https://arxiv.org/pdf/1709.07432v2.pdf
MIMIC-III,,# 8,F1 score,0.7,Linear SVM,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 5,Precision,0.8,Linear SVM,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 8,Recall,0.63,Linear SVM,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 7,F1 score,0.71,Linear Discriminant,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 6,Precision,0.78,Linear Discriminant,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 7,Recall,0.66,Linear Discriminant,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 5,F1 score,0.82,K-NN,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 5,Precision,0.8,K-NN,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 4,Recall,0.85,K-NN,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 6,F1 score,0.72,Logistic regression,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 7,Precision,0.77,Logistic regression,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 6,Recall,0.67,Logistic regression,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 1,F1 score,0.97,Random Forest,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 1,Precision,0.97,Random Forest,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 1,Recall,0.97,Random Forest,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 2,F1 score,0.96,Gaussian SVM,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 2,Precision,0.95,Gaussian SVM,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 2,Recall,0.96,Gaussian SVM,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 3,F1 score,0.91,Decision Tree,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 4,Precision,0.9,Decision Tree,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 3,Recall,0.92,Decision Tree,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 4,F1 score,0.87,Boosted Trees,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 3,Precision,0.91,Boosted Trees,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MIMIC-III,,# 5,Recall,0.83,Boosted Trees,-,Mortality Prediction,Early hospital mortality prediction using vital signals,/paper/early-hospital-mortality-prediction-using,https://arxiv.org/pdf/1803.06589v2.pdf
MultiNLI,,# 3,Matched,73.9,aESIM,-,Natural Language Inference,Attention Boosted Sequential Inference Model,/paper/attention-boosted-sequential-inference-model,https://arxiv.org/pdf/1812.01840v2.pdf
MultiNLI,,# 3,Mismatched,73.9,aESIM,-,Natural Language Inference,Attention Boosted Sequential Inference Model,/paper/attention-boosted-sequential-inference-model,https://arxiv.org/pdf/1812.01840v2.pdf
Quora Question Pairs,,# 1,Accuracy,88.01,aESIM,-,Natural Language Inference,Attention Boosted Sequential Inference Model,/paper/attention-boosted-sequential-inference-model,https://arxiv.org/pdf/1812.01840v2.pdf
SNLI,,# 15,% Test Accuracy,88.1,aESIM,-,Natural Language Inference,Attention Boosted Sequential Inference Model,/paper/attention-boosted-sequential-inference-model,https://arxiv.org/pdf/1812.01840v2.pdf
SNLI,,# 30,% Test Accuracy,85.7,300D Residual stacked encoders,-,Natural Language Inference,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,/paper/shortcut-stacked-sentence-encoders-for-multi,https://arxiv.org/pdf/1708.02312v2.pdf
SNLI,,# 33,% Train Accuracy,89.8,300D Residual stacked encoders,-,Natural Language Inference,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,/paper/shortcut-stacked-sentence-encoders-for-multi,https://arxiv.org/pdf/1708.02312v2.pdf
SNLI,,# 1,Parameters,9.7m,300D Residual stacked encoders,-,Natural Language Inference,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,/paper/shortcut-stacked-sentence-encoders-for-multi,https://arxiv.org/pdf/1708.02312v2.pdf
SNLI,,# 28,% Test Accuracy,86.0,600D Residual stacked encoders,-,Natural Language Inference,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,/paper/shortcut-stacked-sentence-encoders-for-multi,https://arxiv.org/pdf/1708.02312v2.pdf
SNLI,,# 28,% Train Accuracy,91.0,600D Residual stacked encoders,-,Natural Language Inference,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,/paper/shortcut-stacked-sentence-encoders-for-multi,https://arxiv.org/pdf/1708.02312v2.pdf
SNLI,,# 1,Parameters,29m,600D Residual stacked encoders,-,Natural Language Inference,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,/paper/shortcut-stacked-sentence-encoders-for-multi,https://arxiv.org/pdf/1708.02312v2.pdf
PASCAL VOC 2007,,# 5,MAP,81.5%,BlitzNet512 + seg (s8),-,Object Detection,BlitzNet: A Real-Time Deep Network for Scene Understanding,/paper/blitznet-a-real-time-deep-network-for-scene,https://arxiv.org/pdf/1708.02813v1.pdf
PASCAL VOC 2007,,# 1,MAP,81.5%,BlitzNet512 (s8),-,Real-Time Object Detection,BlitzNet: A Real-Time Deep Network for Scene Understanding,/paper/blitznet-a-real-time-deep-network-for-scene,https://arxiv.org/pdf/1708.02813v1.pdf
PASCAL VOC 2007,,# 4,FPS,19.5,BlitzNet512 (s8),-,Real-Time Object Detection,BlitzNet: A Real-Time Deep Network for Scene Understanding,/paper/blitznet-a-real-time-deep-network-for-scene,https://arxiv.org/pdf/1708.02813v1.pdf
PASCAL VOC 2007,,# 3,MAP,79.1%,BlitzNet512 (s4),-,Real-Time Object Detection,BlitzNet: A Real-Time Deep Network for Scene Understanding,/paper/blitznet-a-real-time-deep-network-for-scene,https://arxiv.org/pdf/1708.02813v1.pdf
PASCAL VOC 2007,,# 3,FPS,24,BlitzNet512 (s4),-,Real-Time Object Detection,BlitzNet: A Real-Time Deep Network for Scene Understanding,/paper/blitznet-a-real-time-deep-network-for-scene,https://arxiv.org/pdf/1708.02813v1.pdf
One Billion Word,,# 5,PPL,24.0,Mesh Tensorflow,-,Language Modelling,Mesh-TensorFlow: Deep Learning for Supercomputers,/paper/mesh-tensorflow-deep-learning-for,https://arxiv.org/pdf/1811.02084v1.pdf
One Billion Word,,# 1,Number of params,4.9B,Mesh Tensorflow,-,Language Modelling,Mesh-TensorFlow: Deep Learning for Supercomputers,/paper/mesh-tensorflow-deep-learning-for,https://arxiv.org/pdf/1811.02084v1.pdf
Caltech,,# 7,Reasonable Miss Rate,6.1,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
Caltech,,# 4,Reasonable Miss Rate,4.5,ALFNet + CityPersons dataset,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 2,Reasonable MR^-2,12.0,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 2,Heavy MR^-2,51.9,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 2,Partial MR^-2,11.4,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 4,Bare MR^-2,8.4,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 2,Small MR^-2,19.0,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 2,Medium MR^-2,5.7,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 2,Large MR^-2,6.6,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
CityPersons,,# 1,Test Time,0.27,ALFNet,-,Pedestrian Detection,Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting,/paper/learning-efficient-single-stage-pedestrian,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf
Leeds Sports Poses,,# 1,PCK,93.9%,Stacked hourglass + Inception-resnet,-,Pose Estimation,Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation,/paper/knowledge-guided-deep-fractal-neural-networks,https://arxiv.org/pdf/1705.02407v2.pdf
MPII Human Pose,,# 4,PCKh-0.5,91.2%,Stacked hourglass + Inception-resnet,-,Pose Estimation,Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation,/paper/knowledge-guided-deep-fractal-neural-networks,https://arxiv.org/pdf/1705.02407v2.pdf
IC15,,# 8,F-Measure,78.16%,WordSup,-,Scene Text Detection,WordSup: Exploiting Word Annotations for Character based Text Detection,/paper/wordsup-exploiting-word-annotations-for,https://arxiv.org/pdf/1708.06720v1.pdf
ADE20K,,# 6,Validation mIoU,29.39,FCN,-,Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic-1,https://arxiv.org/pdf/1605.06211v1.pdf
PASCAL Context,,# 13,mIoU,37.8,FCN-8s,-,Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic-1,https://arxiv.org/pdf/1605.06211v1.pdf
Cityscapes,,# 7,mIoU,65.3%,FCN,-,Real-Time Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic,https://arxiv.org/pdf/1605.06211v1.pdf
Cityscapes,,# 5,Time (ms),500,FCN,-,Real-Time Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic,https://arxiv.org/pdf/1605.06211v1.pdf
Cityscapes,,# 7,Frame (fps),2,FCN,-,Real-Time Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic,https://arxiv.org/pdf/1605.06211v1.pdf
Cityscapes,,# 14,Mean IoU,65.3%,FCN,-,Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic,https://arxiv.org/pdf/1605.06211v1.pdf
PASCAL VOC 2012,,# 18,Mean IoU,67.2%,FCN,-,Semantic Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic,https://arxiv.org/pdf/1605.06211v1.pdf
SUN-RGBD,,# 3,Mean IoU,27.39,FCN,-,Scene Segmentation,Fully Convolutional Networks for Semantic Segmentation,/paper/fully-convolutional-networks-for-semantic,https://arxiv.org/pdf/1605.06211v1.pdf
WOS-11967,,# 2,Accuracy,86.07,HDLTex,-,Document Classification,HDLTex: Hierarchical Deep Learning for Text Classification,/paper/hdltex-hierarchical-deep-learning-for-text,https://arxiv.org/pdf/1709.08267v2.pdf
WOS-46985,,# 2,Accuracy,76.58,HDLTex,-,Document Classification,HDLTex: Hierarchical Deep Learning for Text Classification,/paper/hdltex-hierarchical-deep-learning-for-text,https://arxiv.org/pdf/1709.08267v2.pdf
WOS-5736,,# 2,Accuracy,90.93,HDLTex,-,Document Classification,HDLTex: Hierarchical Deep Learning for Text Classification,/paper/hdltex-hierarchical-deep-learning-for-text,https://arxiv.org/pdf/1709.08267v2.pdf
ImageCLEF-DA,,# 1,Accuracy,89.0,MEDA,-,Domain Adaptation,Visual Domain Adaptation with Manifold Embedded Distribution Alignment,/paper/visual-domain-adaptation-with-manifold,https://arxiv.org/pdf/1807.07258v2.pdf
ImageCLEF-DA,,# 1,Accuracy,89.0,MEDA,-,Unsupervised Domain Adaptation,Visual Domain Adaptation with Manifold Embedded Distribution Alignment,/paper/visual-domain-adaptation-with-manifold,https://arxiv.org/pdf/1807.07258v2.pdf
CCGBank,,# 3,Accuracy,94.24,Vaswani et al.,-,CCG Supertagging,Supertagging With LSTMs,/paper/supertagging-with-lstms,https://aclweb.org/anthology/N16-1027
Penn Treebank,,# 9,Accuracy,97.4,Feed Forward,-,Part-Of-Speech Tagging,Supertagging With LSTMs,/paper/supertagging-with-lstms,https://aclweb.org/anthology/N16-1027
WikiHop,,# 1,Test,70.6,CFC,-,Question Answering,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,/paper/coarse-grain-fine-grain-coattention-network,https://arxiv.org/pdf/1901.00603v1.pdf
COCO,,# 5,Average Precision,25.0%,MultiPath Network,-,Instance Segmentation,A MultiPath Network for Object Detection,/paper/a-multipath-network-for-object-detection,https://arxiv.org/pdf/1604.02135v2.pdf
COCO,,# 34,Bounding Box AP,33.2,MultiPath Network,-,Object Detection,A MultiPath Network for Object Detection,/paper/a-multipath-network-for-object-detection,https://arxiv.org/pdf/1604.02135v2.pdf
IC15,,# 2,F-Measure,87.08%,PSENet-1s,-,Scene Text Detection,Shape Robust Text Detection with Progressive Scale Expansion Network,/paper/shape-robust-text-detection-with-progressive,https://arxiv.org/pdf/1806.02559v1.pdf
IC17-MLT,,# 1,F-Measure,72.45%,PSENet-1s,-,Scene Text Detection,Shape Robust Text Detection with Progressive Scale Expansion Network,/paper/shape-robust-text-detection-with-progressive,https://arxiv.org/pdf/1806.02559v1.pdf
SCUT-CTW1500,,# 1,F-Measure,81.17%,PSENet-1s,-,Curved Text Detection,Shape Robust Text Detection with Progressive Scale Expansion Network,/paper/shape-robust-text-detection-with-progressive,https://arxiv.org/pdf/1806.02559v1.pdf
WikiSQL,,# 1,BLEU-4,38.97,Graph2Seq-PGE,-,SQL-to-Text,Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks,/paper/graph2seq-graph-to-sequence-learning-with,https://arxiv.org/pdf/1804.00823v4.pdf
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 3,Percentage correct,66.33,MRN,-,Visual Question Answering,Multimodal Residual Learning for Visual QA,/paper/multimodal-residual-learning-for-visual-qa,https://arxiv.org/pdf/1606.01455v2.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 5,Percentage correct,61.84,MRN + global features,-,Visual Question Answering,Multimodal Residual Learning for Visual QA,/paper/multimodal-residual-learning-for-visual-qa,https://arxiv.org/pdf/1606.01455v2.pdf
VOT2017/18,,# 9,Expected Average Overlap (EAO),0.244,SiamRPN,-,Visual Object Tracking,High Performance Visual Tracking With Siamese Region Proposal Network,/paper/high-performance-visual-tracking-with-siamese,https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf
ECCV HotOrNot,,# 1,Pearson Correlation,0.46799999999999997,CNN features + Bayesian ridge regression,-,Facial Beauty Prediction,Transferring Rich Deep Features for Facial Beauty Prediction,/paper/transferring-rich-deep-features-for-facial,https://arxiv.org/pdf/1803.07253v1.pdf
SCUT-FBP,,# 1,MAE,0.2595,CNN features + Bayesian ridge regression,-,Facial Beauty Prediction,Transferring Rich Deep Features for Facial Beauty Prediction,/paper/transferring-rich-deep-features-for-facial,https://arxiv.org/pdf/1803.07253v1.pdf
Cityscapes,,# 4,Mean IoU,81.5%,Dual Attention Network,-,Semantic Segmentation,Dual Attention Network for Scene Segmentation,/paper/dual-attention-network-for-scene-segmentation,https://arxiv.org/pdf/1809.02983v4.pdf
PASCAL Context,,# 2,mIoU,52.6,Dual Attention Network,-,Semantic Segmentation,Dual Attention Network for Scene Segmentation,/paper/dual-attention-network-for-scene-segmentation,https://arxiv.org/pdf/1809.02983v4.pdf
Multi-Domain Sentiment Dataset,,# 3,DVD,76.57,VFAE,-,Sentiment Analysis,The Variational Fair Autoencoder,/paper/the-variational-fair-autoencoder,https://arxiv.org/pdf/1511.00830v6.pdf
Multi-Domain Sentiment Dataset,,# 3,Books,73.4,VFAE,-,Sentiment Analysis,The Variational Fair Autoencoder,/paper/the-variational-fair-autoencoder,https://arxiv.org/pdf/1511.00830v6.pdf
Multi-Domain Sentiment Dataset,,# 2,Electronics,80.53,VFAE,-,Sentiment Analysis,The Variational Fair Autoencoder,/paper/the-variational-fair-autoencoder,https://arxiv.org/pdf/1511.00830v6.pdf
Multi-Domain Sentiment Dataset,,# 3,Kitchen,82.93,VFAE,-,Sentiment Analysis,The Variational Fair Autoencoder,/paper/the-variational-fair-autoencoder,https://arxiv.org/pdf/1511.00830v6.pdf
Multi-Domain Sentiment Dataset,,# 4,Average,78.36,VFAE,-,Sentiment Analysis,The Variational Fair Autoencoder,/paper/the-variational-fair-autoencoder,https://arxiv.org/pdf/1511.00830v6.pdf
AFLW2000-3D,,# 3,Mean NME,4.49%,3DSTN,-,Face Alignment,Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses,/paper/faster-than-real-time-facial-alignment-a-3d,https://arxiv.org/pdf/1707.05653v2.pdf
SQuAD1.1,,# 88,EM,73.01,jNet (ensemble),-,Question Answering,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,/paper/exploring-question-understanding-and,https://arxiv.org/pdf/1703.04617v2.pdf
SQuAD1.1,,# 88,F1,81.517,jNet (ensemble),-,Question Answering,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,/paper/exploring-question-understanding-and,https://arxiv.org/pdf/1703.04617v2.pdf
SQuAD1.1,,# 108,EM,70.607,jNet (single model),-,Question Answering,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,/paper/exploring-question-understanding-and,https://arxiv.org/pdf/1703.04617v2.pdf
SQuAD1.1,,# 105,F1,79.821,jNet (single model),-,Question Answering,Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering,/paper/exploring-question-understanding-and,https://arxiv.org/pdf/1703.04617v2.pdf
Children's Book Test,,# 7,Accuracy-CN,67.4%,EpiReader,-,Question Answering,Natural Language Comprehension with the EpiReader,/paper/natural-language-comprehension-with-the,https://arxiv.org/pdf/1606.02270v2.pdf
Children's Book Test,,# 8,Accuracy-NE,69.7%,EpiReader,-,Question Answering,Natural Language Comprehension with the EpiReader,/paper/natural-language-comprehension-with-the,https://arxiv.org/pdf/1606.02270v2.pdf
CNN / Daily Mail,,# 9,CNN,74,EpiReader,-,Question Answering,Natural Language Comprehension with the EpiReader,/paper/natural-language-comprehension-with-the,https://arxiv.org/pdf/1606.02270v2.pdf
MR,,# 6,Accuracy,78.26,GRU-RNN-WORD2VEC,-,Sentiment Analysis,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,/paper/all-but-the-top-simple-and-effective,https://arxiv.org/pdf/1702.01417v2.pdf
SST-5 Fine-grained classification,,# 14,Accuracy,45.02,GRU-RNN-WORD2VEC,-,Sentiment Analysis,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,/paper/all-but-the-top-simple-and-effective,https://arxiv.org/pdf/1702.01417v2.pdf
SUBJ,,# 8,Accuracy,91.85,GRU-RNN-GLOVE,-,Subjectivity Analysis,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,/paper/all-but-the-top-simple-and-effective,https://arxiv.org/pdf/1702.01417v2.pdf
TREC-6,,# 7,Error,7.0,GRU-RNN-GLOVE,-,Text Classification,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,/paper/all-but-the-top-simple-and-effective,https://arxiv.org/pdf/1702.01417v2.pdf
SearchQA,,# 3,Unigram Acc,46.8,Focused Hierarchical RNN,-,Open-Domain Question Answering,Focused Hierarchical RNNs for Conditional Sequence Processing,/paper/focused-hierarchical-rnns-for-conditional,https://arxiv.org/pdf/1806.04342v1.pdf
SearchQA,,# 4,N-gram F1,53.4,Focused Hierarchical RNN,-,Open-Domain Question Answering,Focused Hierarchical RNNs for Conditional Sequence Processing,/paper/focused-hierarchical-rnns-for-conditional,https://arxiv.org/pdf/1806.04342v1.pdf
E2E NLG Challenge,,# 1,BLEU,68.6,S_1^R,-,Data-to-Text Generation,Pragmatically Informative Text Generation,/paper/pragmatically-informative-text-generation,https://arxiv.org/pdf/1904.01301v2.pdf
E2E NLG Challenge,,# 1,NIST,8.73,S_1^R,-,Data-to-Text Generation,Pragmatically Informative Text Generation,/paper/pragmatically-informative-text-generation,https://arxiv.org/pdf/1904.01301v2.pdf
E2E NLG Challenge,,# 2,METEOR,45.25,S_1^R,-,Data-to-Text Generation,Pragmatically Informative Text Generation,/paper/pragmatically-informative-text-generation,https://arxiv.org/pdf/1904.01301v2.pdf
E2E NLG Challenge,,# 2,ROUGE-L,70.82,S_1^R,-,Data-to-Text Generation,Pragmatically Informative Text Generation,/paper/pragmatically-informative-text-generation,https://arxiv.org/pdf/1904.01301v2.pdf
E2E NLG Challenge,,# 1,CIDEr,2.37,S_1^R,-,Data-to-Text Generation,Pragmatically Informative Text Generation,/paper/pragmatically-informative-text-generation,https://arxiv.org/pdf/1904.01301v2.pdf
MultiMNIST,,# 1,Percentage error,5.2,CapsNet,-,Image Classification,Dynamic Routing Between Capsules,/paper/dynamic-routing-between-capsules,https://arxiv.org/pdf/1710.09829v2.pdf
IEMOCAP,,# 2,F1,63.5%,ICON,-,Emotion Recognition in Conversation,ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection,/paper/icon-interactive-conversational-memory,https://aclweb.org/anthology/D18-1280
CelebA,,# 1,FID,5.74,COCO-GAN,-,Image Generation,COCO-GAN: Generation by Parts via Conditional Coordinating,/paper/coco-gan-generation-by-parts-via-conditional,https://arxiv.org/pdf/1904.00284v2.pdf
LSUN Bedroom 256 x 256,,# 1,FID,6.95,COCO-GAN,-,Image Generation,COCO-GAN: Generation by Parts via Conditional Coordinating,/paper/coco-gan-generation-by-parts-via-conditional,https://arxiv.org/pdf/1904.00284v2.pdf
IWSLT2015 English-German,,# 5,BLEU score,26.73,ConvS2S,-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
IWSLT2015 German-English,,# 8,BLEU score,32.31,ConvS2S,-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
WMT2014 English-French,,# 11,BLEU score,40.46,ConvS2S,-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
WMT2014 English-French,,# 7,BLEU score,41.29,ConvS2S (ensemble),-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
WMT2014 English-German,,# 9,BLEU score,26.36,ConvS2S (ensemble),-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
WMT2014 English-German,,# 13,BLEU score,25.16,ConvS2S,-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
WMT2016 English-Romanian,,# 1,BLEU score,29.88,ConvS2S BPE40k,-,Machine Translation,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://arxiv.org/pdf/1705.03122v3.pdf
SVNH-to-MNIST,,# 3,Classification Accuracy,76.0%,ADDA,-,Unsupervised Image-To-Image Translation,Adversarial Discriminative Domain Adaptation,/paper/adversarial-discriminative-domain-adaptation,https://arxiv.org/pdf/1702.05464v1.pdf
CIFAR-10,,# 12,Inception score,5.62,BEGAN,-,Image Generation,BEGAN: Boundary Equilibrium Generative Adversarial Networks,/paper/began-boundary-equilibrium-generative,https://arxiv.org/pdf/1703.10717v4.pdf
CUB-200-2011,,# 9,Accuracy,76.6%,PS-CNN,-,Fine-Grained Image Classification,Part-Stacked CNN for Fine-Grained Visual Categorization,/paper/part-stacked-cnn-for-fine-grained-visual,https://arxiv.org/pdf/1512.08086v1.pdf
STL-10,,# 3,Percentage correct,77.79,CC-GANÂ²,-,Image Classification,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,/paper/semi-supervised-learning-with-context,https://arxiv.org/pdf/1611.06430v1.pdf
BP4D,,# 2,Average Accuracy,56.1%,Baseline,-,Facial Action Unit Detection,FERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge,/paper/fera-2017-addressing-head-pose-in-the-third,https://arxiv.org/pdf/1702.04174v1.pdf
BP4D,,# 2,F1,45.2,Baseline,-,Facial Action Unit Detection,FERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge,/paper/fera-2017-addressing-head-pose-in-the-third,https://arxiv.org/pdf/1702.04174v1.pdf
ADE20K Labels-to-Photos,,# 2,mIoU,22.4,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
ADE20K Labels-to-Photos,,# 3,Accuracy,68.8%,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
ADE20K Labels-to-Photos,,# 2,FID,73.3,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 3,mIoU,16.5,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 4,Accuracy,68.6%,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 4,FID,99.0,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
Cityscapes Labels-to-Photo,,# 6,Class IOU,,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
Cityscapes Labels-to-Photo,,# 5,Per-class Accuracy,,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
Cityscapes Labels-to-Photo,,# 3,Per-pixel Accuracy,77.1%,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
Cityscapes Labels-to-Photo,,# 3,mIoU,52.4,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
Cityscapes Labels-to-Photo,,# 4,FID,104.7,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
COCO-Stuff Labels-to-Photos,,# 2,mIoU,23.7,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
COCO-Stuff Labels-to-Photos,,# 3,Accuracy,40.4%,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
COCO-Stuff Labels-to-Photos,,# 2,FID,70.4,CRN,-,Image-to-Image Translation,Photographic Image Synthesis with Cascaded Refinement Networks,/paper/photographic-image-synthesis-with-cascaded,https://arxiv.org/pdf/1707.09405v1.pdf
WMT2014 English-French,,# 29,BLEU score,23.8,ByteNet,-,Machine Translation,Neural Machine Translation in Linear Time,/paper/neural-machine-translation-in-linear-time,https://arxiv.org/pdf/1610.10099v2.pdf
WMT2014 English-German,,# 14,BLEU score,23.75,ByteNet,-,Machine Translation,Neural Machine Translation in Linear Time,/paper/neural-machine-translation-in-linear-time,https://arxiv.org/pdf/1610.10099v2.pdf
WMT2015 English-German,,# 1,BLEU score,26.26,ByteNet,-,Machine Translation,Neural Machine Translation in Linear Time,/paper/neural-machine-translation-in-linear-time,https://arxiv.org/pdf/1610.10099v2.pdf
Leeds Sports Poses,,# 4,PCK,90.1%,ResNet-152 + intermediate supervision,-,Pose Estimation,"DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model",/paper/deepercut-a-deeper-stronger-and-faster-multi,https://arxiv.org/pdf/1605.03170v3.pdf
MPII Human Pose,,# 8,PCKh-0.5,88.52%,ResNet-152 + intermediate supervision,-,Pose Estimation,"DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model",/paper/deepercut-a-deeper-stronger-and-faster-multi,https://arxiv.org/pdf/1605.03170v3.pdf
MPII Multi-Person,,# 7,AP,59.4%,DeeperCut,-,Multi-Person Pose Estimation,"DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model",/paper/deepercut-a-deeper-stronger-and-faster-multi,https://arxiv.org/pdf/1605.03170v3.pdf
WAF,,# 1,AOP,88.10%,DeeperCut,-,Multi-Person Pose Estimation,"DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model",/paper/deepercut-a-deeper-stronger-and-faster-multi,https://arxiv.org/pdf/1605.03170v3.pdf
COCO,,# 14,Bounding Box AP,43.2,ExtremeNet (MS),-,Object Detection,Bottom-up Object Detection by Grouping Extreme and Center Points,/paper/bottom-up-object-detection-by-grouping,https://arxiv.org/pdf/1901.08043v3.pdf
COCO,,# 12,Bounding Box AP,43.7,ExtremeNet,-,Object Detection,Bottom-up Object Detection by Grouping Extreme and Center Points,/paper/bottom-up-object-detection-by-grouping,https://arxiv.org/pdf/1901.08043v3.pdf
Reverb,,# 2,Accuracy,68%,Memory Networks (ensemble),-,Question Answering,Large-scale Simple Question Answering with Memory Networks,/paper/large-scale-simple-question-answering-with,https://arxiv.org/pdf/1506.02075v1.pdf
SimpleQuestions,,# 1,F1,63.9%,Memory Networks (ensemble),-,Question Answering,Large-scale Simple Question Answering with Memory Networks,/paper/large-scale-simple-question-answering-with,https://arxiv.org/pdf/1506.02075v1.pdf
WebQuestions,,# 1,F1,42.2%,Memory Networks (ensemble),-,Question Answering,Large-scale Simple Question Answering with Memory Networks,/paper/large-scale-simple-question-answering-with,https://arxiv.org/pdf/1506.02075v1.pdf
SQuAD1.1,,# 38,EM,78.97800000000001,FusionNet (ensemble),-,Question Answering,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,/paper/fusionnet-fusing-via-fully-aware-attention,https://arxiv.org/pdf/1711.07341v2.pdf
SQuAD1.1,,# 38,F1,86.016,FusionNet (ensemble),-,Question Answering,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,/paper/fusionnet-fusing-via-fully-aware-attention,https://arxiv.org/pdf/1711.07341v2.pdf
SQuAD1.1,,# 66,EM,75.968,FusionNet (single model),-,Question Answering,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,/paper/fusionnet-fusing-via-fully-aware-attention,https://arxiv.org/pdf/1711.07341v2.pdf
SQuAD1.1,,# 65,F1,83.9,FusionNet (single model),-,Question Answering,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,/paper/fusionnet-fusing-via-fully-aware-attention,https://arxiv.org/pdf/1711.07341v2.pdf
SQuAD2.0,,# 70,EM,70.3,FusionNet++ (ensemble),-,Question Answering,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,/paper/fusionnet-fusing-via-fully-aware-attention,https://arxiv.org/pdf/1711.07341v2.pdf
SQuAD2.0,,# 75,F1,72.484,FusionNet++ (ensemble),-,Question Answering,FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension,/paper/fusionnet-fusing-via-fully-aware-attention,https://arxiv.org/pdf/1711.07341v2.pdf
CIFAR-10,,# 45,Percentage correct,89.1,DCNN+GFE,-,Image Classification,Deep Convolutional Neural Networks as Generic Feature Extractors,/paper/deep-convolutional-neural-networks-as-generic,https://arxiv.org/pdf/1710.02286v1.pdf
CIFAR-100,,# 31,Percentage correct,67.7,DCNN+GFE,-,Image Classification,Deep Convolutional Neural Networks as Generic Feature Extractors,/paper/deep-convolutional-neural-networks-as-generic,https://arxiv.org/pdf/1710.02286v1.pdf
MNIST,,# 5,Percentage error,0.5,DCNN+GFE,-,Image Classification,Deep Convolutional Neural Networks as Generic Feature Extractors,/paper/deep-convolutional-neural-networks-as-generic,https://arxiv.org/pdf/1710.02286v1.pdf
GigaWord,,# 7,ROUGE-1,35.98,Pointer + Coverage + EntailmentGen + QuestionGen,-,Text Summarization,Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation,/paper/soft-layer-specific-multi-task-summarization-1,https://aclweb.org/anthology/P18-1064
GigaWord,,# 3,ROUGE-2,17.76,Pointer + Coverage + EntailmentGen + QuestionGen,-,Text Summarization,Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation,/paper/soft-layer-specific-multi-task-summarization-1,https://aclweb.org/anthology/P18-1064
GigaWord,,# 7,ROUGE-L,33.63,Pointer + Coverage + EntailmentGen + QuestionGen,-,Text Summarization,Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation,/paper/soft-layer-specific-multi-task-summarization-1,https://aclweb.org/anthology/P18-1064
COCO,,# 23,Bounding Box AP,40.6,IoU-Net,-,Object Detection,Acquisition of Localization Confidence for Accurate Object Detection,/paper/acquisition-of-localization-confidence-for,https://arxiv.org/pdf/1807.11590v1.pdf
CamVid,,# 3,mIoU,67.1%,ICNet,-,Real-Time Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
CamVid,,# 1,Time (ms),36,ICNet,-,Real-Time Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
CamVid,,# 1,Frame (fps),27.8,ICNet,-,Real-Time Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
Cityscapes,,# 5,mIoU,70.6%,ICNet,-,Real-Time Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
Cityscapes,,# 2,Time (ms),33,ICNet,-,Real-Time Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
Cityscapes,,# 4,Frame (fps),30.3,ICNet,-,Real-Time Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
Cityscapes,,# 11,Mean IoU,70.6%,ICNet,-,Semantic Segmentation,ICNet for Real-Time Semantic Segmentation on High-Resolution Images,/paper/icnet-for-real-time-semantic-segmentation-on,https://arxiv.org/pdf/1704.08545v2.pdf
Penn Treebank (Word Level),,# 9,Validation perplexity,55.89,AWD-LSTM-MoS + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to-1,https://arxiv.org/pdf/1903.04167
Penn Treebank (Word Level),,# 10,Test perplexity,53.92,AWD-LSTM-MoS + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to-1,https://arxiv.org/pdf/1903.04167
Penn Treebank (Word Level),,# 1,Params,22M,AWD-LSTM-MoS + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to-1,https://arxiv.org/pdf/1903.04167
WikiText-2,,# 9,Validation perplexity,62.38,AWD-LSTM-MoS + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to-1,https://arxiv.org/pdf/1903.04167
WikiText-2,,# 10,Test perplexity,59.98,AWD-LSTM-MoS + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to-1,https://arxiv.org/pdf/1903.04167
WikiText-2,,# 1,Number of params,35M,AWD-LSTM-MoS + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to-1,https://arxiv.org/pdf/1903.04167
Penn Treebank (Word Level),,# 6,Validation perplexity,53.79,AWD-LSTM-DOC + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to,https://arxiv.org/pdf/1903.04167
Penn Treebank (Word Level),,# 7,Test perplexity,52.0,AWD-LSTM-DOC + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to,https://arxiv.org/pdf/1903.04167
Penn Treebank (Word Level),,# 1,Params,23M,AWD-LSTM-DOC + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to,https://arxiv.org/pdf/1903.04167
WikiText-2,,# 7,Validation perplexity,60.16,AWD-LSTM-DOC + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to,https://arxiv.org/pdf/1903.04167
WikiText-2,,# 8,Test perplexity,57.85,AWD-LSTM-DOC + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to,https://arxiv.org/pdf/1903.04167
WikiText-2,,# 1,Number of params,37M,AWD-LSTM-DOC + Partial Shuffle,-,Language Modelling,Partially Shuffling the Training Data to Improve Language Models,/paper/partially-shuffling-the-training-data-to,https://arxiv.org/pdf/1903.04167
CIFAR-10 Image Classification,,# 3,Percentage error,2.30,PathLevel EAS + c/o,-,Architecture Search,Path-Level Network Transformation for Efficient Architecture Search,/paper/path-level-network-transformation-for,https://arxiv.org/pdf/1806.02639v1.pdf
CIFAR-10 Image Classification,,# 1,Params,14.3M,PathLevel EAS + c/o,-,Architecture Search,Path-Level Network Transformation for Efficient Architecture Search,/paper/path-level-network-transformation-for,https://arxiv.org/pdf/1806.02639v1.pdf
SemEval,,# 2,F1-score,0.677,Deep Bi-LSTM+attention,-,Sentiment Analysis,DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis,/paper/datastories-at-semeval-2017-task-4-deep-lstm,https://aclweb.org/anthology/S17-2126
KITTI Cars Easy,,# 5,AP,81.20%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cars Easy,,# 2,AP,88.70%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cars Hard,,# 2,AP,75.33%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cars Hard,,# 5,AP,62.19%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cars Moderate,,# 1,AP,84.00%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cars Moderate,,# 7,AP,70.39%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cyclists Easy,,# 1,AP,71.96%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cyclists Easy,,# 1,AP,75.38%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cyclists Hard,,# 1,AP,50.39%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cyclists Hard,,# 1,AP,54.68%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cyclists Moderate,,# 1,AP,61.96%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Cyclists Moderate,,# 2,AP,56.77%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Pedestrians Easy,,# 1,AP,58.09%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Pedestrians Easy,,# 2,AP,51.21%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Pedestrians Hard,,# 1,AP,47.20%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Pedestrians Hard,,# 3,AP,40.23%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Pedestrians Moderate,,# 1,AP,50.22%,Frustum PointNets,-,Object Localization,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
KITTI Pedestrians Moderate,,# 1,AP,44.89%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
SUN-RGBD,,# 1,MAP,54.0%,Frustum PointNets,-,3D Object Detection,Frustum PointNets for 3D Object Detection from RGB-D Data,/paper/frustum-pointnets-for-3d-object-detection,https://arxiv.org/pdf/1711.08488v2.pdf
IJB-C,,# 2,TAR @ FAR=0.01,92.70%,MN-vc,-,Face Verification,Multicolumn Networks for Face Recognition,/paper/multicolumn-networks-for-face-recognition,https://arxiv.org/pdf/1807.09192v1.pdf
RVL-CDIP,,# 4,Accuracy,89.80%,Document section-based models + AlexNet transfer learning,-,Document Image Classification,Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval,/paper/evaluation-of-deep-convolutional-nets-for,https://arxiv.org/pdf/1502.07058v1.pdf
swb_hub_500 WER fullSWBCH,,# 3,Percentage error,12.2,"RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + ""model M"" + NNLM language model",-,Speech Recognition,The IBM 2016 English Conversational Telephone Speech Recognition System,/paper/the-ibm-2016-english-conversational-telephone,https://arxiv.org/pdf/1604.08242v2.pdf
Switchboard + Hub500,,# 7,Percentage error,6.9,IBM 2016,-,Speech Recognition,The IBM 2016 English Conversational Telephone Speech Recognition System,/paper/the-ibm-2016-english-conversational-telephone,https://arxiv.org/pdf/1604.08242v2.pdf
Switchboard + Hub500,,# 5,Percentage error,6.6,"RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + ""model M"" + NNLM language model",-,Speech Recognition,The IBM 2016 English Conversational Telephone Speech Recognition System,/paper/the-ibm-2016-english-conversational-telephone,https://arxiv.org/pdf/1604.08242v2.pdf
Penn Treebank (Word Level),,# 13,Validation perplexity,58.3,Differentiable NAS,-,Language Modelling,DARTS: Differentiable Architecture Search,/paper/darts-differentiable-architecture-search,https://arxiv.org/pdf/1806.09055v2.pdf
Penn Treebank (Word Level),,# 15,Test perplexity,56.1,Differentiable NAS,-,Language Modelling,DARTS: Differentiable Architecture Search,/paper/darts-differentiable-architecture-search,https://arxiv.org/pdf/1806.09055v2.pdf
Penn Treebank (Word Level),,# 1,Params,23M,Differentiable NAS,-,Language Modelling,DARTS: Differentiable Architecture Search,/paper/darts-differentiable-architecture-search,https://arxiv.org/pdf/1806.09055v2.pdf
HPatches,,# 1,Viewpoint I AEPE,1.55,DGC-Net aff+tps+homo,-,Dense Pixel Correspondence Estimation,DGC-Net: Dense Geometric Correspondence Network,/paper/dgc-net-dense-geometric-correspondence,https://arxiv.org/pdf/1810.08393v2.pdf
HPatches,,# 2,Viewpoint II AEPE,5.53,DGC-Net aff+tps+homo,-,Dense Pixel Correspondence Estimation,DGC-Net: Dense Geometric Correspondence Network,/paper/dgc-net-dense-geometric-correspondence,https://arxiv.org/pdf/1810.08393v2.pdf
HPatches,,# 1,Viewpoint III AEPE,8.98,DGC-Net aff+tps+homo,-,Dense Pixel Correspondence Estimation,DGC-Net: Dense Geometric Correspondence Network,/paper/dgc-net-dense-geometric-correspondence,https://arxiv.org/pdf/1810.08393v2.pdf
HPatches,,# 1,Viewpoint IV AEPE,11.66,DGC-Net aff+tps+homo,-,Dense Pixel Correspondence Estimation,DGC-Net: Dense Geometric Correspondence Network,/paper/dgc-net-dense-geometric-correspondence,https://arxiv.org/pdf/1810.08393v2.pdf
HPatches,,# 1,Viewpoint V AEPE,16.7,DGC-Net aff+tps+homo,-,Dense Pixel Correspondence Estimation,DGC-Net: Dense Geometric Correspondence Network,/paper/dgc-net-dense-geometric-correspondence,https://arxiv.org/pdf/1810.08393v2.pdf
ImageNet,,# 8,Top 1 Accuracy,80.1%,RandWire-WS,-,Image Classification,Exploring Randomly Wired Neural Networks for Image Recognition,/paper/exploring-randomly-wired-neural-networks-for,https://arxiv.org/pdf/1904.01569v2.pdf
ImageNet,,# 7,Top 5 Accuracy,94.8%,RandWire-WS,-,Image Classification,Exploring Randomly Wired Neural Networks for Image Recognition,/paper/exploring-randomly-wired-neural-networks-for,https://arxiv.org/pdf/1904.01569v2.pdf
ScanNet,,# 1,Average Accuracy,75.0%,3DMV,-,Scene Segmentation,3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation,/paper/3dmv-joint-3d-multi-view-prediction-for-3d,https://arxiv.org/pdf/1803.10409v1.pdf
Penn Treebank,,# 3,F1 score,94.66,Model combination,-,Constituency Parsing,Improving Neural Parsing by Disentangling Model Combination and Reranking Effects,/paper/improving-neural-parsing-by-disentangling,https://arxiv.org/pdf/1707.03058v1.pdf
CIFAR-10,,# 7,Model Entropy,2.89,Image Transformer,-,Image Generation,Image Transformer,/paper/image-transformer,https://arxiv.org/pdf/1802.05751v3.pdf
LibriSpeech test-clean,,# 5,Word Error Rate (WER),3.44,Convolutional Speech Recognition,-,Speech Recognition,Fully Convolutional Speech Recognition,/paper/fully-convolutional-speech-recognition,https://arxiv.org/pdf/1812.06864v2.pdf
AFLW2000,,# 2,MAE,7.393,3DDFA,-,Head Pose Estimation,Face Alignment Across Large Poses: A 3D Solution,/paper/face-alignment-across-large-poses-a-3d,https://arxiv.org/pdf/1511.07212v1.pdf
AFLW2000-3D,,# 5,Mean NME,4.94%,3DDFA + SDM,-,Face Alignment,Face Alignment Across Large Poses: A 3D Solution,/paper/face-alignment-across-large-poses-a-3d,https://arxiv.org/pdf/1511.07212v1.pdf
AFLW2000-3D,,# 2,Mean NME,5.3695%,3DDFA,-,3D Face Reconstruction,Face Alignment Across Large Poses: A 3D Solution,/paper/face-alignment-across-large-poses-a-3d,https://arxiv.org/pdf/1511.07212v1.pdf
BIWI,,# 4,MAE,19.068,3DDFA,-,Head Pose Estimation,Face Alignment Across Large Poses: A 3D Solution,/paper/face-alignment-across-large-poses-a-3d,https://arxiv.org/pdf/1511.07212v1.pdf
Florence,,# 3,Mean NME,6.3833%,3DDFA,-,3D Face Reconstruction,Face Alignment Across Large Poses: A 3D Solution,/paper/face-alignment-across-large-poses-a-3d,https://arxiv.org/pdf/1511.07212v1.pdf
CIFAR-100,,# 7,Percentage correct,81.6,AA-Wide-ResNet,-,Image Classification,Attention Augmented Convolutional Networks,/paper/190409925,https://arxiv.org/pdf/1904.09925v1.pdf
COCO,,# 27,Bounding Box AP,39.2,AA-ResNet-10 + RetinaNet,-,Object Detection,Attention Augmented Convolutional Networks,/paper/190409925,https://arxiv.org/pdf/1904.09925v1.pdf
ImageNet,,# 12,Top 1 Accuracy,79.1%,AA-ResNet-152,-,Image Classification,Attention Augmented Convolutional Networks,/paper/190409925,https://arxiv.org/pdf/1904.09925v1.pdf
ImageNet,,# 9,Top 5 Accuracy,94.6%,AA-ResNet-152,-,Image Classification,Attention Augmented Convolutional Networks,/paper/190409925,https://arxiv.org/pdf/1904.09925v1.pdf
RumourEval,,# 1,Accuracy,0.784,Kochkina et al. 2017,-,Stance Detection,Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM,/paper/turing-at-semeval-2017-task-8-sequential,https://arxiv.org/pdf/1704.07221v1.pdf
QM9,,# 1,Error ratio,0.68,MPNNs,-,Drug Discovery,Neural Message Passing for Quantum Chemistry,/paper/neural-message-passing-for-quantum-chemistry,https://arxiv.org/pdf/1704.01212v2.pdf
STL-10,,# 2,Percentage correct,87.26,Cutout,-,Image Classification,Improved Regularization of Convolutional Neural Networks with Cutout,/paper/improved-regularization-of-convolutional,https://arxiv.org/pdf/1708.04552v2.pdf
FB15k,,# 4,MRR,0.727,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
FB15k,,# 4,[emailÂ protected],0.838,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
FB15k,,# 3,[emailÂ protected],0.773,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
FB15k,,# 3,[emailÂ protected],0.66,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
WN18,,# 5,MRR,0.9420000000000001,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
WN18,,# 4,[emailÂ protected],0.9470000000000001,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
WN18,,# 3,[emailÂ protected],0.9440000000000001,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
WN18,,# 4,[emailÂ protected],0.9390000000000001,SimplE,-,Link Prediction,SimplE Embedding for Link Prediction in Knowledge Graphs,/paper/simple-embedding-for-link-prediction-in,https://arxiv.org/pdf/1802.04868v2.pdf
ImageCLEF-DA,,# 2,Accuracy,80.6,IDDA(Alexnet),-,Domain Adaptation,Looking back at Labels: A Class based Domain Adaptation Technique,/paper/looking-back-at-labels-a-class-based-domain,https://arxiv.org/pdf/1904.01341v1.pdf
Office-31,,# 1,Accuracy,78.5,IDDA(Alexnet),-,Domain Adaptation,Looking back at Labels: A Class based Domain Adaptation Technique,/paper/looking-back-at-labels-a-class-based-domain,https://arxiv.org/pdf/1904.01341v1.pdf
NarrativeQA,,# 4,BLEU-1,36.55,BiAttention + DCU-LSTM,-,Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
NarrativeQA,,# 4,BLEU-4,19.79,BiAttention + DCU-LSTM,-,Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
NarrativeQA,,# 4,METEOR,17.87,BiAttention + DCU-LSTM,-,Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
NarrativeQA,,# 4,Rouge-L,41.44,BiAttention + DCU-LSTM,-,Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
SearchQA,,# 2,Unigram Acc,49.4,Bi-Attention + DCU-LSTM,-,Open-Domain Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
SearchQA,,# 2,N-gram F1,59.5,Bi-Attention + DCU-LSTM,-,Open-Domain Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
SearchQA,,# 4,EM,-,Bi-Attention + DCU-LSTM,-,Open-Domain Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
SearchQA,,# 4,F1,-,Bi-Attention + DCU-LSTM,-,Open-Domain Question Answering,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,/paper/multi-granular-sequence-encoding-via-dilated,https://aclweb.org/anthology/D18-1238
SemEvalCQA,,# 1,[emailÂ protected],0.809,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
SemEvalCQA,,# 1,MAP,0.795,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
TrecQA,,# 1,MAP,0.77,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
TrecQA,,# 1,MRR,0.825,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
WikiQA,,# 1,MAP,0.7120000000000001,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
WikiQA,,# 1,MRR,0.727,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
YahooCQA,,# 5,[emailÂ protected],0.413,CNN,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
YahooCQA,,# 5,MRR,0.632,CNN,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
YahooCQA,,# 1,[emailÂ protected],0.6829999999999999,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
YahooCQA,,# 1,MRR,0.8009999999999999,HyperQA,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
YahooCQA,,# 4,[emailÂ protected],0.465,LSTM,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
YahooCQA,,# 4,MRR,0.669,LSTM,-,Question Answering,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,/paper/hyperbolic-representation-learning-for-fast,https://arxiv.org/pdf/1707.07847v3.pdf
Google Street Images,,# 1,Accuracy,81%,CNN,-,Safety Perception Recognition,Predicting city safety perception based on visual image content,/paper/predicting-city-safety-perception-based-on-1,https://arxiv.org/pdf/1902.06871
ImageNet,,# 13,Top 1 Accuracy,79%,Xception,-,Image Classification,Xception: Deep Learning with Depthwise Separable Convolutions,/paper/xception-deep-learning-with-depthwise,https://arxiv.org/pdf/1610.02357v3.pdf
ImageNet,,# 10,Top 5 Accuracy,94.5%,Xception,-,Image Classification,Xception: Deep Learning with Depthwise Separable Convolutions,/paper/xception-deep-learning-with-depthwise,https://arxiv.org/pdf/1610.02357v3.pdf
T-LESS,,# 1,Accuracy,57.14,RetinaNet+Augmented Autoencoders+ICP,-,6D Pose Estimation,Implicit 3D Orientation Learning for 6D Object Detection from RGB Images,/paper/implicit-3d-orientation-learning-for-6d,https://arxiv.org/pdf/1902.01275v1.pdf
E2E NLG Challenge,,# 2,BLEU,66.19,Slug,-,Data-to-Text Generation,A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation,/paper/a-deep-ensemble-model-with-slot-alignment-for,https://arxiv.org/pdf/1805.06553v1.pdf
E2E NLG Challenge,,# 2,NIST,8.613,Slug,-,Data-to-Text Generation,A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation,/paper/a-deep-ensemble-model-with-slot-alignment-for,https://arxiv.org/pdf/1805.06553v1.pdf
E2E NLG Challenge,,# 6,METEOR,44.54,Slug,-,Data-to-Text Generation,A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation,/paper/a-deep-ensemble-model-with-slot-alignment-for,https://arxiv.org/pdf/1805.06553v1.pdf
E2E NLG Challenge,,# 5,ROUGE-L,67.72,Slug,-,Data-to-Text Generation,A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation,/paper/a-deep-ensemble-model-with-slot-alignment-for,https://arxiv.org/pdf/1805.06553v1.pdf
Market-1501,,# 23,Rank-1,61.02,DNS,-,Person Re-Identification,Learning a Discriminative Null Space for Person Re-identification,/paper/learning-a-discriminative-null-space-for,https://arxiv.org/pdf/1603.02139v1.pdf
Market-1501,,# 23,MAP,35.68,DNS,-,Person Re-Identification,Learning a Discriminative Null Space for Person Re-identification,/paper/learning-a-discriminative-null-space-for,https://arxiv.org/pdf/1603.02139v1.pdf
SQuAD1.1,,# 60,EM,76.461,r-net (single model),-,Question Answering,Gated Self-Matching Networks for Reading Comprehension and Question Answering,/paper/gated-self-matching-networks-for-reading,https://aclweb.org/anthology/P17-1018
SQuAD1.1,,# 61,F1,84.265,r-net (single model),-,Question Answering,Gated Self-Matching Networks for Reading Comprehension and Question Answering,/paper/gated-self-matching-networks-for-reading,https://aclweb.org/anthology/P17-1018
swb_hub_500 WER fullSWBCH,,# 2,Percentage error,11.9,"VGG/Resnet/LACE/BiLSTM acoustic model trained on SWB+Fisher+CH, N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast",-,Speech Recognition,The Microsoft 2016 Conversational Speech Recognition System,/paper/the-microsoft-2016-conversational-speech,https://arxiv.org/pdf/1609.03528v2.pdf
Switchboard + Hub500,,# 4,Percentage error,6.3,"VGG/Resnet/LACE/BiLSTM acoustic model trained on SWB+Fisher+CH, N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast",-,Speech Recognition,The Microsoft 2016 Conversational Speech Recognition System,/paper/the-microsoft-2016-conversational-speech,https://arxiv.org/pdf/1609.03528v2.pdf
Switchboard + Hub500,,# 7,Percentage error,6.9,RNNLM,-,Speech Recognition,The Microsoft 2016 Conversational Speech Recognition System,/paper/the-microsoft-2016-conversational-speech,https://arxiv.org/pdf/1609.03528v2.pdf
Switchboard + Hub500,,# 3,Percentage error,6.2,Microsoft 2016,-,Speech Recognition,The Microsoft 2016 Conversational Speech Recognition System,/paper/the-microsoft-2016-conversational-speech,https://arxiv.org/pdf/1609.03528v2.pdf
Market-1501,,# 19,Rank-1,80.31,MSCAN,-,Person Re-Identification,Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification,/paper/learning-deep-context-aware-features-over,https://arxiv.org/pdf/1710.06555v1.pdf
Market-1501,,# 21,MAP,57.53,MSCAN,-,Person Re-Identification,Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification,/paper/learning-deep-context-aware-features-over,https://arxiv.org/pdf/1710.06555v1.pdf
Restricted,,# 2,F0.5,56.52,CNN Seq2Seq + Quality Estimation,-,Grammatical Error Correction,Neural Quality Estimation of Grammatical Error Correction,/paper/neural-quality-estimation-of-grammatical,https://aclweb.org/anthology/D18-1274
MovieLens 1M,,# 6,RMSE,0.843,NNMF,-,Collaborative Filtering,Neural Network Matrix Factorization,/paper/neural-network-matrix-factorization,https://arxiv.org/pdf/1511.06443v2.pdf
CIFAR-10,,# 47,Percentage correct,88.8,MCDNN,-,Image Classification,Multi-column Deep Neural Networks for Image Classification,/paper/multi-column-deep-neural-networks-for-image,https://arxiv.org/pdf/1202.2745v1.pdf
GTSRB,,# 1,Error rate,11.21%,MCDNN,-,Traffic Sign Recognition,Multi-column Deep Neural Networks for Image Classification,/paper/multi-column-deep-neural-networks-for-image,https://arxiv.org/pdf/1202.2745v1.pdf
GTSRB,,# 1,Accuracy,99.5%,MCDNN,-,Traffic Sign Recognition,Multi-column Deep Neural Networks for Image Classification,/paper/multi-column-deep-neural-networks-for-image,https://arxiv.org/pdf/1202.2745v1.pdf
MNIST,,# 2,Percentage error,0.2,MCDNN,-,Image Classification,Multi-column Deep Neural Networks for Image Classification,/paper/multi-column-deep-neural-networks-for-image,https://arxiv.org/pdf/1202.2745v1.pdf
PASCAL VOC 2007,,# 9,MAP,77.1%,DeNet-101 (skip),-,Object Detection,DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling,/paper/denet-scalable-real-time-object-detection,https://arxiv.org/pdf/1703.10295v3.pdf
bAbi,,# 1,Accuracy (trained on 10k),99.7%,QRN,-,Question Answering,Query-Reduction Networks for Question Answering,/paper/query-reduction-networks-for-question,https://arxiv.org/pdf/1606.04582v6.pdf
bAbi,,# 1,Accuracy (trained on 1k),90.1%,QRN,-,Question Answering,Query-Reduction Networks for Question Answering,/paper/query-reduction-networks-for-question,https://arxiv.org/pdf/1606.04582v6.pdf
bAbi,,# 1,Mean Error Rate,0.3%,QRN,-,Question Answering,Query-Reduction Networks for Question Answering,/paper/query-reduction-networks-for-question,https://arxiv.org/pdf/1606.04582v6.pdf
CHiME clean,,# 1,Percentage error,3.34,Deep Speech 2,-,Noisy Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
CHiME real,,# 3,Percentage error,21.79,Deep Speech 2,-,Noisy Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
LibriSpeech test-other,,# 5,Word Error Rate (WER),13.25,Deep Speech 2,-,Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
VoxForge American-Canadian,,# 1,Percentage error,7.55,Deep Speech 2,-,Accented Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
VoxForge Commonwealth,,# 1,Percentage error,13.56,Deep Speech 2,-,Accented Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
VoxForge European,,# 1,Percentage error,17.55,Deep Speech 2,-,Accented Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
VoxForge Indian,,# 1,Percentage error,22.44,Deep Speech 2,-,Accented Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
WSJ eval92,,# 4,Percentage error,3.6,Deep Speech 2,-,Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
WSJ eval93,,# 1,Percentage error,4.98,Deep Speech 2,-,Speech Recognition,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,/paper/deep-speech-2-end-to-end-speech-recognition,https://arxiv.org/pdf/1512.02595v1.pdf
COCO,,# 2,Test AP,66.7,PifPaf â single-scale (ours),-,Keypoint Detection,PifPaf: Composite Fields for Human Pose Estimation,/paper/pifpaf-composite-fields-for-human-pose,https://arxiv.org/pdf/1903.06593v2.pdf
Human3.6M,,# 6,Average 3D Error,88.39,Projected-pose belief maps + 2D fusion layers,-,3D Human Pose Estimation,Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image,/paper/lifting-from-the-deep-convolutional-3d-pose,https://arxiv.org/pdf/1701.00295v4.pdf
Story Cloze Test,,# 2,Accuracy,78.7,Memory chains and semantic supervision,-,Question Answering,UNIMELB at SemEval-2016 Tasks 4A and 4B: An Ensemble of Neural Networks and a Word2Vec Based Model for Sentiment Classification,/paper/unimelb-at-semeval-2016-tasks-4a-and-4b-an,https://aclweb.org/anthology/S16-1027
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 4,Percentage correct,66.1,HQI+ResNet,-,Visual Question Answering,Hierarchical Question-Image Co-Attention for Visual Question Answering,/paper/hierarchical-question-image-co-attention-for,https://arxiv.org/pdf/1606.00061v5.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 4,Percentage correct,62.1,HQI+ResNet,-,Visual Question Answering,Hierarchical Question-Image Co-Attention for Visual Question Answering,/paper/hierarchical-question-image-co-attention-for,https://arxiv.org/pdf/1606.00061v5.pdf
ScanNet,,# 2,Average Accuracy,60.2%,PointNet++,-,Scene Segmentation,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,/paper/pointnet-deep-learning-on-point-sets-for-3d,https://arxiv.org/pdf/1612.00593v2.pdf
ShapeNet-Part,,# 5,Class Average IoU,80.4,PointNet,-,3D Part Segmentation,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,/paper/pointnet-deep-learning-on-point-sets-for-3d,https://arxiv.org/pdf/1612.00593v2.pdf
ShapeNet-Part,,# 7,Instance Average IoU,83.7,PointNet,-,3D Part Segmentation,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,/paper/pointnet-deep-learning-on-point-sets-for-3d,https://arxiv.org/pdf/1612.00593v2.pdf
General,,# 7,MAP,2.68,Apollo,-,Hypernym Discovery,Apollo at SemEval-2018 Task 9: Detecting Hypernymy Relations Using Syntactic Dependencies,/paper/apollo-at-semeval-2018-task-9-detecting,https://aclweb.org/anthology/S18-1146
General,,# 7,MRR,6.01,Apollo,-,Hypernym Discovery,Apollo at SemEval-2018 Task 9: Detecting Hypernymy Relations Using Syntactic Dependencies,/paper/apollo-at-semeval-2018-task-9-detecting,https://aclweb.org/anthology/S18-1146
General,,# 7,[emailÂ protected],2.69,Apollo,-,Hypernym Discovery,Apollo at SemEval-2018 Task 9: Detecting Hypernymy Relations Using Syntactic Dependencies,/paper/apollo-at-semeval-2018-task-9-detecting,https://aclweb.org/anthology/S18-1146
Atari 2600 Freeway,,# 6,Score,33.0,DQN-CTS,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Freeway,,# 7,Score,31.7,DQN-PixelCNN,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Gravitar,,# 7,Score,498.3,DQN-PixelCNN,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Gravitar,,# 22,Score,238.0,DQN-CTS,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Montezuma's Revenge,,# 3,Score,3705.5,DQN-PixelCNN,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,DQN-CTS,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Private Eye,,# 3,Score,8358.7,DQN-PixelCNN,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Private Eye,,# 15,Score,206.0,DQN-CTS,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Venture,,# 17,Score,82.2,DQN-PixelCNN,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
Atari 2600 Venture,,# 20,Score,48.0,DQN-CTS,-,Atari Games,Count-Based Exploration with Neural Density Models,/paper/count-based-exploration-with-neural-density,https://arxiv.org/pdf/1703.01310v2.pdf
CIFAR-10,,# 7,Inception score,7.3,SN-SMMDGAN,-,Image Generation,On gradient regularizers for MMD GANs,/paper/on-gradient-regularizers-for-mmd-gans,https://arxiv.org/pdf/1805.11565v4.pdf
CIFAR-10,,# 4,FID,25.0,SN-SMMDGAN,-,Image Generation,On gradient regularizers for MMD GANs,/paper/on-gradient-regularizers-for-mmd-gans,https://arxiv.org/pdf/1805.11565v4.pdf
BlogCatalog,,# 4,Accuracy,21.50%,node2vec,-,Node Classification,node2vec: Scalable Feature Learning for Networks,/paper/node2vec-scalable-feature-learning-for,https://arxiv.org/pdf/1607.00653v1.pdf
BlogCatalog,,# 4,Macro-F1,0.206,node2vec,-,Node Classification,node2vec: Scalable Feature Learning for Networks,/paper/node2vec-scalable-feature-learning-for,https://arxiv.org/pdf/1607.00653v1.pdf
Wikipedia,,# 4,Accuracy,19.10%,node2vec,-,Node Classification,node2vec: Scalable Feature Learning for Networks,/paper/node2vec-scalable-feature-learning-for,https://arxiv.org/pdf/1607.00653v1.pdf
Wikipedia,,# 4,Macro-F1,0.179,node2vec,-,Node Classification,node2vec: Scalable Feature Learning for Networks,/paper/node2vec-scalable-feature-learning-for,https://arxiv.org/pdf/1607.00653v1.pdf
DUC 2004 Task 1,,# 6,ROUGE-1,28.97,RAS-Elman,-,Text Summarization,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,/paper/abstractive-sentence-summarization-with,https://aclweb.org/anthology/N16-1012
DUC 2004 Task 1,,# 8,ROUGE-2,8.26,RAS-Elman,-,Text Summarization,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,/paper/abstractive-sentence-summarization-with,https://aclweb.org/anthology/N16-1012
DUC 2004 Task 1,,# 6,ROUGE-L,24.06,RAS-Elman,-,Text Summarization,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,/paper/abstractive-sentence-summarization-with,https://aclweb.org/anthology/N16-1012
GigaWord,,# 10,ROUGE-1,33.78,RAS-Elman,-,Text Summarization,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,/paper/abstractive-sentence-summarization-with,https://aclweb.org/anthology/N16-1012
GigaWord,,# 12,ROUGE-2,15.97,RAS-Elman,-,Text Summarization,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,/paper/abstractive-sentence-summarization-with,https://aclweb.org/anthology/N16-1012
GigaWord,,# 11,ROUGE-L,31.15,RAS-Elman,-,Text Summarization,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,/paper/abstractive-sentence-summarization-with,https://aclweb.org/anthology/N16-1012
Bosch Small Traffic Lights,,# 3,MAP,0.41,Background Threshold Model,-,Traffic Sign Recognition,A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection,/paper/a-hierarchical-deep-architecture-and-mini,https://arxiv.org/pdf/1806.07987v2.pdf
Bosch Small Traffic Lights,,# 2,MAP,0.45,Hierarchical Model,-,Traffic Sign Recognition,A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection,/paper/a-hierarchical-deep-architecture-and-mini,https://arxiv.org/pdf/1806.07987v2.pdf
Bosch Small Traffic Lights,,# 1,MAP,0.46,Hierarchical + Background Threshold Model,-,Traffic Sign Recognition,A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection,/paper/a-hierarchical-deep-architecture-and-mini,https://arxiv.org/pdf/1806.07987v2.pdf
Tsinghua-Tencent 100K,,# 2,MAP,0.31,Hierarchical + Background Threshold Model,-,Traffic Sign Recognition,A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection,/paper/a-hierarchical-deep-architecture-and-mini,https://arxiv.org/pdf/1806.07987v2.pdf
Tsinghua-Tencent 100K,,# 1,MAP,0.32,Background Threshold Model,-,Traffic Sign Recognition,A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection,/paper/a-hierarchical-deep-architecture-and-mini,https://arxiv.org/pdf/1806.07987v2.pdf
Tsinghua-Tencent 100K,,# 3,MAP,0.3,Hierarchical Model,-,Traffic Sign Recognition,A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection,/paper/a-hierarchical-deep-architecture-and-mini,https://arxiv.org/pdf/1806.07987v2.pdf
CoNLL 2012,,# 1,Avg F1,73.0,"(Lee et al., 2017)+ELMo",-,Coreference Resolution,Higher-Order Coreference Resolution with Coarse-to-Fine Inference,/paper/higher-order-coreference-resolution-with-1,https://aclweb.org/anthology/N18-2108
Disguised Faces in the Wild,,# 2,GAR @0.1% FAR,17.73,VGG-Face model features + cosine similarity metric,-,Disguised Face Verification,Recognizing Disguised Faces in the Wild,/paper/recognizing-disguised-faces-in-the-wild,https://arxiv.org/pdf/1811.08837v1.pdf
Disguised Faces in the Wild,,# 2,GAR @1% FAR,33.76,VGG-Face model features + cosine similarity metric,-,Disguised Face Verification,Recognizing Disguised Faces in the Wild,/paper/recognizing-disguised-faces-in-the-wild,https://arxiv.org/pdf/1811.08837v1.pdf
street2shop - topwear,,# 1,Accuracy,94.98,Ranknet,-,Image Retrieval,Retrieving Similar E-Commerce Images Using Deep Learning,/paper/retrieving-similar-e-commerce-images-using,https://arxiv.org/pdf/1901.03546v1.pdf
CIFAR-10,,# 31,Percentage correct,92.2,SWWAE,-,Image Classification,Stacked What-Where Auto-encoders,/paper/stacked-what-where-auto-encoders,https://arxiv.org/pdf/1506.02351v8.pdf
CIFAR-100,,# 25,Percentage correct,69.1,SWWAE,-,Image Classification,Stacked What-Where Auto-encoders,/paper/stacked-what-where-auto-encoders,https://arxiv.org/pdf/1506.02351v8.pdf
STL-10,,# 4,Percentage correct,74.33,SWWAE,-,Image Classification,Stacked What-Where Auto-encoders,/paper/stacked-what-where-auto-encoders,https://arxiv.org/pdf/1506.02351v8.pdf
BSD68 sigma15,,# 4,PSNR,31.63,FFDNet,-,Image Denoising,FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising,/paper/ffdnet-toward-a-fast-and-flexible-solution,https://arxiv.org/pdf/1710.04026v2.pdf
BSD68 sigma25,,# 4,PSNR,29.19,FFDNet,-,Image Denoising,FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising,/paper/ffdnet-toward-a-fast-and-flexible-solution,https://arxiv.org/pdf/1710.04026v2.pdf
BSD68 sigma50,,# 5,PSNR,26.29,FFDNet,-,Image Denoising,FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising,/paper/ffdnet-toward-a-fast-and-flexible-solution,https://arxiv.org/pdf/1710.04026v2.pdf
V-SNLI,,# 1,Accuracy,86.99,V-BiMPM,-,Natural Language Inference,Grounded Textual Entailment,/paper/grounded-textual-entailment,https://arxiv.org/pdf/1806.05645v1.pdf
V-SNLI,,# 2,Accuracy,86.41,BiMPM,-,Natural Language Inference,Grounded Textual Entailment,/paper/grounded-textual-entailment,https://arxiv.org/pdf/1806.05645v1.pdf
Labeled Faces in the Wild,,# 6,Accuracy,99.42%,SphereFace,-,Face Verification,SphereFace: Deep Hypersphere Embedding for Face Recognition,/paper/sphereface-deep-hypersphere-embedding-for,https://arxiv.org/pdf/1704.08063v4.pdf
MegaFace,,# 5,Accuracy,85.561%,SphereFace (single model),-,Face Verification,SphereFace: Deep Hypersphere Embedding for Face Recognition,/paper/sphereface-deep-hypersphere-embedding-for,https://arxiv.org/pdf/1704.08063v4.pdf
MegaFace,,# 3,Accuracy,89.142%,SphereFace (3-patch ensemble),-,Face Verification,SphereFace: Deep Hypersphere Embedding for Face Recognition,/paper/sphereface-deep-hypersphere-embedding-for,https://arxiv.org/pdf/1704.08063v4.pdf
MegaFace,,# 5,Accuracy,72.729%,SphereFace (single model),-,Face Identification,SphereFace: Deep Hypersphere Embedding for Face Recognition,/paper/sphereface-deep-hypersphere-embedding-for,https://arxiv.org/pdf/1704.08063v4.pdf
MegaFace,,# 3,Accuracy,75.766%,SphereFace (3-patch ensemble),-,Face Identification,SphereFace: Deep Hypersphere Embedding for Face Recognition,/paper/sphereface-deep-hypersphere-embedding-for,https://arxiv.org/pdf/1704.08063v4.pdf
YouTube Faces DB,,# 8,Accuracy,95.0%,SphereFace,-,Face Verification,SphereFace: Deep Hypersphere Embedding for Face Recognition,/paper/sphereface-deep-hypersphere-embedding-for,https://arxiv.org/pdf/1704.08063v4.pdf
SQuAD1.1,,# 84,EM,73.723,SEDT+BiDAF (ensemble),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 86,F1,81.53,SEDT+BiDAF (ensemble),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 113,EM,68.47800000000001,SEDT+BiDAF (single model),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 115,F1,77.971,SEDT+BiDAF (single model),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 116,EM,68.163,SEDT (single model),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 119,F1,77.527,SEDT (single model),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 80,EM,74.09,SEDT (ensemble model),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
SQuAD1.1,,# 83,F1,81.76100000000001,SEDT (ensemble model),-,Question Answering,Structural Embedding of Syntactic Trees for Machine Comprehension,/paper/structural-embedding-of-syntactic-trees-for,https://arxiv.org/pdf/1703.00572v3.pdf
ImageNet,,# 5,Top 1 Accuracy,81.5%,DPN-131,-,Image Classification,Dual Path Networks,/paper/dual-path-networks,https://arxiv.org/pdf/1707.01629v2.pdf
ImageNet,,# 4,Top 5 Accuracy,95.8%,DPN-131,-,Image Classification,Dual Path Networks,/paper/dual-path-networks,https://arxiv.org/pdf/1707.01629v2.pdf
COCO,,# 1,Average Precision,42.0%,PANet,-,Instance Segmentation,Path Aggregation Network for Instance Segmentation,/paper/path-aggregation-network-for-instance,https://arxiv.org/pdf/1803.01534v4.pdf
COCO,,# 2,Bounding Box AP,47.4,PANet + ResNeXt-101,-,Object Detection,Path Aggregation Network for Instance Segmentation,/paper/path-aggregation-network-for-instance,https://arxiv.org/pdf/1803.01534v4.pdf
IJB-A,,# 1,TAR @ FAR=0.01,0.972,SE-GV-3,-,Face Recognition,GhostVLAD for set-based face recognition,/paper/ghostvlad-for-set-based-face-recognition,https://arxiv.org/pdf/1810.09951v1.pdf
IJB-A,,# 2,TAR @ FAR=0.01,97.2,SE-GV-4-g1,-,Face Verification,GhostVLAD for set-based face recognition,/paper/ghostvlad-for-set-based-face-recognition,https://arxiv.org/pdf/1810.09951v1.pdf
IJB-B,,# 1,TAR @ FAR=0.01,0.963,"GhostVLAD, SE-GV-3",-,Face Recognition,GhostVLAD for set-based face recognition,/paper/ghostvlad-for-set-based-face-recognition,https://arxiv.org/pdf/1810.09951v1.pdf
IJB-B,,# 2,TAR @ FAR=0.01,96.4,SE-GV-3-g2,-,Face Verification,GhostVLAD for set-based face recognition,/paper/ghostvlad-for-set-based-face-recognition,https://arxiv.org/pdf/1810.09951v1.pdf
PASCAL VOC 2012,,# 11,Mean IoU,79.00%,Deeplab-v2 with Lovasz-Softmax loss,-,Semantic Segmentation,The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks,/paper/the-lovasz-softmax-loss-a-tractable-surrogate,https://arxiv.org/pdf/1705.08790v2.pdf
COCO Visual Question Answering (VQA) real images 2.0 open ended,,# 1,Percentage correct,70.34,Up-Down,-,Visual Question Answering,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,/paper/bottom-up-and-top-down-attention-for-image,https://arxiv.org/pdf/1707.07998v3.pdf
SemEvalCQA,,# 4,[emailÂ protected],0.7509999999999999,Kelp,-,Question Answering,KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers,/paper/kelp-at-semeval-2016-task-3-learning-semantic,https://aclweb.org/anthology/S16-1172
SemEvalCQA,,# 2,MAP,0.792,Kelp,-,Question Answering,KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers,/paper/kelp-at-semeval-2016-task-3-learning-semantic,https://aclweb.org/anthology/S16-1172
MNIST,,# 7,Percentage error,0.7,Deep Fried Convnets,-,Image Classification,Deep Fried Convnets,/paper/deep-fried-convnets,https://arxiv.org/pdf/1412.7149v4.pdf
SNLI,,# 41,% Test Accuracy,83.3,600D (300+300) BiLSTM encoders,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 43,% Train Accuracy,86.4,600D (300+300) BiLSTM encoders,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 1,Parameters,2.0m,600D (300+300) BiLSTM encoders,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 38,% Test Accuracy,84.2,600D (300+300) BiLSTM encoders with intra-attention,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 50,% Train Accuracy,84.5,600D (300+300) BiLSTM encoders with intra-attention,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 1,Parameters,2.8m,600D (300+300) BiLSTM encoders with intra-attention,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 35,% Test Accuracy,85.0,600D (300+300) BiLSTM encoders with intra-attention and symbolic preproc.,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 45,% Train Accuracy,85.9,600D (300+300) BiLSTM encoders with intra-attention and symbolic preproc.,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
SNLI,,# 1,Parameters,2.8m,600D (300+300) BiLSTM encoders with intra-attention and symbolic preproc.,-,Natural Language Inference,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,/paper/learning-natural-language-inference-using,https://arxiv.org/pdf/1605.09090v1.pdf
COCO,,# 15,Bounding Box AP,42.8,Cascade R-CNN,-,Object Detection,Cascade R-CNN: Delving into High Quality Object Detection,/paper/cascade-r-cnn-delving-into-high-quality,https://arxiv.org/pdf/1712.00726v1.pdf
AG News,,# 3,Error,5.53,DRNN,-,Text Classification,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
Amazon Review Full,,# 3,Accuracy,64.43,DRNN,-,Sentiment Analysis,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
Amazon Review Polarity,,# 4,Accuracy,96.49,DRNN,-,Sentiment Analysis,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
DBpedia,,# 4,Error,0.81,DRNN,-,Text Classification,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
Yahoo! Answers,,# 1,Accuracy,76.26,DRNN,-,Text Classification,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
Yelp Binary classification,,# 5,Error,2.73,DRNN,-,Sentiment Analysis,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
Yelp Fine-grained classification,,# 4,Error,30.85,DRNN,-,Sentiment Analysis,Disconnected Recurrent Neural Networks for Text Categorization,/paper/disconnected-recurrent-neural-networks-for,https://aclweb.org/anthology/P18-1215
CIFAR-10,,# 25,Percentage correct,93.5,Exponential Linear Units,-,Image Classification,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),/paper/fast-and-accurate-deep-network-learning-by,https://arxiv.org/pdf/1511.07289v5.pdf
CIFAR-100,,# 14,Percentage correct,75.7,Exponential Linear Units,-,Image Classification,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),/paper/fast-and-accurate-deep-network-learning-by,https://arxiv.org/pdf/1511.07289v5.pdf
One Billion Word,,# 10,PPL,31.9,GCNN-14 bottleneck,-,Language Modelling,Language Modeling with Gated Convolutional Networks,/paper/language-modeling-with-gated-convolutional,https://arxiv.org/pdf/1612.08083v3.pdf
WikiText-103,,# 8,Validation perplexity,-,Gated CNN,-,Language Modelling,Language Modeling with Gated Convolutional Networks,/paper/language-modeling-with-gated-convolutional,https://arxiv.org/pdf/1612.08083v3.pdf
WikiText-103,,# 10,Test perplexity,37.2,Gated CNN,-,Language Modelling,Language Modeling with Gated Convolutional Networks,/paper/language-modeling-with-gated-convolutional,https://arxiv.org/pdf/1612.08083v3.pdf
IWSLT2015 English-German,,# 3,BLEU score,27.99,Pervasive Attention,-,Machine Translation,Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction,/paper/pervasive-attention-2d-convolutional-neural-1,https://arxiv.org/pdf/1808.03867v3.pdf
IWSLT2015 German-English,,# 2,BLEU score,34.18,Pervasive Attention,-,Machine Translation,Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction,/paper/pervasive-attention-2d-convolutional-neural-1,https://arxiv.org/pdf/1808.03867v3.pdf
Mushroom,,# 2,Cumulative regret,1.92,NeuralLinear FullPosterior-MR,-,Multi-Armed Bandits,Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,/paper/deep-bayesian-bandits-showdown-an-empirical,https://arxiv.org/pdf/1802.09127v1.pdf
Mushroom,,# 1,Cumulative regret,1.82,Linear FullPosterior-MR,-,Multi-Armed Bandits,Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,/paper/deep-bayesian-bandits-showdown-an-empirical,https://arxiv.org/pdf/1802.09127v1.pdf
PASCAL VOC 2007,,# 3,MAP,82.7%,CoupleNet,-,Object Detection,CoupleNet: Coupling Global Structure with Local Parts for Object Detection,/paper/couplenet-coupling-global-structure-with,https://arxiv.org/pdf/1708.02863v1.pdf
Citeseer,,# 5,Accuracy,70.3%,GCN,-,Node Classification,Semi-Supervised Classification with Graph Convolutional Networks,/paper/semi-supervised-classification-with-graph,https://arxiv.org/pdf/1609.02907v4.pdf
Cora,,# 4,Accuracy,81.5%,GCN,-,Node Classification,Semi-Supervised Classification with Graph Convolutional Networks,/paper/semi-supervised-classification-with-graph,https://arxiv.org/pdf/1609.02907v4.pdf
Cora,,# 5,Accuracy,81.5%,Graph-CNN,-,Document Classification,Semi-Supervised Classification with Graph Convolutional Networks,/paper/semi-supervised-classification-with-graph,https://arxiv.org/pdf/1609.02907v4.pdf
NELL,,# 1,Accuracy,66.0%,GCN,-,Node Classification,Semi-Supervised Classification with Graph Convolutional Networks,/paper/semi-supervised-classification-with-graph,https://arxiv.org/pdf/1609.02907v4.pdf
Pubmed,,# 4,Accuracy,79.0%,GCN,-,Node Classification,Semi-Supervised Classification with Graph Convolutional Networks,/paper/semi-supervised-classification-with-graph,https://arxiv.org/pdf/1609.02907v4.pdf
WMT2014 English-French,,# 31,BLEU score,41.0*,RNMT+,-,Machine Translation,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,/paper/the-best-of-both-worlds-combining-recent,https://arxiv.org/pdf/1804.09849v2.pdf
WMT2014 English-German,,# 24,BLEU score,28.5*,RNMT+,-,Machine Translation,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,/paper/the-best-of-both-worlds-combining-recent,https://arxiv.org/pdf/1804.09849v2.pdf
BRATS-2013,,# 1,Dice Score,0.88,InputCascadeCNN,-,Brain Tumor Segmentation,Brain Tumor Segmentation with Deep Neural Networks,/paper/brain-tumor-segmentation-with-deep-neural,https://arxiv.org/pdf/1505.03540v3.pdf
CIFAR-10,,# 4,Inception score,8.25,AC-GAN,-,Conditional Image Generation,Conditional Image Synthesis With Auxiliary Classifier GANs,/paper/conditional-image-synthesis-with-auxiliary,https://arxiv.org/pdf/1610.09585v4.pdf
ImageNet 128x128,,# 6,Inception score,28.5,AC-GAN,-,Conditional Image Generation,Conditional Image Synthesis With Auxiliary Classifier GANs,/paper/conditional-image-synthesis-with-auxiliary,https://arxiv.org/pdf/1610.09585v4.pdf
SentEval,,# 1,MRPC,76.2/83.1,InferSent,-,Semantic Textual Similarity,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
SentEval,,# 2,SICK-R,0.884,InferSent,-,Semantic Textual Similarity,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
SentEval,,# 2,SICK-E,86.3,InferSent,-,Semantic Textual Similarity,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
SentEval,,# 1,STS,75.8/75.5,InferSent,-,Semantic Textual Similarity,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
SNLI,,# 37,% Test Accuracy,84.5,4096D BiLSTM with max-pooling,-,Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
SNLI,,# 46,% Train Accuracy,85.6,4096D BiLSTM with max-pooling,-,Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
SNLI,,# 1,Parameters,40m,4096D BiLSTM with max-pooling,-,Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
XNLI Zero-Shot English-to-French,,# 3,Accuracy,60.3%,X-CBOW,-,Cross-Lingual Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
XNLI Zero-Shot English-to-French,,# 2,Accuracy,67.7%,X-BiLSTM,-,Cross-Lingual Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
XNLI Zero-Shot English-to-German,,# 4,Accuracy,61.0%,X-CBOW,-,Cross-Lingual Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
XNLI Zero-Shot English-to-German,,# 3,Accuracy,67.7%,X-BiLSTM,-,Cross-Lingual Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
XNLI Zero-Shot English-to-Spanish,,# 3,Accuracy,68.7%,X-BiLSTM,-,Cross-Lingual Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
XNLI Zero-Shot English-to-Spanish,,# 4,Accuracy,60.7%,X-CBOW,-,Cross-Lingual Natural Language Inference,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,/paper/supervised-learning-of-universal-sentence,https://arxiv.org/pdf/1705.02364v5.pdf
IMDb,,# 6,Accuracy,94.1,Virtual adversarial training,-,Sentiment Analysis,Adversarial Training Methods for Semi-Supervised Text Classification,/paper/adversarial-training-methods-for-semi,https://arxiv.org/pdf/1605.07725v3.pdf
IMDb,,# 11,Accuracy,88.9,CNN+LSTM,-,Sentiment Analysis,On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis,/paper/on-the-role-of-text-preprocessing-in-neural,https://arxiv.org/pdf/1707.01780v3.pdf
Ohsumed,,# 3,Accuracy,36.2,CNN+Lowercased,-,Text Classification,On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis,/paper/on-the-role-of-text-preprocessing-in-neural,https://arxiv.org/pdf/1707.01780v3.pdf
SST-2 Binary classification,,# 6,Accuracy,91.2,CNN,-,Sentiment Analysis,On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis,/paper/on-the-role-of-text-preprocessing-in-neural,https://arxiv.org/pdf/1707.01780v3.pdf
North American English,,# 1,Mean Opinion Score,4.526,Tacotron 2,-,Speech Synthesis,Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions,/paper/natural-tts-synthesis-by-conditioning-wavenet,https://arxiv.org/pdf/1712.05884v2.pdf
North American English,,# 2,Mean Opinion Score,4.341,WaveNet (Linguistic),-,Speech Synthesis,Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions,/paper/natural-tts-synthesis-by-conditioning-wavenet,https://arxiv.org/pdf/1712.05884v2.pdf
COCO,,# 1,Bounding Box AP,48.4,TridentNet,-,Object Detection,Scale-Aware Trident Networks for Object Detection,/paper/scale-aware-trident-networks-for-object,https://arxiv.org/pdf/1901.01892v1.pdf
Labeled Faces in the Wild,,# 5,Accuracy,99.47%,DeepId2+,-,Face Verification,"Deeply learned face representations are sparse, selective, and robust",/paper/deeply-learned-face-representations-are,https://arxiv.org/pdf/1412.1265v1.pdf
YouTube Faces DB,,# 9,Accuracy,93.2%,DeepId2+,-,Face Verification,"Deeply learned face representations are sparse, selective, and robust",/paper/deeply-learned-face-representations-are,https://arxiv.org/pdf/1412.1265v1.pdf
Indian Pines,,# 2,Overall Accuracy,96.12%,CNN-MRF,-,Hyperspectral Image Classification,Hyperspectral Image Classification with Markov Random Fields and a Convolutional Neural Network,/paper/hyperspectral-image-classification-with-1,https://arxiv.org/pdf/1705.00727v2.pdf
Pavia University,,# 3,Overall Accuracy,96.18%,CNN-MRF,-,Hyperspectral Image Classification,Hyperspectral Image Classification with Markov Random Fields and a Convolutional Neural Network,/paper/hyperspectral-image-classification-with-1,https://arxiv.org/pdf/1705.00727v2.pdf
TREC Robust04,,# 8,MAP,0.27899999999999997,DRMM,-,Ad-Hoc Information Retrieval,A Deep Relevance Matching Model for Ad-hoc Retrieval,/paper/a-deep-relevance-matching-model-for-ad-hoc,https://arxiv.org/pdf/1711.08611v1.pdf
VOT2017/18,,# 4,Expected Average Overlap (EAO),0.337,SA Siam R,-,Visual Object Tracking,Towards a Better Match in Siamese Network Based Visual Object Tracker,/paper/towards-a-better-match-in-siamese-network,https://arxiv.org/pdf/1809.01368v1.pdf
DukeMTMC-reID,,# 2,Rank-1,88.7,MGN,-,Person Re-Identification,Learning Discriminative Features with Multiple Granularities for Person Re-Identification,/paper/learning-discriminative-features-with,https://arxiv.org/pdf/1804.01438v3.pdf
DukeMTMC-reID,,# 2,MAP,78.4,MGN,-,Person Re-Identification,Learning Discriminative Features with Multiple Granularities for Person Re-Identification,/paper/learning-discriminative-features-with,https://arxiv.org/pdf/1804.01438v3.pdf
Market-1501,,# 1,Rank-1,95.7,MGN,-,Person Re-Identification,Learning Discriminative Features with Multiple Granularities for Person Re-Identification,/paper/learning-discriminative-features-with,https://arxiv.org/pdf/1804.01438v3.pdf
Market-1501,,# 2,MAP,86.9,MGN,-,Person Re-Identification,Learning Discriminative Features with Multiple Granularities for Person Re-Identification,/paper/learning-discriminative-features-with,https://arxiv.org/pdf/1804.01438v3.pdf
AFLW2000,,# 3,MAE,9.116,FAN (12 points),-,Head Pose Estimation,"How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)",/paper/how-far-are-we-from-solving-the-2d-3d-face,https://arxiv.org/pdf/1703.07332v3.pdf
BIWI,,# 2,MAE,7.882000000000001,FAN (12 points),-,Head Pose Estimation,"How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)",/paper/how-far-are-we-from-solving-the-2d-3d-face,https://arxiv.org/pdf/1703.07332v3.pdf
Sequential MNIST,,# 2,Unpermuted Accuracy,99%,BN LSTM,-,Sequential Image Classification,Recurrent Batch Normalization,/paper/recurrent-batch-normalization,https://arxiv.org/pdf/1603.09025v5.pdf
Sequential MNIST,,# 1,Permuted Accuracy,95.4%,BN LSTM,-,Sequential Image Classification,Recurrent Batch Normalization,/paper/recurrent-batch-normalization,https://arxiv.org/pdf/1603.09025v5.pdf
Text8,,# 9,Bit per Character (BPC),1.36,BN LSTM,-,Language Modelling,Recurrent Batch Normalization,/paper/recurrent-batch-normalization,https://arxiv.org/pdf/1603.09025v5.pdf
Text8,,# 1,Number of params,16M,BN LSTM,-,Language Modelling,Recurrent Batch Normalization,/paper/recurrent-batch-normalization,https://arxiv.org/pdf/1603.09025v5.pdf
Quora Question Pairs,,# 4,Accuracy,88.4,pt-DecAtt,-,Paraphrase Identification,Neural Paraphrase Identification of Questions with Noisy Pretraining,/paper/neural-paraphrase-identification-of-questions,https://arxiv.org/pdf/1704.04565v2.pdf
Penn Treebank (Word Level),,# 12,Validation perplexity,57.1,2-layer skip-LSTM + dropout tuning,-,Language Modelling,Pushing the bounds of dropout,/paper/pushing-the-bounds-of-dropout,https://arxiv.org/pdf/1805.09208v2.pdf
Penn Treebank (Word Level),,# 14,Test perplexity,55.3,2-layer skip-LSTM + dropout tuning,-,Language Modelling,Pushing the bounds of dropout,/paper/pushing-the-bounds-of-dropout,https://arxiv.org/pdf/1805.09208v2.pdf
Penn Treebank (Word Level),,# 1,Params,24M,2-layer skip-LSTM + dropout tuning,-,Language Modelling,Pushing the bounds of dropout,/paper/pushing-the-bounds-of-dropout,https://arxiv.org/pdf/1805.09208v2.pdf
BSD100 - 4x upscaling,,# 4,PSNR,27.72,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
BSD100 - 4x upscaling,,# 8,SSIM,0.7409,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
BSD100 - 4x upscaling,,# 7,MOS,,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Manga109 - 4x upscaling,,# 3,PSNR,31.15,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Manga109 - 4x upscaling,,# 3,SSIM,0.916,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Set14 - 4x upscaling,,# 6,PSNR,28.81,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Set14 - 4x upscaling,,# 9,SSIM,0.7868,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Set5 - 4x upscaling,,# 4,PSNR,32.47,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Set5 - 4x upscaling,,# 6,SSIM,0.8983,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Set5 - 4x upscaling,,# 7,MOS,,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Urban100 - 4x upscaling,,# 6,PSNR,26.6,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
Urban100 - 4x upscaling,,# 5,SSIM,0.8015,SRFBN,-,Image Super-Resolution,Feedback Network for Image Super-Resolution,/paper/feedback-network-for-image-super-resolution,https://arxiv.org/pdf/1903.09814v1.pdf
One Billion Word,,# 8,PPL,30.0,LSTM-8192-1024 + CNN Input,-,Language Modelling,Exploring the Limits of Language Modeling,/paper/exploring-the-limits-of-language-modeling,https://arxiv.org/pdf/1602.02410v2.pdf
One Billion Word,,# 1,Number of params,1.04B,LSTM-8192-1024 + CNN Input,-,Language Modelling,Exploring the Limits of Language Modeling,/paper/exploring-the-limits-of-language-modeling,https://arxiv.org/pdf/1602.02410v2.pdf
One Billion Word,,# 9,PPL,30.6,LSTM-8192-1024,-,Language Modelling,Exploring the Limits of Language Modeling,/paper/exploring-the-limits-of-language-modeling,https://arxiv.org/pdf/1602.02410v2.pdf
One Billion Word,,# 1,Number of params,1.8B,LSTM-8192-1024,-,Language Modelling,Exploring the Limits of Language Modeling,/paper/exploring-the-limits-of-language-modeling,https://arxiv.org/pdf/1602.02410v2.pdf
SARC (all-bal),,# 2,Accuracy,75.8,Bag-of-Bigrams,-,Sarcasm Detection,A Large Self-Annotated Corpus for Sarcasm,/paper/a-large-self-annotated-corpus-for-sarcasm,https://arxiv.org/pdf/1704.05579v4.pdf
SARC (pol-bal),,# 1,Accuracy,76.5,Bag-of-Bigrams,-,Sarcasm Detection,A Large Self-Annotated Corpus for Sarcasm,/paper/a-large-self-annotated-corpus-for-sarcasm,https://arxiv.org/pdf/1704.05579v4.pdf
SARC (pol-unbal),,# 1,Avg F1,27.0,Bag-of-Words,-,Sarcasm Detection,A Large Self-Annotated Corpus for Sarcasm,/paper/a-large-self-annotated-corpus-for-sarcasm,https://arxiv.org/pdf/1704.05579v4.pdf
PASCAL Context,,# 9,mIoU,41.3,HO CRF,-,Semantic Segmentation,Higher Order Conditional Random Fields in Deep Neural Networks,/paper/higher-order-conditional-random-fields-in,https://arxiv.org/pdf/1511.08119v4.pdf
ImageNet,,# 4,Top 1 Accuracy,82.7%,NASNET-A(6),-,Image Classification,Learning Transferable Architectures for Scalable Image Recognition,/paper/learning-transferable-architectures-for,https://arxiv.org/pdf/1707.07012v4.pdf
ImageNet,,# 3,Top 5 Accuracy,96.2%,NASNET-A(6),-,Image Classification,Learning Transferable Architectures for Scalable Image Recognition,/paper/learning-transferable-architectures-for,https://arxiv.org/pdf/1707.07012v4.pdf
SQuAD2.0,,# 62,EM,71.767,Reinforced Mnemonic Reader + Answer Verifier (single model),-,Question Answering,Read + Verify: Machine Reading Comprehension with Unanswerable Questions,/paper/read-verify-machine-reading-comprehension,https://arxiv.org/pdf/1808.05759v5.pdf
SQuAD2.0,,# 70,F1,74.295,Reinforced Mnemonic Reader + Answer Verifier (single model),-,Question Answering,Read + Verify: Machine Reading Comprehension with Unanswerable Questions,/paper/read-verify-machine-reading-comprehension,https://arxiv.org/pdf/1808.05759v5.pdf
Netflix,,# 1,RMSE,0.9099,DeepRec,-,Collaborative Filtering,Training Deep AutoEncoders for Collaborative Filtering,/paper/training-deep-autoencoders-for-collaborative,https://arxiv.org/pdf/1708.01715v3.pdf
STL-10,,# 16,Percentage correct,61.0,No more meta-parameter tuning in unsupervised sparse feature learning,-,Image Classification,No more meta-parameter tuning in unsupervised sparse feature learning,/paper/no-more-meta-parameter-tuning-in-unsupervised,https://arxiv.org/pdf/1402.5766v1.pdf
Amazon,,# 5,AUC,0.8637,Wide & Deep,-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Bing News,,# 2,AUC,0.8377,Wide & Deep,-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Bing News,,# 2,Log Loss,0.2668,Wide & Deep,-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Company*,,# 6,AUC,0.8661,Wide & Deep (FM & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Company*,,# 6,Log Loss,0.0264,Wide & Deep (FM & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Company*,,# 3,AUC,0.8673,Wide & Deep (LR & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Company*,,# 3,Log Loss,0.02634,Wide & Deep (LR & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Criteo,,# 8,AUC,0.785,Wide & Deep (FM & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Criteo,,# 6,Log Loss,0.45382,Wide & Deep (FM & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Criteo,,# 5,AUC,0.7981,Wide & Deep (LR & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Criteo,,# 8,Log Loss,0.46771999999999997,Wide & Deep (LR & DNN),-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Dianping,,# 4,AUC,0.8361,Wide & Deep,-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
Dianping,,# 3,Log Loss,0.3364,Wide & Deep,-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
MovieLens 20M,,# 5,AUC,0.7304,Wide & Deep,-,Click-Through Rate Prediction,Wide & Deep Learning for Recommender Systems,/paper/wide-deep-learning-for-recommender-systems,https://arxiv.org/pdf/1606.07792v1.pdf
DISFA,,# 1,Accuracy,99.45%,Deep CNN,-,Smile Recognition,Deep Learning For Smile Recognition,/paper/deep-learning-for-smile-recognition,https://arxiv.org/pdf/1602.00172v2.pdf
Mandarin Chinese,,# 1,Mean Opinion Score,4.08,WaveNet (L+F),-,Speech Synthesis,WaveNet: A Generative Model for Raw Audio,/paper/wavenet-a-generative-model-for-raw-audio,https://arxiv.org/pdf/1609.03499v2.pdf
Mandarin Chinese,,# 3,Mean Opinion Score,3.47,HMM-driven concatenative,-,Speech Synthesis,WaveNet: A Generative Model for Raw Audio,/paper/wavenet-a-generative-model-for-raw-audio,https://arxiv.org/pdf/1609.03499v2.pdf
Mandarin Chinese,,# 2,Mean Opinion Score,3.79,LSTM-RNN parametric,-,Speech Synthesis,WaveNet: A Generative Model for Raw Audio,/paper/wavenet-a-generative-model-for-raw-audio,https://arxiv.org/pdf/1609.03499v2.pdf
North American English,,# 3,Mean Opinion Score,4.21,WaveNet (L+F),-,Speech Synthesis,WaveNet: A Generative Model for Raw Audio,/paper/wavenet-a-generative-model-for-raw-audio,https://arxiv.org/pdf/1609.03499v2.pdf
North American English,,# 5,Mean Opinion Score,3.86,HMM-driven concatenative,-,Speech Synthesis,WaveNet: A Generative Model for Raw Audio,/paper/wavenet-a-generative-model-for-raw-audio,https://arxiv.org/pdf/1609.03499v2.pdf
North American English,,# 6,Mean Opinion Score,3.67,LSTM-RNN parametric,-,Speech Synthesis,WaveNet: A Generative Model for Raw Audio,/paper/wavenet-a-generative-model-for-raw-audio,https://arxiv.org/pdf/1609.03499v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 17,Restaurant (Acc),80.0,IARM,-,Aspect-Based Sentiment Analysis,IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis,/paper/iarm-inter-aspect-relation-modeling-with,https://aclweb.org/anthology/D18-1377
SemEval 2014 Task 4 Sub Task 2,,# 12,Laptop (Acc),73.8,IARM,-,Aspect-Based Sentiment Analysis,IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis,/paper/iarm-inter-aspect-relation-modeling-with,https://aclweb.org/anthology/D18-1377
CoNLL 2003 (English),,# 1,Accuracy,92.22,deep joint entity disambiguation w/ neural attention,-,Entity Resolution,Deep Joint Entity Disambiguation with Local Neural Attention,/paper/deep-joint-entity-disambiguation-with-local,https://arxiv.org/pdf/1704.04920v3.pdf
ADE20K,,# 5,Validation mIoU,32.31,DilatedNet,-,Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
CamVid,,# 4,Mean IoU,65.3%,Dilated Convolutions,-,Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
CamVid,,# 4,mIoU,65.3%,Dilation10,-,Real-Time Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
CamVid,,# 5,Time (ms),227,Dilation10,-,Real-Time Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
CamVid,,# 5,Frame (fps),4.4,Dilation10,-,Real-Time Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
Cityscapes,,# 13,Mean IoU,67.1%,Dilation10,-,Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
Cityscapes,,# 6,mIoU,67.1%,Dilation10,-,Real-Time Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
Cityscapes,,# 8,Time (ms),4000,Dilation10,-,Real-Time Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
Cityscapes,,# 10,Frame (fps),0.25,Dilation10,-,Real-Time Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
PASCAL VOC 2012,,# 17,Mean IoU,67.6%,Dilated Convolutions,-,Semantic Segmentation,Multi-Scale Context Aggregation by Dilated Convolutions,/paper/multi-scale-context-aggregation-by-dilated,https://arxiv.org/pdf/1511.07122v3.pdf
WMT2014 English-French,,# 3,BLEU,29.5,SMT as posterior regularization,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,/paper/unsupervised-neural-machine-translation-with,https://arxiv.org/pdf/1901.04112v1.pdf
WMT2014 English-German,,# 2,BLEU,17.0,SMT as posterior regularization,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,/paper/unsupervised-neural-machine-translation-with,https://arxiv.org/pdf/1901.04112v1.pdf
WMT2014 French-English,,# 3,BLEU,28.9,SMT as posterior regularization,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,/paper/unsupervised-neural-machine-translation-with,https://arxiv.org/pdf/1901.04112v1.pdf
WMT2014 German-English,,# 2,BLEU,20.4,SMT as posterior regularization,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,/paper/unsupervised-neural-machine-translation-with,https://arxiv.org/pdf/1901.04112v1.pdf
WMT2016 English-German,,# 3,BLEU,21.7,SMT as posterior regularization,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,/paper/unsupervised-neural-machine-translation-with,https://arxiv.org/pdf/1901.04112v1.pdf
WMT2016 German-English,,# 4,BLEU,26.3,SMT as posterior regularization,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,/paper/unsupervised-neural-machine-translation-with,https://arxiv.org/pdf/1901.04112v1.pdf
BSD100 - 4x upscaling,,# 11,PSNR,27.5,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
BSD100 - 4x upscaling,,# 17,SSIM,0.7326,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
Set14 - 4x upscaling,,# 12,PSNR,28.42,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
Set14 - 4x upscaling,,# 17,SSIM,0.7774,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
Set5 - 4x upscaling,,# 15,PSNR,31.74,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
Set5 - 4x upscaling,,# 20,SSIM,0.8869,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
Urban100 - 4x upscaling,,# 14,PSNR,25.66,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
Urban100 - 4x upscaling,,# 15,SSIM,0.7703,ENet-E,-,Image Super-Resolution,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,/paper/enhancenet-single-image-super-resolution,https://arxiv.org/pdf/1612.07919v2.pdf
CUB-200-2011,,# 4,Accuracy,88.1%,Hierarchical Semantic Embedding,-,Fine-Grained Image Classification,Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding,/paper/fine-grained-representation-learning-and,https://arxiv.org/pdf/1808.04505v1.pdf
CIFAR-10,,# 7,Percentage correct,96.69,Deep pyramidal residual network,-,Image Classification,Deep Pyramidal Residual Networks,/paper/deep-pyramidal-residual-networks,https://arxiv.org/pdf/1610.02915v4.pdf
CIFAR-10,,# 7,Percentage error,3.31,Deep pyramidal residual network,-,Image Classification,Deep Pyramidal Residual Networks,/paper/deep-pyramidal-residual-networks,https://arxiv.org/pdf/1610.02915v4.pdf
SNLI,,# 13,% Test Accuracy,88.5,Stochastic Answer Network,-,Natural Language Inference,Stochastic Answer Networks for Natural Language Inference,/paper/stochastic-answer-networks-for-natural,https://arxiv.org/pdf/1804.07888v2.pdf
SNLI,,# 16,% Train Accuracy,93.3,Stochastic Answer Network,-,Natural Language Inference,Stochastic Answer Networks for Natural Language Inference,/paper/stochastic-answer-networks-for-natural,https://arxiv.org/pdf/1804.07888v2.pdf
SNLI,,# 1,Parameters,3.5m,Stochastic Answer Network,-,Natural Language Inference,Stochastic Answer Networks for Natural Language Inference,/paper/stochastic-answer-networks-for-natural,https://arxiv.org/pdf/1804.07888v2.pdf
COMPLEXQUESTIONS,,# 1,F1,53.6,WebQA,-,Question Answering,Evaluating Semantic Parsing against a Simple Web-based Question Answering Model,/paper/evaluating-semantic-parsing-against-a-simple,https://arxiv.org/pdf/1707.04412v1.pdf
FCE,,# 5,F0.5,41.88,Bi-LSTM + charattn,-,Grammatical Error Detection,Attending to Characters in Neural Sequence Labeling Models,/paper/attending-to-characters-in-neural-sequence,https://arxiv.org/pdf/1611.04361v1.pdf
Penn Treebank,,# 11,Accuracy,97.27,Bi-LSTM + charattn,-,Part-Of-Speech Tagging,Attending to Characters in Neural Sequence Labeling Models,/paper/attending-to-characters-in-neural-sequence,https://arxiv.org/pdf/1611.04361v1.pdf
The ARRAU Corpus,,# 1,Average Precision,43.83,MR-LSTM,-,Abstract Anaphora Resolution,A Mention-Ranking Model for Abstract Anaphora Resolution,/paper/a-mention-ranking-model-for-abstract-anaphora,https://arxiv.org/pdf/1706.02256v2.pdf
ImageNet Detection,,# 2,MAP,24.3%,OverFeat,-,Object Detection,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",/paper/overfeat-integrated-recognition-localization,https://arxiv.org/pdf/1312.6229v4.pdf
Penn Treebank,,# 2,F1 score,95.13,Self-attentive encoder + ELMo,-,Constituency Parsing,Constituency Parsing with a Self-Attentive Encoder,/paper/constituency-parsing-with-a-self-attentive,https://arxiv.org/pdf/1805.01052v1.pdf
ImageNet 64x64,,# 3,Bits per byte,3.57,PixelCNN,-,Image Generation,PixelCNN Models with Auxiliary Variables for Natural Image Modeling,/paper/pixelcnn-models-with-auxiliary-variables-for,https://arxiv.org/pdf/1612.08185v4.pdf
swb_hub_500 WER fullSWBCH,,# 9,Percentage error,19.1,DNN + Dropout,-,Speech Recognition,Building DNN Acoustic Models for Large Vocabulary Speech Recognition,/paper/building-dnn-acoustic-models-for-large,https://arxiv.org/pdf/1406.7806v2.pdf
Switchboard + Hub500,,# 17,Percentage error,15.0,DNN + Dropout,-,Speech Recognition,Building DNN Acoustic Models for Large Vocabulary Speech Recognition,/paper/building-dnn-acoustic-models-for-large,https://arxiv.org/pdf/1406.7806v2.pdf
Switchboard + Hub500,,# 18,Percentage error,16.0,DNN,-,Speech Recognition,Building DNN Acoustic Models for Large Vocabulary Speech Recognition,/paper/building-dnn-acoustic-models-for-large,https://arxiv.org/pdf/1406.7806v2.pdf
SemEval 2007 Task 17,,# 1,F1,66.81,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
SemEval 2007 Task 7,,# 1,F1,86.02,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
SemEval 2013 Task 12,,# 1,F1,72.63,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
SemEval 2015 Task 13,,# 1,F1,74.46,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
SensEval 2,,# 1,F1,75.15,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
SensEval 3 Task 1,,# 6,F1,70.11,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
Supervised:,,# 10,Senseval 2,75.15,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
Supervised:,,# 8,Senseval 3,70.11,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
Supervised:,,# 6,SemEval 2007,66.81,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
Supervised:,,# 12,SemEval 2013,72.63,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
Supervised:,,# 11,SemEval 2015,74.46,"SemCor+WNGT, vocabulary reduced, ensemble",-,Word Sense Disambiguation,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,/paper/improving-the-coverage-and-the-generalization,https://arxiv.org/pdf/1811.00960v1.pdf
Labeled Faces in the Wild,,# 10,Accuracy,99.03%,SeqFace,-,Face Verification,SeqFace: Make full use of sequence information for face recognition,/paper/seqface-make-full-use-of-sequence-information,https://arxiv.org/pdf/1803.06524v2.pdf
YouTube Faces DB,,# 1,Accuracy,98.12,"SeqFace, 1 ResNet-64",-,Face Verification,SeqFace: Make full use of sequence information for face recognition,/paper/seqface-make-full-use-of-sequence-information,https://arxiv.org/pdf/1803.06524v2.pdf
CIFAR-10,,# 8,NLL Test,3.58,Conv DRAW,-,Image Generation,Towards Conceptual Compression,/paper/towards-conceptual-compression,https://arxiv.org/pdf/1604.08772v1.pdf
TIMIT,,# 10,Percentage error,17.3,RNN-CRF on 24(x3) MFSC,-,Speech Recognition,Segmental Recurrent Neural Networks for End-to-end Speech Recognition,/paper/segmental-recurrent-neural-networks-for-end,https://arxiv.org/pdf/1603.00223v2.pdf
iNaturalist,,# 1,Top 1 Accuracy,67.3%,IncResNetV2 SE,-,Image Classification,The iNaturalist Species Classification and Detection Dataset,/paper/the-inaturalist-species-classification-and,https://arxiv.org/pdf/1707.06642v2.pdf
iNaturalist,,# 1,Top 5 Accuracy,87.5%,IncResNetV2 SE,-,Image Classification,The iNaturalist Species Classification and Detection Dataset,/paper/the-inaturalist-species-classification-and,https://arxiv.org/pdf/1707.06642v2.pdf
Vid4 - 4x upscaling,,# 2,PSNR,27.12,RBPN/6-PF,-,Video Super-Resolution,Recurrent Back-Projection Network for Video Super-Resolution,/paper/recurrent-back-projection-network-for-video,https://arxiv.org/pdf/1903.10128v1.pdf
Vid4 - 4x upscaling,,# 9,SSIM,0.818,RBPN/6-PF,-,Video Super-Resolution,Recurrent Back-Projection Network for Video Super-Resolution,/paper/recurrent-back-projection-network-for-video,https://arxiv.org/pdf/1903.10128v1.pdf
Market-1501,,# 5,Rank-1,89.9,GLAD*,-,Person Re-Identification,GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval,/paper/glad-global-local-alignment-descriptor-for,https://arxiv.org/pdf/1709.04329v1.pdf
Market-1501,,# 5,MAP,73.9,GLAD*,-,Person Re-Identification,GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval,/paper/glad-global-local-alignment-descriptor-for,https://arxiv.org/pdf/1709.04329v1.pdf
SNLI,,# 40,% Test Accuracy,83.4,300D NTI-SLSTM-LSTM encoders,-,Natural Language Inference,Neural Tree Indexers for Text Understanding,/paper/neural-tree-indexers-for-text-understanding,https://arxiv.org/pdf/1607.04492v2.pdf
SNLI,,# 53,% Train Accuracy,82.5,300D NTI-SLSTM-LSTM encoders,-,Natural Language Inference,Neural Tree Indexers for Text Understanding,/paper/neural-tree-indexers-for-text-understanding,https://arxiv.org/pdf/1607.04492v2.pdf
SNLI,,# 1,Parameters,4.0m,300D NTI-SLSTM-LSTM encoders,-,Natural Language Inference,Neural Tree Indexers for Text Understanding,/paper/neural-tree-indexers-for-text-understanding,https://arxiv.org/pdf/1607.04492v2.pdf
SNLI,,# 19,% Test Accuracy,87.3,300D Full tree matching NTI-SLSTM-LSTM w/ global attention,-,Natural Language Inference,Neural Tree Indexers for Text Understanding,/paper/neural-tree-indexers-for-text-understanding,https://arxiv.org/pdf/1607.04492v2.pdf
SNLI,,# 39,% Train Accuracy,88.5,300D Full tree matching NTI-SLSTM-LSTM w/ global attention,-,Natural Language Inference,Neural Tree Indexers for Text Understanding,/paper/neural-tree-indexers-for-text-understanding,https://arxiv.org/pdf/1607.04492v2.pdf
SNLI,,# 1,Parameters,3.2m,300D Full tree matching NTI-SLSTM-LSTM w/ global attention,-,Natural Language Inference,Neural Tree Indexers for Text Understanding,/paper/neural-tree-indexers-for-text-understanding,https://arxiv.org/pdf/1607.04492v2.pdf
ACE 2004,,# 1,F1,73.3,Neural transition-based model,-,Nested Mention Recognition,A Neural Transition-based Model for Nested Mention Recognition,/paper/a-neural-transition-based-model-for-nested,https://arxiv.org/pdf/1810.01808v1.pdf
ACE 2004,,# 1,F1,73.3,Neural transition-based model,-,Nested Named Entity Recognition,A Neural Transition-based Model for Nested Mention Recognition,/paper/a-neural-transition-based-model-for-nested,https://arxiv.org/pdf/1810.01808v1.pdf
ACE 2005,,# 1,F1,73.0,Neural transition-based model,-,Nested Mention Recognition,A Neural Transition-based Model for Nested Mention Recognition,/paper/a-neural-transition-based-model-for-nested,https://arxiv.org/pdf/1810.01808v1.pdf
Penn Treebank,,# 1,POS,97.97,jPTDP,-,Dependency Parsing,An improved neural network model for joint POS tagging and dependency parsing,/paper/an-improved-neural-network-model-for-joint,https://arxiv.org/pdf/1807.03955v2.pdf
Penn Treebank,,# 4,UAS,94.51,jPTDP,-,Dependency Parsing,An improved neural network model for joint POS tagging and dependency parsing,/paper/an-improved-neural-network-model-for-joint,https://arxiv.org/pdf/1807.03955v2.pdf
Penn Treebank,,# 3,LAS,92.87,jPTDP,-,Dependency Parsing,An improved neural network model for joint POS tagging and dependency parsing,/paper/an-improved-neural-network-model-for-joint,https://arxiv.org/pdf/1807.03955v2.pdf
Second dialogue state tracking challenge,,# 1,Request,97.5,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Second dialogue state tracking challenge,,# 3,Area,-,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Second dialogue state tracking challenge,,# 3,Food,-,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Second dialogue state tracking challenge,,# 4,Price,-,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Second dialogue state tracking challenge,,# 2,Joint,74.5,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Wizard-of-Oz,,# 1,Request,97.1,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Wizard-of-Oz,,# 2,Joint,88.1,Zhong et al.,-,Dialogue State Tracking,Global-Locally Self-Attentive Dialogue State Tracker,/paper/global-locally-self-attentive-dialogue-state,https://arxiv.org/pdf/1805.09655v3.pdf
Reverb,,# 1,Accuracy,73%,Weakly Supervised Embeddings,-,Question Answering,Open Question Answering with Weakly Supervised Embedding Models,/paper/open-question-answering-with-weakly,https://arxiv.org/pdf/1404.4326v1.pdf
WebQuestions,,# 3,F1,29.7%,Weakly Supervised Embeddings,-,Question Answering,Open Question Answering with Weakly Supervised Embedding Models,/paper/open-question-answering-with-weakly,https://arxiv.org/pdf/1404.4326v1.pdf
MNIST,,# 3,Accuracy,95.9,Adversarial AE,-,Unsupervised MNIST,Adversarial Autoencoders,/paper/adversarial-autoencoders,https://arxiv.org/pdf/1511.05644v2.pdf
MNIST,,# 3,Accuracy,95.9,Adversarial AE,-,Unsupervised image classification,Adversarial Autoencoders,/paper/adversarial-autoencoders,https://arxiv.org/pdf/1511.05644v2.pdf
PASCAL VOC 2007,,# 16,MAP,70.0%,FRCN,-,Object Detection,Fast R-CNN,/paper/fast-r-cnn,https://arxiv.org/pdf/1504.08083v2.pdf
CoNLL 2003 (English),,# 12,F1,91.56,LSTM with dynamic skip,-,Named Entity Recognition (NER),Long Short-Term Memory with Dynamic Skip Connections,/paper/long-short-term-memory-with-dynamic-skip,https://arxiv.org/pdf/1811.03873v1.pdf
IMDb,,# 10,Accuracy,90.1,LSTM with dynamic skip,-,Sentiment Analysis,Long Short-Term Memory with Dynamic Skip Connections,/paper/long-short-term-memory-with-dynamic-skip,https://arxiv.org/pdf/1811.03873v1.pdf
SVNH-to-MNIST,,# 4,Classification Accuracy,73.6%,DANN,-,Unsupervised Image-To-Image Translation,Unsupervised Domain Adaptation by Backpropagation,/paper/unsupervised-domain-adaptation-by,https://arxiv.org/pdf/1409.7495v2.pdf
Atari 2600 Freeway,,# 12,Score,29.9,Sarsa-Îµ,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Freeway,,# 23,Score,0.0,Sarsa-Ï-EB,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Frostbite,,# 10,Score,2770.1,Sarsa-Ï-EB,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Frostbite,,# 15,Score,1394.3,Sarsa-Îµ,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Montezuma's Revenge,,# 5,Score,2745.4,Sarsa-Ï-EB,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Montezuma's Revenge,,# 7,Score,399.5,Sarsa-Îµ,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Q*Bert,,# 22,Score,4111.8,Sarsa-Ï-EB,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Q*Bert,,# 23,Score,3895.3,Sarsa-Îµ,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Venture,,# 5,Score,1169.2,Sarsa-Ï-EB,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
Atari 2600 Venture,,# 27,Score,0.0,Sarsa-Îµ,-,Atari Games,Count-Based Exploration in Feature Space for Reinforcement Learning,/paper/count-based-exploration-in-feature-space-for,https://arxiv.org/pdf/1706.08090v1.pdf
CityPersons,,# 6,Reasonable MR^-2,14.8,FRCNN+Seg,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 3,Small MR^-2,22.6,FRCNN+Seg,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 3,Medium MR^-2,6.7,FRCNN+Seg,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 4,Large MR^-2,8.0,FRCNN+Seg,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 7,Reasonable MR^-2,15.4,FRCNN,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 4,Small MR^-2,25.6,FRCNN,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 4,Medium MR^-2,7.2,FRCNN,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
CityPersons,,# 3,Large MR^-2,7.9,FRCNN,-,Pedestrian Detection,CityPersons: A Diverse Dataset for Pedestrian Detection,/paper/citypersons-a-diverse-dataset-for-pedestrian,https://arxiv.org/pdf/1702.05693v1.pdf
LibriSpeech test-other,,# 7,Word Error Rate (WER),16.5,Snips,-,Speech Recognition,Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces,/paper/snips-voice-platform-an-embedded-spoken,https://arxiv.org/pdf/1805.10190v3.pdf
SQuAD1.1,,# 99,EM,71.4,SRU,-,Question Answering,Simple Recurrent Units for Highly Parallelizable Recurrence,/paper/simple-recurrent-units-for-highly,https://arxiv.org/pdf/1709.02755v5.pdf
SQuAD1.1,,# 98,F1,80.2,SRU,-,Question Answering,Simple Recurrent Units for Highly Parallelizable Recurrence,/paper/simple-recurrent-units-for-highly,https://arxiv.org/pdf/1709.02755v5.pdf
WMT2014 English-German,,# 7,BLEU score,28.4,Transformer + SRU,-,Machine Translation,Simple Recurrent Units for Highly Parallelizable Recurrence,/paper/simple-recurrent-units-for-highly,https://arxiv.org/pdf/1709.02755v5.pdf
"CIFAR-10, 4000 Labels",,# 7,Accuracy,79.6,Î-model,-,Semi-Supervised Image Classification,Semi-Supervised Learning with Ladder Networks,/paper/semi-supervised-learning-with-ladder-networks,https://arxiv.org/pdf/1507.02672v2.pdf
GPS,,# 1,Accuracy,88%,Support Vector Machines,-,Trajectory Prediction,Inferring hybrid transportation modes from sparse GPS data using a moving window SVM classification,/paper/inferring-hybrid-transportation-modes-from,https://www.sciencedirect.com/science/article/pii/S0198971512000543/pdfft?md5=94446acd89554d6c2de6789dbdf8ad3f&pid=1-s2.0-S0198971512000543-main.pdf
Sentihood,,# 4,Aspect,78.18,Sentic LSTM + TA + SA,-,Aspect-Based Sentiment Analysis,Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM,/paper/targeted-aspect-based-sentiment-analysis-via,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16541/16152
Sentihood,,# 4,Sentiment,89.32,Sentic LSTM + TA + SA,-,Aspect-Based Sentiment Analysis,Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM,/paper/targeted-aspect-based-sentiment-analysis-via,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16541/16152
WMT2014 English-French,,# 5,BLEU score,41.4,Weighted Transformer (large),-,Machine Translation,Weighted Transformer Network for Machine Translation,/paper/weighted-transformer-network-for-machine,https://arxiv.org/pdf/1711.02132v1.pdf
WMT2014 English-German,,# 6,BLEU score,28.9,Weighted Transformer (large),-,Machine Translation,Weighted Transformer Network for Machine Translation,/paper/weighted-transformer-network-for-machine,https://arxiv.org/pdf/1711.02132v1.pdf
SQuAD1.1,,# 39,EM,78.852,DCN+ (ensemble),-,Question Answering,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,/paper/dcn-mixed-objective-and-deep-residual,https://arxiv.org/pdf/1711.00106v2.pdf
SQuAD1.1,,# 40,F1,85.99600000000001,DCN+ (ensemble),-,Question Answering,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,/paper/dcn-mixed-objective-and-deep-residual,https://arxiv.org/pdf/1711.00106v2.pdf
SQuAD1.1,,# 74,EM,74.866,DCN+ (single model),-,Question Answering,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,/paper/dcn-mixed-objective-and-deep-residual,https://arxiv.org/pdf/1711.00106v2.pdf
SQuAD1.1,,# 72,F1,82.806,DCN+ (single model),-,Question Answering,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,/paper/dcn-mixed-objective-and-deep-residual,https://arxiv.org/pdf/1711.00106v2.pdf
WikiText-103,,# 13,Test perplexity,48.7,LSTM,-,Language Modelling,Improving Neural Language Models with a Continuous Cache,/paper/improving-neural-language-models-with-a,https://arxiv.org/pdf/1612.04426v1.pdf
WikiText-103,,# 11,Test perplexity,40.8,Neural cache model,-,Language Modelling,Improving Neural Language Models with a Continuous Cache,/paper/improving-neural-language-models-with-a,https://arxiv.org/pdf/1612.04426v1.pdf
Cityscapes,,# 10,mIoU,58.3%,ENet,-,Real-Time Semantic Segmentation,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,/paper/enet-a-deep-neural-network-architecture-for,https://arxiv.org/pdf/1606.02147v1.pdf
Cityscapes,,# 1,Time (ms),13,ENet,-,Real-Time Semantic Segmentation,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,/paper/enet-a-deep-neural-network-architecture-for,https://arxiv.org/pdf/1606.02147v1.pdf
Cityscapes,,# 1,Frame (fps),76.9,ENet,-,Real-Time Semantic Segmentation,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,/paper/enet-a-deep-neural-network-architecture-for,https://arxiv.org/pdf/1606.02147v1.pdf
Multi-Domain Sentiment Dataset,,# 1,DVD,81.0,Distributional Correspondence Indexing,-,Sentiment Analysis,Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments,/paper/revisiting-distributional-correspondence,https://arxiv.org/pdf/1810.09311v1.pdf
Multi-Domain Sentiment Dataset,,# 1,Books,81.4,Distributional Correspondence Indexing,-,Sentiment Analysis,Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments,/paper/revisiting-distributional-correspondence,https://arxiv.org/pdf/1810.09311v1.pdf
Multi-Domain Sentiment Dataset,,# 5,Electronics,8506.0,Distributional Correspondence Indexing,-,Sentiment Analysis,Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments,/paper/revisiting-distributional-correspondence,https://arxiv.org/pdf/1810.09311v1.pdf
Multi-Domain Sentiment Dataset,,# 1,Kitchen,85.9,Distributional Correspondence Indexing,-,Sentiment Analysis,Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments,/paper/revisiting-distributional-correspondence,https://arxiv.org/pdf/1810.09311v1.pdf
Multi-Domain Sentiment Dataset,,# 1,Average,83.3,Distributional Correspondence Indexing,-,Sentiment Analysis,Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments,/paper/revisiting-distributional-correspondence,https://arxiv.org/pdf/1810.09311v1.pdf
TuSimple,,# 2,Accuracy,96.50%,Pairwise pixel supervision + FCN,-,Lane Detection,Learning to Cluster for Proposal-Free Instance Segmentation,/paper/learning-to-cluster-for-proposal-free,https://arxiv.org/pdf/1803.06459v1.pdf
Indian Pines,,# 3,Overall Accuracy,90.35%,St-SS-pGRU,-,Hyperspectral Image Classification,Shorten Spatial-spectral RNN with Parallel-GRU for Hyperspectral Image Classification,/paper/shorten-spatial-spectral-rnn-with-parallel,https://arxiv.org/pdf/1810.12563v1.pdf
Pavia University,,# 1,Overall Accuracy,98.44%,St-SS-pGRU,-,Hyperspectral Image Classification,Shorten Spatial-spectral RNN with Parallel-GRU for Hyperspectral Image Classification,/paper/shorten-spatial-spectral-rnn-with-parallel,https://arxiv.org/pdf/1810.12563v1.pdf
SemEval-2010 Task 8,,# 1,F1,87.1,TRE,-,Relation Classification,Improving Relation Extraction by Pre-trained Language Representations,/paper/improving-relation-extraction-by-pre-trained,https://openreview.net/pdf?id=BJgrxbqp67
TACRED,,# 1,F1,67.4,TRE,-,Relation Extraction,Improving Relation Extraction by Pre-trained Language Representations,/paper/improving-relation-extraction-by-pre-trained,https://openreview.net/pdf?id=BJgrxbqp67
TREC-50,,# 1,Error,2.8,Rules,-,Text Classification,High Accuracy Rule-based Question Classification using Question Syntax and Semantics,/paper/high-accuracy-rule-based-question,https://aclweb.org/anthology/C16-1116
WMT2016 Czech-English,,# 1,BLEU score,31.4,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 English-Czech,,# 1,BLEU score,25.8,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 English-German,,# 1,BLEU score,34.2,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 English-Romanian,,# 5,BLEU score,28.1,BiGRU,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 English-Russian,,# 1,BLEU score,26.0,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 German-English,,# 1,BLEU score,38.6,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 Romanian-English,,# 2,BLEU score,33.3,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
WMT2016 Russian-English,,# 1,BLEU score,28.0,Attentional encoder-decoder + BPE,-,Machine Translation,Edinburgh Neural Machine Translation Systems for WMT 16,/paper/edinburgh-neural-machine-translation-systems,https://arxiv.org/pdf/1606.02891v2.pdf
CR,,# 2,Accuracy,87.45,USE_T+CNN (w2v w.e.),-,Sentiment Analysis,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
MPQA,,# 1,Accuracy,88.14,USE_T+DAN (w2v w.e.),-,Sentiment Analysis,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
MR,,# 5,Accuracy,81.59,USE_T+CNN,-,Sentiment Analysis,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
SST-2 Binary classification,,# 14,Accuracy,87.21,USE_T+CNN (lrn w.e.),-,Sentiment Analysis,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
STS Benchmark,,# 1,Pearson Correlation,0.782,USE_T,-,Semantic Textual Similarity,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
SUBJ,,# 4,Accuracy,93.9,USE,-,Subjectivity Analysis,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
TREC-6,,# 1,Error,1.93,USE_T+CNN,-,Text Classification,Universal Sentence Encoder,/paper/universal-sentence-encoder,https://arxiv.org/pdf/1803.11175v2.pdf
Yahoo Questions,,# 2,NLL,327.5,SA-VAE,-,Text Generation,Semi-Amortized Variational Autoencoders,/paper/semi-amortized-variational-autoencoders,https://arxiv.org/pdf/1802.02550v7.pdf
Yahoo Questions,,# 2,KL,7.19,SA-VAE,-,Text Generation,Semi-Amortized Variational Autoencoders,/paper/semi-amortized-variational-autoencoders,https://arxiv.org/pdf/1802.02550v7.pdf
Yahoo Questions,,# 2,Perplexity,60.4,SA-VAE,-,Text Generation,Semi-Amortized Variational Autoencoders,/paper/semi-amortized-variational-autoencoders,https://arxiv.org/pdf/1802.02550v7.pdf
PASCAL VOC 2007,,# 12,MAP,39.3,WSDDN-Ens,-,Weakly Supervised Object Detection,Weakly Supervised Deep Detection Networks,/paper/weakly-supervised-deep-detection-networks,https://arxiv.org/pdf/1511.02853v4.pdf
CIFAR-10,,# 48,Percentage correct,87.7,ReNet,-,Image Classification,ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks,/paper/renet-a-recurrent-neural-network-based,https://arxiv.org/pdf/1505.00393v3.pdf
MNIST,,# 5,Percentage error,0.5,ReNet,-,Image Classification,ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks,/paper/renet-a-recurrent-neural-network-based,https://arxiv.org/pdf/1505.00393v3.pdf
SVHN,,# 18,Percentage error,2.38,ReNet,-,Image Classification,ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks,/paper/renet-a-recurrent-neural-network-based,https://arxiv.org/pdf/1505.00393v3.pdf
Amazon Review Full,,# 1,Accuracy,65.83,BERT large,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Amazon Review Full,,# 4,Accuracy,62.88,BERT large finetune UDA,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Amazon Review Polarity,,# 3,Accuracy,96.5,BERT large finetune UDA,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Amazon Review Polarity,,# 1,Accuracy,97.37,BERT large,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
DBpedia,,# 10,Error,1.09,BERT large UDA,-,Text Classification,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
DBpedia,,# 1,Error,0.68,BERT large,-,Text Classification,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
IMDb,,# 1,Accuracy,95.8,BERT large finetune UDA,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
IMDb,,# 3,Accuracy,95.49,BERT large,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Yelp Binary classification,,# 2,Error,2.05,BERT large finetune UDA,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Yelp Binary classification,,# 1,Error,1.89,BERT large,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Yelp Fine-grained classification,,# 1,Error,29.32,BERT large,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
Yelp Fine-grained classification,,# 5,Error,32.08,BERT large finetune UDA,-,Sentiment Analysis,Unsupervised Data Augmentation,/paper/unsupervised-data-augmentation,https://arxiv.org/pdf/1904.12848.pdf
ICVL Hands,,# 4,Average 3D Error,7.5,REN,-,Hand Pose Estimation,Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation,/paper/region-ensemble-network-improving,https://arxiv.org/pdf/1702.02447v2.pdf
MSRA Hands,,# 5,Average 3D Error,9.8,REN,-,Hand Pose Estimation,Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation,/paper/region-ensemble-network-improving,https://arxiv.org/pdf/1702.02447v2.pdf
NYU Hands,,# 5,Average 3D Error,12.7,REN,-,Hand Pose Estimation,Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation,/paper/region-ensemble-network-improving,https://arxiv.org/pdf/1702.02447v2.pdf
WebQSP-WD,,# 1,Avg F1,0.2588,GGNN,-,Knowledge Base Question Answering,Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering,/paper/modeling-semantics-with-gated-graph-neural-1,https://aclweb.org/anthology/C18-1280
WMT2016 English-German,,# 5,BLEU,20.0,Synthetic bilingual data init,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation,/paper/unsupervised-neural-machine-translation-1,https://arxiv.org/pdf/1810.12703v1.pdf
WMT2016 German-English,,# 3,BLEU,26.7,Synthetic bilingual data init,-,Unsupervised Machine Translation,Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation,/paper/unsupervised-neural-machine-translation-1,https://arxiv.org/pdf/1810.12703v1.pdf
AG News,,# 6,Error,7.0,EXAM,-,Text Classification,Explicit Interaction Model towards Text Classification,/paper/explicit-interaction-model-towards-text,https://arxiv.org/pdf/1811.09386v1.pdf
Amazon Review Full,,# 5,Accuracy,61.9,EXAM,-,Sentiment Analysis,Explicit Interaction Model towards Text Classification,/paper/explicit-interaction-model-towards-text,https://arxiv.org/pdf/1811.09386v1.pdf
Amazon Review Polarity,,# 5,Accuracy,95.5,EXAM,-,Sentiment Analysis,Explicit Interaction Model towards Text Classification,/paper/explicit-interaction-model-towards-text,https://arxiv.org/pdf/1811.09386v1.pdf
DBpedia,,# 8,Error,1.0,EXAM,-,Text Classification,Explicit Interaction Model towards Text Classification,/paper/explicit-interaction-model-towards-text,https://arxiv.org/pdf/1811.09386v1.pdf
Yahoo! Answers,,# 2,Accuracy,74.8,EXAM,-,Text Classification,Explicit Interaction Model towards Text Classification,/paper/explicit-interaction-model-towards-text,https://arxiv.org/pdf/1811.09386v1.pdf
STL-10,,# 9,Percentage correct,70.1,Multi-Task Bayesian Optimization,-,Image Classification,Multi-Task Bayesian Optimization,/paper/multi-task-bayesian-optimization,https://papers.nips.cc/paper/5086-multi-task-bayesian-optimization.pdf
One Billion Word,,# 6,PPL,26.67,DynamicConv,-,Language Modelling,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://arxiv.org/pdf/1901.10430v2.pdf
One Billion Word,,# 1,Number of params,0.34B,DynamicConv,-,Language Modelling,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://arxiv.org/pdf/1901.10430v2.pdf
WMT2014 English-French,,# 2,BLEU score,43.2,DynamicConv,-,Machine Translation,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://arxiv.org/pdf/1901.10430v2.pdf
WMT2014 English-French,,# 3,BLEU score,43.1,LightConv,-,Machine Translation,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://arxiv.org/pdf/1901.10430v2.pdf
WMT2014 English-German,,# 2,BLEU score,29.7,DynamicConv,-,Machine Translation,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://arxiv.org/pdf/1901.10430v2.pdf
WMT2014 English-German,,# 6,BLEU score,28.9,LightConv,-,Machine Translation,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://arxiv.org/pdf/1901.10430v2.pdf
COCO,,# 19,Bounding Box AP,41.8,RefineDet512+,-,Object Detection,Single-Shot Refinement Neural Network for Object Detection,/paper/single-shot-refinement-neural-network-for,https://arxiv.org/pdf/1711.06897v3.pdf
PASCAL VOC 2007,,# 2,MAP,83.8%,RefineDet512+,-,Object Detection,Single-Shot Refinement Neural Network for Object Detection,/paper/single-shot-refinement-neural-network-for,https://arxiv.org/pdf/1711.06897v3.pdf
PanoContext,,# 4,3DIoU,74.48%,LayoutNet,-,3D Room Layouts From A Single Rgb Panorama,LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image,/paper/layoutnet-reconstructing-the-3d-room-layout,https://arxiv.org/pdf/1803.08999v1.pdf
Realtor360,,# 2,3DIoU,62.77%,LayoutNet,-,3D Room Layouts From A Single Rgb Panorama,LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image,/paper/layoutnet-reconstructing-the-3d-room-layout,https://arxiv.org/pdf/1803.08999v1.pdf
Stanford 2D-3D,,# 3,3DIoU,76.33%,LayoutNet,-,3D Room Layouts From A Single Rgb Panorama,LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image,/paper/layoutnet-reconstructing-the-3d-room-layout,https://arxiv.org/pdf/1803.08999v1.pdf
PASCAL VOC 2007,,# 14,MAP,31.3,Self-paced curriculum learning,-,Weakly Supervised Object Detection,Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self-paced Curriculum Learning,/paper/bridging-saliency-detection-to-weakly,https://arxiv.org/pdf/1703.01290v1.pdf
TempEval-3,,# 1,Temporal awareness,67.2,Ning et al.,-,Temporal Information Extraction,A Structured Learning Approach to Temporal Relation Extraction,/paper/a-structured-learning-approach-to-temporal,https://aclweb.org/anthology/D17-1108
Pascal3D+,,# 2,Mean PCK,78.6,StarMap,-,Keypoint Detection,StarMap for Category-Agnostic Keypoint and Viewpoint Estimation,/paper/starmap-for-category-agnostic-keypoint-and,https://arxiv.org/pdf/1803.09331v2.pdf
bAbi,,# 4,Accuracy (trained on 1k),73.2%,RUM,-,Question Answering,Rotational Unit of Memory,/paper/rotational-unit-of-memory,https://arxiv.org/pdf/1710.09537v1.pdf
PASCAL VOC 2012,,# 2,Mean IoU,86.9%,DeepLabv3-JFT,-,Semantic Segmentation,Rethinking Atrous Convolution for Semantic Image Segmentation,/paper/rethinking-atrous-convolution-for-semantic,https://arxiv.org/pdf/1706.05587v3.pdf
TuSimple,,# 3,Accuracy,96.4%,LaneNet,-,Lane Detection,Towards End-to-End Lane Detection: an Instance Segmentation Approach,/paper/towards-end-to-end-lane-detection-an-instance,https://arxiv.org/pdf/1802.05591v1.pdf
ShapeNet-Part,,# 3,Class Average IoU,82.0,SSCNN,-,3D Part Segmentation,SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation,/paper/syncspeccnn-synchronized-spectral-cnn-for-3d,https://arxiv.org/pdf/1612.00606v1.pdf
ShapeNet-Part,,# 5,Instance Average IoU,84.7,SSCNN,-,3D Part Segmentation,SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation,/paper/syncspeccnn-synchronized-spectral-cnn-for-3d,https://arxiv.org/pdf/1612.00606v1.pdf
PASCAL VOC 2007,,# 13,MAP,36.3,WSDDN + context,-,Weakly Supervised Object Detection,ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization,/paper/contextlocnet-context-aware-deep-network,https://arxiv.org/pdf/1609.04331v1.pdf
PASCAL VOC 2012,,# 11,MAP,35.3,WSDDN + context,-,Weakly Supervised Object Detection,ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization,/paper/contextlocnet-context-aware-deep-network,https://arxiv.org/pdf/1609.04331v1.pdf
PROMISE 2012,,# 2,Dice Score,0.78,Fully-connected CRF,-,Volumetric Medical Image Segmentation,Conditional Random Fields as Recurrent Neural Networks for 3D Medical Imaging Segmentation,/paper/conditional-random-fields-as-recurrent-neural,https://arxiv.org/pdf/1807.07464v1.pdf
ImageNet 128x128,,# 2,FID,7.7,S3 GAN,-,Conditional Image Generation,High-Fidelity Image Generation With Fewer Labels,/paper/high-fidelity-image-generation-with-fewer,https://arxiv.org/pdf/1903.02271v1.pdf
ImageNet 128x128,,# 3,Inception score,83.1,S3 GAN,-,Conditional Image Generation,High-Fidelity Image Generation With Fewer Labels,/paper/high-fidelity-image-generation-with-fewer,https://arxiv.org/pdf/1903.02271v1.pdf
BSD100 - 4x upscaling,,# 4,PSNR,27.72,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
BSD100 - 4x upscaling,,# 9,SSIM,0.74,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Manga109 - 4x upscaling,,# 6,SSIM,0.914,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Set14 - 4x upscaling,,# 5,PSNR,28.82,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Set14 - 4x upscaling,,# 10,SSIM,0.7859999999999999,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Set5 - 4x upscaling,,# 4,PSNR,32.47,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Set5 - 4x upscaling,,# 7,SSIM,0.898,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Urban100 - 4x upscaling,,# 6,SSIM,0.795,D-DBPN,-,Image Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Vid4 - 4x upscaling,,# 6,PSNR,25.37,DBPN,-,Video Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
Vid4 - 4x upscaling,,# 4,SSIM,0.737,DBPN,-,Video Super-Resolution,Deep Back-Projection Networks For Super-Resolution,/paper/deep-back-projection-networks-for-super,https://arxiv.org/pdf/1803.02735v1.pdf
BSD100 - 4x upscaling,,# 7,PSNR,27.62,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
BSD100 - 4x upscaling,,# 12,SSIM,0.7355,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
BSD68 sigma15,,# 2,PSNR,31.86,MWCNN,-,Image Denoising,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
BSD68 sigma25,,# 1,PSNR,29.41,MWCNN,-,Image Denoising,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
BSD68 sigma50,,# 1,PSNR,26.53,MWCNN,-,Image Denoising,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Set14 - 4x upscaling,,# 13,PSNR,28.41,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Set14 - 4x upscaling,,# 13,SSIM,0.7816,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Set5 - 4x upscaling,,# 9,PSNR,32.12,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Set5 - 4x upscaling,,# 10,SSIM,0.8941,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Urban100 - 4x upscaling,,# 8,PSNR,26.27,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Urban100 - 4x upscaling,,# 8,SSIM,0.789,MWCNN,-,Image Super-Resolution,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
Urban100 sigma50,,# 3,PSNR,27.42,MWCNN,-,Image Denoising,Multi-level Wavelet-CNN for Image Restoration,/paper/multi-level-wavelet-cnn-for-image-restoration,https://arxiv.org/pdf/1805.07071v2.pdf
WMT2014 English-French,,# 26,BLEU score,26.4,GRU+Attention,-,Machine Translation,Can Active Memory Replace Attention?,/paper/can-active-memory-replace-attention,https://arxiv.org/pdf/1610.08613v2.pdf
Douban,,# 3,RMSE,0.7340000000000001,GC-MC,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
Flixster,,# 2,RMSE,0.917,GC-MC,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
MovieLens 100K,,# 2,RMSE,0.91,GC-MC,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
MovieLens 100K,,# 1,RMSE,0.905,GC-MC + feat,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
MovieLens 10M,,# 4,RMSE,0.777,GC-MC,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
MovieLens 1M,,# 4,RMSE,0.8320000000000001,GC-MC,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
YahooMusic,,# 2,RMSE,20.5,GC-MC,-,Collaborative Filtering,Graph Convolutional Matrix Completion,/paper/graph-convolutional-matrix-completion,https://arxiv.org/pdf/1706.02263v2.pdf
OccludedLINEMOD,,# 2,Accuracy,76.7%,Fully-connected CRF + geometric check,-,6D Pose Estimation,Global Hypothesis Generation for 6D Object Pose Estimation,/paper/global-hypothesis-generation-for-6d-object,https://arxiv.org/pdf/1612.02287v3.pdf
Pascal3D+,,# 3,Mean PCK,68.8,CNN + viewpoint estimates,-,Keypoint Detection,Viewpoints and Keypoints,/paper/viewpoints-and-keypoints,https://arxiv.org/pdf/1411.6067v2.pdf
WN18RR,,# 1,MRR,0.4983,M3GM,-,Link Prediction,Predicting Semantic Relations using Global Graph Properties,/paper/predicting-semantic-relations-using-global,https://arxiv.org/pdf/1808.08644v1.pdf
WN18RR,,# 1,[emailÂ protected],0.5902,M3GM,-,Link Prediction,Predicting Semantic Relations using Global Graph Properties,/paper/predicting-semantic-relations-using-global,https://arxiv.org/pdf/1808.08644v1.pdf
WN18RR,,# 1,[emailÂ protected],0.4537,M3GM,-,Link Prediction,Predicting Semantic Relations using Global Graph Properties,/paper/predicting-semantic-relations-using-global,https://arxiv.org/pdf/1808.08644v1.pdf
PanoContext,,# 2,3DIoU,78.79%,CFL,-,3D Room Layouts From A Single Rgb Panorama,Corners for Layout: End-to-End Layout Recovery from 360 Images,/paper/corners-for-layout-end-to-end-layout-recovery,https://arxiv.org/pdf/1903.08094v2.pdf
MS MARCO,,# 2,Rouge-L,52.01,Deep Cascade QA,-,Question Answering,A Deep Cascade Model for Multi-Document Reading Comprehension,/paper/a-deep-cascade-model-for-multi-document,https://arxiv.org/pdf/1811.11374v1.pdf
MS MARCO,,# 1,BLEU-1,54.64,Deep Cascade QA,-,Question Answering,A Deep Cascade Model for Multi-Document Reading Comprehension,/paper/a-deep-cascade-model-for-multi-document,https://arxiv.org/pdf/1811.11374v1.pdf
"CIFAR-10, 4000 Labels",,# 2,Accuracy,93.72,Mean Teacher,-,Semi-Supervised Image Classification,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,/paper/mean-teachers-are-better-role-models-weight,https://arxiv.org/pdf/1703.01780v6.pdf
"SVHN, 1000 labels",,# 2,Accuracy,96.05,Mean Teacher,-,Semi-Supervised Image Classification,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,/paper/mean-teachers-are-better-role-models-weight,https://arxiv.org/pdf/1703.01780v6.pdf
KITTI2012,,# 1,three pixel error,6.1,AnyNet,-,Stereo Depth Estimation,Anytime Stereo Image Depth Estimation on Mobile Devices,/paper/anytime-stereo-image-depth-estimation-on,https://arxiv.org/pdf/1810.11408v2.pdf
KITTI2015,,# 1,three pixel error,6.2,AnyNet,-,Stereo Depth Estimation,Anytime Stereo Image Depth Estimation on Mobile Devices,/paper/anytime-stereo-image-depth-estimation-on,https://arxiv.org/pdf/1810.11408v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 15,Restaurant (Acc),80.63,LSTM+SynATT+TarRep,-,Aspect-Based Sentiment Analysis,Effective Attention Modeling for Aspect-Level Sentiment Classification,/paper/effective-attention-modeling-for-aspect-level,https://aclweb.org/anthology/C18-1096
SemEval 2014 Task 4 Sub Task 2,,# 7,Laptop (Acc),71.94,LSTM+SynATT+TarRep,-,Aspect-Based Sentiment Analysis,Effective Attention Modeling for Aspect-Level Sentiment Classification,/paper/effective-attention-modeling-for-aspect-level,https://aclweb.org/anthology/C18-1096
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 8,Percentage correct,58.24,SMem-VQA,-,Visual Question Answering,"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",/paper/ask-attend-and-answer-exploring-question,https://arxiv.org/pdf/1511.05234v2.pdf
WSJ eval92,,# 1,Percentage error,2.32,tdnn + chain,-,Speech Recognition,Purely sequence-trained neural networks for ASR based on lattice-free MMI,/paper/purely-sequence-trained-neural-networks-for,https://www.danielpovey.com/files/2016_interspeech_mmi.pdf
CACDVS,,# 3,Accuracy,99.20%,OE-CNN,-,Age-Invariant Face Recognition,Orthogonal Deep Features Decomposition for Age-Invariant Face Recognition,/paper/orthogonal-deep-features-decomposition-for,https://arxiv.org/pdf/1810.07599v1.pdf
MORPH Album2,,# 3,Rank-1 Recognition Rate,98.55%,OE-CNN,-,Age-Invariant Face Recognition,Orthogonal Deep Features Decomposition for Age-Invariant Face Recognition,/paper/orthogonal-deep-features-decomposition-for,https://arxiv.org/pdf/1810.07599v1.pdf
CHALL H80K,,# 1,MPJPE,55.3,ResNet,-,3D Human Pose Estimation,An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge,/paper/an-integral-pose-regression-system-for-the,https://arxiv.org/pdf/1809.06079v1.pdf
BRATS-2015,,# 1,Dice Score,85%,U-Net + more filters + data augmentation + dice-loss,-,Brain Tumor Segmentation,Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution to the BRATS 2017 Challenge,/paper/brain-tumor-segmentation-and-radiomics,https://arxiv.org/pdf/1802.10508v1.pdf
MPII Human Pose,,# 10,PCKh-0.5,82.40%,DeepCut,-,Pose Estimation,DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation,/paper/deepcut-joint-subset-partition-and-labeling,https://arxiv.org/pdf/1511.06645v2.pdf
WAF,,# 2,AOP,86.5%,DeepCut,-,Multi-Person Pose Estimation,DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation,/paper/deepcut-joint-subset-partition-and-labeling,https://arxiv.org/pdf/1511.06645v2.pdf
CoNLL 2003 (English),,# 15,F1,91.26,Yang et al.,-,Named Entity Recognition (NER),Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,/paper/transfer-learning-for-sequence-tagging-with,https://arxiv.org/pdf/1703.06345v1.pdf
Penn Treebank,,# 5,Accuracy,97.55,Yang et al.,-,Part-Of-Speech Tagging,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,/paper/transfer-learning-for-sequence-tagging-with,https://arxiv.org/pdf/1703.06345v1.pdf
CIFAR-10 Image Classification,,# 5,Percentage error,2.83,DARTS + c/o,-,Architecture Search,Learning Efficient Convolutional Networks through Network Slimming,/paper/learning-efficient-convolutional-networks,https://arxiv.org/pdf/1708.06519v1.pdf
CIFAR-10 Image Classification,,# 1,Params,3.4M,DARTS + c/o,-,Architecture Search,Learning Efficient Convolutional Networks through Network Slimming,/paper/learning-efficient-convolutional-networks,https://arxiv.org/pdf/1708.06519v1.pdf
One Billion Word,,# 14,PPL,51.3,RNN-1024 + 9 Gram,-,Language Modelling,One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,/paper/one-billion-word-benchmark-for-measuring,https://arxiv.org/pdf/1312.3005v3.pdf
One Billion Word,,# 1,Number of params,20B,RNN-1024 + 9 Gram,-,Language Modelling,One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,/paper/one-billion-word-benchmark-for-measuring,https://arxiv.org/pdf/1312.3005v3.pdf
Cityscapes Labels-to-Photo,,# 4,Class IOU,0.04,SimGAN,-,Image-to-Image Translation,Learning from Simulated and Unsupervised Images through Adversarial Training,/paper/learning-from-simulated-and-unsupervised,https://arxiv.org/pdf/1612.07828v2.pdf
Cityscapes Labels-to-Photo,,# 3,Per-class Accuracy,10%,SimGAN,-,Image-to-Image Translation,Learning from Simulated and Unsupervised Images through Adversarial Training,/paper/learning-from-simulated-and-unsupervised,https://arxiv.org/pdf/1612.07828v2.pdf
Cityscapes Labels-to-Photo,,# 8,Per-pixel Accuracy,20%,SimGAN,-,Image-to-Image Translation,Learning from Simulated and Unsupervised Images through Adversarial Training,/paper/learning-from-simulated-and-unsupervised,https://arxiv.org/pdf/1612.07828v2.pdf
Cityscapes Photo-to-Labels,,# 3,Per-pixel Accuracy,47%,SimGAN,-,Image-to-Image Translation,Learning from Simulated and Unsupervised Images through Adversarial Training,/paper/learning-from-simulated-and-unsupervised,https://arxiv.org/pdf/1612.07828v2.pdf
Cityscapes Photo-to-Labels,,# 4,Per-class Accuracy,11%,SimGAN,-,Image-to-Image Translation,Learning from Simulated and Unsupervised Images through Adversarial Training,/paper/learning-from-simulated-and-unsupervised,https://arxiv.org/pdf/1612.07828v2.pdf
Cityscapes Photo-to-Labels,,# 4,Class IOU,0.07,SimGAN,-,Image-to-Image Translation,Learning from Simulated and Unsupervised Images through Adversarial Training,/paper/learning-from-simulated-and-unsupervised,https://arxiv.org/pdf/1612.07828v2.pdf
Sintel-clean,,# 2,Average End-Point Error,4.54,LiteFlowNet,-,Optical Flow Estimation,LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation,/paper/liteflownet-a-lightweight-convolutional,https://arxiv.org/pdf/1805.07036v1.pdf
Sintel-final,,# 2,Average End-Point Error,5.38,LiteFlowNet,-,Optical Flow Estimation,LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation,/paper/liteflownet-a-lightweight-convolutional,https://arxiv.org/pdf/1805.07036v1.pdf
ISIC 2017,,# 1,Mean IoU,0.765,Automatic skin lesion segmentation with fully convolutional-deconvolutional networks,-,Lesion Segmentation,Automatic skin lesion segmentation with fully convolutional-deconvolutional networks,/paper/automatic-skin-lesion-segmentation-with-fully,https://arxiv.org/pdf/1703.05165v2.pdf
IEMOCAP,,# 1,F1,71.8,Ensemble (Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression),-,Speech Emotion Recognition,Multimodal Speech Emotion Recognition and Ambiguity Resolution,/paper/multimodal-speech-emotion-recognition-and,https://arxiv.org/pdf/1904.06022v1.pdf
PASCAL VOC 2007,,# 5,MAP,47.6,ZLDN-L,-,Weakly Supervised Object Detection,Zigzag Learning for Weakly Supervised Object Detection,/paper/zigzag-learning-for-weakly-supervised-object,https://arxiv.org/pdf/1804.09466v1.pdf
PASCAL VOC 2012,,# 4,MAP,42.9,ZLDN-L,-,Weakly Supervised Object Detection,Zigzag Learning for Weakly Supervised Object Detection,/paper/zigzag-learning-for-weakly-supervised-object,https://arxiv.org/pdf/1804.09466v1.pdf
COCO,,# 35,Bounding Box AP,33.0,YOLOv3 + Darknet-53,-,Object Detection,YOLOv3: An Incremental Improvement,/paper/yolov3-an-incremental-improvement,https://arxiv.org/pdf/1804.02767v1.pdf
COCO,,# 5,MAP,33.1%,YOLOv3-tiny,-,Real-Time Object Detection,YOLOv3: An Incremental Improvement,/paper/yolov3-an-incremental-improvement,https://arxiv.org/pdf/1804.02767v1.pdf
COCO,,# 1,FPS,220,YOLOv3-tiny,-,Real-Time Object Detection,YOLOv3: An Incremental Improvement,/paper/yolov3-an-incremental-improvement,https://arxiv.org/pdf/1804.02767v1.pdf
COCO,,# 1,MAP,55.3%,YOLOv3-416,-,Real-Time Object Detection,YOLOv3: An Incremental Improvement,/paper/yolov3-an-incremental-improvement,https://arxiv.org/pdf/1804.02767v1.pdf
COCO,,# 3,FPS,34,YOLOv3-416,-,Real-Time Object Detection,YOLOv3: An Incremental Improvement,/paper/yolov3-an-incremental-improvement,https://arxiv.org/pdf/1804.02767v1.pdf
Atari 2600 Gravitar,,# 1,Score,3906,RND,-,Atari Games,Exploration by Random Network Distillation,/paper/exploration-by-random-network-distillation,https://arxiv.org/pdf/1810.12894v1.pdf
Atari 2600 Montezuma's Revenge,,# 1,Score,8152,RND,-,Atari Games,Exploration by Random Network Distillation,/paper/exploration-by-random-network-distillation,https://arxiv.org/pdf/1810.12894v1.pdf
Atari 2600 Pitfall!,,# 2,Score,-3,RND,-,Atari Games,Exploration by Random Network Distillation,/paper/exploration-by-random-network-distillation,https://arxiv.org/pdf/1810.12894v1.pdf
Atari 2600 Private Eye,,# 2,Score,8666,RND,-,Atari Games,Exploration by Random Network Distillation,/paper/exploration-by-random-network-distillation,https://arxiv.org/pdf/1810.12894v1.pdf
Atari 2600 Solaris,,# 2,Score,3282,RND,-,Atari Games,Exploration by Random Network Distillation,/paper/exploration-by-random-network-distillation,https://arxiv.org/pdf/1810.12894v1.pdf
Atari 2600 Venture,,# 1,Score,1859,RND,-,Atari Games,Exploration by Random Network Distillation,/paper/exploration-by-random-network-distillation,https://arxiv.org/pdf/1810.12894v1.pdf
FDDB,,# 1,AP,0.991,DSFD,-,Face Detection,DSFD: Dual Shot Face Detector,/paper/dsfd-dual-shot-face-detector,https://arxiv.org/pdf/1810.10220v3.pdf
WIDER Face (Easy),,# 1,AP,0.96,DSFD,-,Face Detection,DSFD: Dual Shot Face Detector,/paper/dsfd-dual-shot-face-detector,https://arxiv.org/pdf/1810.10220v3.pdf
WIDER Face (Hard),,# 1,AP,0.9,DSFD,-,Face Detection,DSFD: Dual Shot Face Detector,/paper/dsfd-dual-shot-face-detector,https://arxiv.org/pdf/1810.10220v3.pdf
WIDER Face (Medium),,# 1,AP,0.953,DSFD,-,Face Detection,DSFD: Dual Shot Face Detector,/paper/dsfd-dual-shot-face-detector,https://arxiv.org/pdf/1810.10220v3.pdf
Atari 2600 Alien,,# 10,Score,1620.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Alien,,# 19,Score,634.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Alien,,# 13,Score,1033.4,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Alien,,# 17,Score,823.7,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Amidar,,# 8,Score,978.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Amidar,,# 16,Score,178.4,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Amidar,,# 13,Score,238.4,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Amidar,,# 19,Score,169.1,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Assault,,# 14,Score,4280.4,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Assault,,# 17,Score,3489.3,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Assault,,# 4,Score,10950.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Assault,,# 10,Score,6060.8,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asterix,,# 17,Score,4359.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asterix,,# 19,Score,3170.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asterix,,# 3,Score,364200.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asterix,,# 13,Score,16837.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asteroids,,# 13,Score,1458.7,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asteroids,,# 14,Score,1364.5,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asteroids,,# 15,Score,1193.2,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Asteroids,,# 18,Score,1021.9,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Atlantis,,# 17,Score,292491.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Atlantis,,# 18,Score,279987.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Atlantis,,# 10,Score,423252.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Atlantis,,# 16,Score,319688.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bank Heist,,# 16,Score,455.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bank Heist,,# 19,Score,312.7,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bank Heist,,# 9,Score,1004.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bank Heist,,# 14,Score,886.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Battle Zone,,# 9,Score,29900.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Battle Zone,,# 14,Score,23750.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Battle Zone,,# 8,Score,30650.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Battle Zone,,# 13,Score,24740.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Beam Rider,,# 15,Score,9743.2,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Beam Rider,,# 16,Score,8627.5,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Beam Rider,,# 2,Score,37412.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Beam Rider,,# 9,Score,17417.2,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Berzerk,,# 16,Score,585.6,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Berzerk,,# 17,Score,493.4,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Berzerk,,# 2,Score,2178.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Berzerk,,# 10,Score,1011.1,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bowling,,# 9,Score,56.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bowling,,# 12,Score,50.4,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bowling,,# 4,Score,69.6,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Bowling,,# 12,Score,50.4,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Boxing,,# 9,Score,88.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Boxing,,# 16,Score,70.3,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Boxing,,# 10,Score,79.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Boxing,,# 13,Score,73.5,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Breakout,,# 10,Score,385.5,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Breakout,,# 15,Score,354.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Breakout,,# 12,Score,368.9,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Breakout,,# 14,Score,354.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Centipede,,# 13,Score,4657.7,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Centipede,,# 17,Score,3973.9,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Centipede,,# 10,Score,5570.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Centipede,,# 18,Score,3853.5,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Chopper Command,,# 10,Score,6126.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Chopper Command,,# 12,Score,5017.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Chopper Command,,# 7,Score,8058.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Chopper Command,,# 18,Score,3495.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Crazy Climber,,# 16,Score,110763.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Crazy Climber,,# 18,Score,98128.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Crazy Climber,,# 8,Score,127853.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Crazy Climber,,# 14,Score,113782.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Demon Attack,,# 17,Score,12550.7,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Demon Attack,,# 18,Score,12149.4,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Demon Attack,,# 7,Score,73371.3,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Demon Attack,,# 10,Score,69803.4,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Double Dunk,,# 12,Score,-6.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Double Dunk,,# 13,Score,-6.6,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Double Dunk,,# 9,Score,-0.3,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Double Dunk,,# 14,Score,-10.7,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Enduro,,# 13,Score,729.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Enduro,,# 15,Score,626.7,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Enduro,,# 5,Score,2223.9,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Enduro,,# 11,Score,1216.6,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Fishing Derby,,# 17,Score,-1.6,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Fishing Derby,,# 19,Score,-4.9,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Fishing Derby,,# 9,Score,17.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Fishing Derby,,# 15,Score,3.2,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Freeway,,# 9,Score,30.8,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Freeway,,# 17,Score,26.9,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Freeway,,# 14,Score,28.8,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Freeway,,# 15,Score,28.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Frostbite,,# 16,Score,797.4,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Frostbite,,# 18,Score,496.1,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Frostbite,,# 6,Score,4038.4,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Frostbite,,# 14,Score,1448.1,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gopher,,# 15,Score,8777.4,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gopher,,# 18,Score,8190.4,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gopher,,# 2,Score,105148.4,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gopher,,# 12,Score,15253.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gravitar,,# 9,Score,473.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gravitar,,# 17,Score,298.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gravitar,,# 23,Score,200.5,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Gravitar,,# 24,Score,167.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 HERO,,# 11,Score,20437.8,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 HERO,,# 16,Score,14992.9,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 HERO,,# 14,Score,15459.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 HERO,,# 17,Score,14892.5,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ice Hockey,,# 7,Score,-1.6,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ice Hockey,,# 9,Score,-1.9,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ice Hockey,,# 2,Score,0.5,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ice Hockey,,# 10,Score,-2.5,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 James Bond,,# 10,Score,768.5,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 James Bond,,# 11,Score,697.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 James Bond,,# 13,Score,585.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 James Bond,,# 15,Score,573.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kangaroo,,# 12,Score,7259.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kangaroo,,# 14,Score,4496.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kangaroo,,# 9,Score,11204.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kangaroo,,# 18,Score,861.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Krull,,# 9,Score,8422.3,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Krull,,# 17,Score,6206.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Krull,,# 13,Score,7658.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Krull,,# 15,Score,6796.1,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kung-Fu Master,,# 15,Score,26059.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kung-Fu Master,,# 18,Score,20882.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kung-Fu Master,,# 6,Score,37484.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Kung-Fu Master,,# 11,Score,30207.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Montezuma's Revenge,,# 17,Score,47.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Montezuma's Revenge,,# 18,Score,42.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Montezuma's Revenge,,# 20,Score,24.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ms. Pacman,,# 7,Score,3085.6,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ms. Pacman,,# 17,Score,1092.3,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ms. Pacman,,# 15,Score,1241.3,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Ms. Pacman,,# 18,Score,1007.8,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Name This Game,,# 16,Score,8207.8,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Name This Game,,# 18,Score,6738.8,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Name This Game,,# 4,Score,13637.9,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Name This Game,,# 15,Score,8960.3,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Pong,,# 4,Score,19.5,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Pong,,# 9,Score,18.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Pong,,# 5,Score,19.1,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Pong,,# 8,Score,18.4,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Private Eye,,# 13,Score,207.9,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Private Eye,,# 18,Score,146.7,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Private Eye,,# 7,Score,1277.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Private Eye,,# 24,Score,-575.5,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Q*Bert,,# 14,Score,13117.3,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Q*Bert,,# 18,Score,9271.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Q*Bert,,# 12,Score,14063.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Q*Bert,,# 15,Score,11020.8,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 River Raid,,# 16,Score,7377.6,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 River Raid,,# 20,Score,4748.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 River Raid,,# 6,Score,16496.8,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 River Raid,,# 13,Score,10838.4,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Road Runner,,# 15,Score,39544.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Road Runner,,# 16,Score,35215.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Road Runner,,# 8,Score,54630.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Road Runner,,# 13,Score,43156.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Robotank,,# 5,Score,63.9,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Robotank,,# 11,Score,58.7,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Robotank,,# 10,Score,59.1,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Robotank,,# 18,Score,24.7,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Seaquest,,# 12,Score,5860.6,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Seaquest,,# 14,Score,4216.7,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Seaquest,,# 8,Score,14498.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Seaquest,,# 18,Score,1431.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Space Invaders,,# 17,Score,1692.3,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Space Invaders,,# 18,Score,1293.8,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Space Invaders,,# 5,Score,8978.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Space Invaders,,# 12,Score,2628.7,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Star Gunner,,# 15,Score,54282.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Star Gunner,,# 16,Score,52970.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Star Gunner,,# 3,Score,127073.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Star Gunner,,# 12,Score,58365.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tennis,,# 3,Score,12.2,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tennis,,# 5,Score,11.1,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tennis,,# 16,Score,-7.8,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tennis,,# 18,Score,-13.2,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Time Pilot,,# 18,Score,4870.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Time Pilot,,# 19,Score,4786.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Time Pilot,,# 11,Score,6608.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Time Pilot,,# 17,Score,4871.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tutankham,,# 18,Score,68.1,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tutankham,,# 21,Score,45.6,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tutankham,,# 15,Score,108.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Tutankham,,# 17,Score,92.2,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Up and Down,,# 17,Score,9989.9,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Up and Down,,# 20,Score,8038.5,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Up and Down,,# 11,Score,22681.3,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Up and Down,,# 13,Score,19086.9,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Venture,,# 13,Score,163.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Venture,,# 14,Score,136.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Venture,,# 21,Score,29.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Venture,,# 24,Score,21.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Video Pinball,,# 12,Score,196760.4,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Video Pinball,,# 14,Score,154414.1,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Video Pinball,,# 6,Score,447408.6,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Video Pinball,,# 7,Score,367823.7,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Wizard of Wor,,# 18,Score,2704.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Wizard of Wor,,# 20,Score,1609.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Wizard of Wor,,# 5,Score,10471.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Wizard of Wor,,# 12,Score,6201.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Zaxxon,,# 17,Score,5363.0,DQN noop,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Zaxxon,,# 19,Score,4412.0,DQN hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Zaxxon,,# 8,Score,11320.0,Prior+Duel hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
Atari 2600 Zaxxon,,# 14,Score,8593.0,DDQN (tuned) hs,-,Atari Games,Deep Reinforcement Learning with Double Q-learning,/paper/deep-reinforcement-learning-with-double-q,https://arxiv.org/pdf/1509.06461v3.pdf
CUB-200-2011,,# 3,Accuracy,88.7%,MPN-COV,-,Fine-Grained Image Classification,Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization,/paper/towards-faster-training-of-global-covariance,https://arxiv.org/pdf/1712.01034v2.pdf
FGVC Aircraft,,# 2,Accuracy,91.4%,MPN-COV,-,Fine-Grained Image Classification,Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization,/paper/towards-faster-training-of-global-covariance,https://arxiv.org/pdf/1712.01034v2.pdf
Stanford Cars,,# 2,Accuracy,93.3%,MPN-COV,-,Fine-Grained Image Classification,Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization,/paper/towards-faster-training-of-global-covariance,https://arxiv.org/pdf/1712.01034v2.pdf
COCO,,# 4,Average Precision,37.1%,Mask R-CNN,-,Instance Segmentation,Mask R-CNN,/paper/mask-r-cnn,https://arxiv.org/pdf/1703.06870v3.pdf
COCO,,# 5,Validation AP,69.2,Mask R-CNN,-,Keypoint Detection,Mask R-CNN,/paper/mask-r-cnn,https://arxiv.org/pdf/1703.06870v3.pdf
COCO,,# 4,Test AP,63.1,Mask R-CNN,-,Keypoint Detection,Mask R-CNN,/paper/mask-r-cnn,https://arxiv.org/pdf/1703.06870v3.pdf
COCO,,# 25,Bounding Box AP,39.8,Mask R-CNN,-,Object Detection,Mask R-CNN,/paper/mask-r-cnn,https://arxiv.org/pdf/1703.06870v3.pdf
MHP v1.0,,# 2,AP 0.5,52.68%,Mask R-CNN,-,Multi-Human Parsing,Mask R-CNN,/paper/mask-r-cnn,https://arxiv.org/pdf/1703.06870v3.pdf
MHP v2.0,,# 3,AP 0.5,14.90%,Mask R-CNN,-,Multi-Human Parsing,Mask R-CNN,/paper/mask-r-cnn,https://arxiv.org/pdf/1703.06870v3.pdf
WikiHop,,# 4,Test,42.9,BiDAF,-,Question Answering,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,/paper/constructing-datasets-for-multi-hop-reading,https://arxiv.org/pdf/1710.06481v2.pdf
BPI challenge '12,,# 1,Accuracy,0.76,LSTM,-,Multivariate Time Series Forecasting,Predictive Business Process Monitoring with LSTM Neural Networks,/paper/predictive-business-process-monitoring-with,https://arxiv.org/pdf/1612.02130v2.pdf
Helpdesk,,# 1,Accuracy,0.7123,LSTM,-,Multivariate Time Series Forecasting,Predictive Business Process Monitoring with LSTM Neural Networks,/paper/predictive-business-process-monitoring-with,https://arxiv.org/pdf/1612.02130v2.pdf
SemEvalCQA,,# 2,[emailÂ protected],0.755,ConvKN,-,Question Answering,ConvKN at SemEval-2016 Task 3: Answer and Question Selection for Question Answering on Arabic and English Fora,/paper/convkn-at-semeval-2016-task-3-answer-and,https://aclweb.org/anthology/S16-1138
SemEvalCQA,,# 4,MAP,0.777,ConvKN,-,Question Answering,ConvKN at SemEval-2016 Task 3: Answer and Question Selection for Question Answering on Arabic and English Fora,/paper/convkn-at-semeval-2016-task-3-answer-and,https://aclweb.org/anthology/S16-1138
Cora,,# 1,Accuracy,83.5%,ACNet,-,Document Classification,Adaptively Connected Neural Networks,/paper/adaptively-connected-neural-networks,https://arxiv.org/pdf/1904.03579v1.pdf
Florence,,# 2,Average 3D Error,1.5,Unsupervised-3DMMR,-,3D Face Reconstruction,Unsupervised Training for 3D Morphable Model Regression,/paper/unsupervised-training-for-3d-morphable-model,https://arxiv.org/pdf/1806.06098v1.pdf
WN18,,# 7,MRR,0.8220000000000001,DistMult,-,Link Prediction,Embedding Entities and Relations for Learning and Inference in Knowledge Bases,/paper/embedding-entities-and-relations-for-learning,https://arxiv.org/pdf/1412.6575v4.pdf
WN18,,# 6,[emailÂ protected],0.9359999999999999,DistMult,-,Link Prediction,Embedding Entities and Relations for Learning and Inference in Knowledge Bases,/paper/embedding-entities-and-relations-for-learning,https://arxiv.org/pdf/1412.6575v4.pdf
WN18,,# 5,[emailÂ protected],0.914,DistMult,-,Link Prediction,Embedding Entities and Relations for Learning and Inference in Knowledge Bases,/paper/embedding-entities-and-relations-for-learning,https://arxiv.org/pdf/1412.6575v4.pdf
WN18,,# 7,[emailÂ protected],0.728,DistMult,-,Link Prediction,Embedding Entities and Relations for Learning and Inference in Knowledge Bases,/paper/embedding-entities-and-relations-for-learning,https://arxiv.org/pdf/1412.6575v4.pdf
WN18,,# 1,MR,902.0,DistMult,-,Link Prediction,Embedding Entities and Relations for Learning and Inference in Knowledge Bases,/paper/embedding-entities-and-relations-for-learning,https://arxiv.org/pdf/1412.6575v4.pdf
BlogCatalog,,# 2,Accuracy,22.80%,Struc2vec,-,Node Classification,struc2vec: Learning Node Representations from Structural Identity,/paper/struc2vec-learning-node-representations-from,https://arxiv.org/pdf/1704.03165v3.pdf
BlogCatalog,,# 2,Macro-F1,0.216,Struc2vec,-,Node Classification,struc2vec: Learning Node Representations from Structural Identity,/paper/struc2vec-learning-node-representations-from,https://arxiv.org/pdf/1704.03165v3.pdf
Wikipedia,,# 2,Accuracy,21.10%,Struc2vec,-,Node Classification,struc2vec: Learning Node Representations from Structural Identity,/paper/struc2vec-learning-node-representations-from,https://arxiv.org/pdf/1704.03165v3.pdf
Wikipedia,,# 2,Macro-F1,0.190,Struc2vec,-,Node Classification,struc2vec: Learning Node Representations from Structural Identity,/paper/struc2vec-learning-node-representations-from,https://arxiv.org/pdf/1704.03165v3.pdf
Oxf105k,,# 3,MAP,82.6%,DELF+FT+ATT,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Oxf105k,,# 1,MAP,88.5%,DELF+FT+ATT+DIR+QE,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Oxf5k,,# 1,MAP,90.0%,DELF+FT+ATT+DIR+QE,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Oxf5k,,# 3,MAP,83.8%,DELF+FT+ATT,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Par106k,,# 3,Accuracy,81.7%,DELF+FT+ATT,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Par106k,,# 1,Accuracy,92.8%,DELF+FT+ATT+DIR+QE,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Par6k,,# 4,Accuracy,85.0%,DELF+FT+ATT,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
Par6k,,# 1,Accuracy,95.7%,DELF+FT+ATT+DIR+QE,-,Image Retrieval,Large-Scale Image Retrieval with Attentive Deep Local Features,/paper/large-scale-image-retrieval-with-attentive,https://arxiv.org/pdf/1612.06321v4.pdf
AG News,,# 10,Error,7.55,LEAM,-,Text Classification,Joint Embedding of Words and Labels for Text Classification,/paper/joint-embedding-of-words-and-labels-for-text,https://arxiv.org/pdf/1805.04174v1.pdf
DBpedia,,# 7,Error,0.98,LEAM,-,Text Classification,Joint Embedding of Words and Labels for Text Classification,/paper/joint-embedding-of-words-and-labels-for-text,https://arxiv.org/pdf/1805.04174v1.pdf
Yelp Binary classification,,# 13,Error,4.69,LEAM,-,Sentiment Analysis,Joint Embedding of Words and Labels for Text Classification,/paper/joint-embedding-of-words-and-labels-for-text,https://arxiv.org/pdf/1805.04174v1.pdf
Yelp Fine-grained classification,,# 9,Error,35.91,LEAM,-,Sentiment Analysis,Joint Embedding of Words and Labels for Text Classification,/paper/joint-embedding-of-words-and-labels-for-text,https://arxiv.org/pdf/1805.04174v1.pdf
CIFAR-10,,# 11,Percentage correct,96.4,Neural Architecture Search,-,Image Classification,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
CIFAR-10 Image Classification,,# 4,Percentage error,2.40,NASNet-A + c/o,-,Architecture Search,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
CIFAR-10 Image Classification,,# 1,Params,27.6M,NASNet-A + c/o,-,Architecture Search,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
Penn Treebank (Character Level),,# 7,Bit per Character (BPC),1.214,NASCell,-,Language Modelling,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
Penn Treebank (Character Level),,# 1,Number of params,16.3M,NASCell,-,Language Modelling,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
Penn Treebank (Word Level),,# 19,Test perplexity,64.0,NAS Cell,-,Language Modelling,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
Penn Treebank (Word Level),,# 1,Params,25M,NAS Cell,-,Language Modelling,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement,https://arxiv.org/pdf/1611.01578v2.pdf
BSD100 - 4x upscaling,,# 4,PSNR,27.72,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
BSD100 - 4x upscaling,,# 7,SSIM,0.7419,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Manga109 - 4x upscaling,,# 5,PSNR,31.0,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Manga109 - 4x upscaling,,# 4,SSIM,0.9151,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Set14 - 4x upscaling,,# 6,PSNR,28.81,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Set14 - 4x upscaling,,# 8,SSIM,0.7871,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Set5 - 4x upscaling,,# 4,PSNR,32.47,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Set5 - 4x upscaling,,# 5,SSIM,0.899,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Urban100 - 4x upscaling,,# 5,PSNR,26.61,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
Urban100 - 4x upscaling,,# 4,SSIM,0.8028,RDN,-,Image Super-Resolution,Residual Dense Network for Image Super-Resolution,/paper/residual-dense-network-for-image-super,https://arxiv.org/pdf/1802.08797v2.pdf
SQuAD1.1,,# 110,EM,70.387,Multi-Perspective Matching (single model),-,Question Answering,Multi-Perspective Context Matching for Machine Comprehension,/paper/multi-perspective-context-matching-for,https://arxiv.org/pdf/1612.04211v1.pdf
SQuAD1.1,,# 111,F1,78.78399999999999,Multi-Perspective Matching (single model),-,Question Answering,Multi-Perspective Context Matching for Machine Comprehension,/paper/multi-perspective-context-matching-for,https://arxiv.org/pdf/1612.04211v1.pdf
SQuAD1.1,,# 82,EM,73.765,Multi-Perspective Matching (ensemble),-,Question Answering,Multi-Perspective Context Matching for Machine Comprehension,/paper/multi-perspective-context-matching-for,https://arxiv.org/pdf/1612.04211v1.pdf
SQuAD1.1,,# 90,F1,81.257,Multi-Perspective Matching (ensemble),-,Question Answering,Multi-Perspective Context Matching for Machine Comprehension,/paper/multi-perspective-context-matching-for,https://arxiv.org/pdf/1612.04211v1.pdf
CIFAR-10,,# 46,Percentage correct,89,DCNN,-,Image Classification,ImageNet Classification with Deep Convolutional Neural Networks,/paper/imagenet-classification-with-deep,https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
CIFAR-10,,# 26,Percentage correct,93.3,BNM NiN,-,Image Classification,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network,https://arxiv.org/pdf/1511.02583v1.pdf
CIFAR-10,,# 14,Percentage error,6.75,BNM NiN,-,Image Classification,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network,https://arxiv.org/pdf/1511.02583v1.pdf
CIFAR-100,,# 22,Percentage correct,71.1,BNM NiN,-,Image Classification,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network,https://arxiv.org/pdf/1511.02583v1.pdf
CIFAR-100,,# 8,Percentage error,28.86,BNM NiN,-,Image Classification,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network,https://arxiv.org/pdf/1511.02583v1.pdf
MNIST,,# 2,Percentage error,0.2,BNM NiN,-,Image Classification,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network,https://arxiv.org/pdf/1511.02583v1.pdf
SVHN,,# 10,Percentage error,1.81,BNM NiN,-,Image Classification,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network,https://arxiv.org/pdf/1511.02583v1.pdf
Penn Treebank (Word Level),,# 7,Validation perplexity,53.9,AWD-LSTM + continuous cache pointer,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
Penn Treebank (Word Level),,# 9,Test perplexity,52.8,AWD-LSTM + continuous cache pointer,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
Penn Treebank (Word Level),,# 1,Params,24M,AWD-LSTM + continuous cache pointer,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
Penn Treebank (Word Level),,# 15,Validation perplexity,60.0,AWD-LSTM,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
Penn Treebank (Word Level),,# 17,Test perplexity,57.3,AWD-LSTM,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
Penn Treebank (Word Level),,# 1,Params,24M,AWD-LSTM,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
WikiText-2,,# 5,Validation perplexity,53.8,AWD-LSTM + continuous cache pointer,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
WikiText-2,,# 6,Test perplexity,52.0,AWD-LSTM + continuous cache pointer,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
WikiText-2,,# 1,Number of params,33M,AWD-LSTM + continuous cache pointer,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
WikiText-2,,# 12,Validation perplexity,68.6,AWD-LSTM,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
WikiText-2,,# 13,Test perplexity,65.8,AWD-LSTM,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
WikiText-2,,# 1,Number of params,33M,AWD-LSTM,-,Language Modelling,Regularizing and Optimizing LSTM Language Models,/paper/regularizing-and-optimizing-lstm-language,https://arxiv.org/pdf/1708.02182v1.pdf
SNLI,,# 43,% Test Accuracy,82.1,300D Tree-based CNN encoders,-,Natural Language Inference,Natural Language Inference by Tree-Based Convolution and Heuristic Matching,/paper/natural-language-inference-by-tree-based,https://arxiv.org/pdf/1512.08422v3.pdf
SNLI,,# 52,% Train Accuracy,83.3,300D Tree-based CNN encoders,-,Natural Language Inference,Natural Language Inference by Tree-Based Convolution and Heuristic Matching,/paper/natural-language-inference-by-tree-based,https://arxiv.org/pdf/1512.08422v3.pdf
SNLI,,# 1,Parameters,3.5m,300D Tree-based CNN encoders,-,Natural Language Inference,Natural Language Inference by Tree-Based Convolution and Heuristic Matching,/paper/natural-language-inference-by-tree-based,https://arxiv.org/pdf/1512.08422v3.pdf
AFLW2000-3D,,# 2,Mean NME,3.62%,PRN,-,Face Alignment,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,/paper/joint-3d-face-reconstruction-and-dense,https://arxiv.org/pdf/1803.07835v1.pdf
AFLW2000-3D,,# 1,Mean NME,3.9625%,PRN,-,3D Face Reconstruction,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,/paper/joint-3d-face-reconstruction-and-dense,https://arxiv.org/pdf/1803.07835v1.pdf
AFLW-LFPA,,# 1,Mean NME,2.93%,FPN,-,Face Alignment,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,/paper/joint-3d-face-reconstruction-and-dense,https://arxiv.org/pdf/1803.07835v1.pdf
Florence,,# 1,Mean NME,3.7551%,PRN,-,3D Face Reconstruction,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,/paper/joint-3d-face-reconstruction-and-dense,https://arxiv.org/pdf/1803.07835v1.pdf
Google Dataset,,# 3,F1,0.8,BiLSTM,-,Sentence Compression,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,/paper/can-syntax-help-improving-an-lstm-based,https://aclweb.org/anthology/P17-1127
Google Dataset,,# 1,CR,0.43,BiLSTM,-,Sentence Compression,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,/paper/can-syntax-help-improving-an-lstm-based,https://aclweb.org/anthology/P17-1127
IMDb,,# 13,Accuracy,45.1,Standard DR-AGG,-,Sentiment Analysis,Information Aggregation via Dynamic Routing for Sequence Encoding,/paper/information-aggregation-via-dynamic-routing,https://arxiv.org/pdf/1806.01501v1.pdf
IMDb,,# 14,Accuracy,44.5,Reverse DR-AGG,-,Sentiment Analysis,Information Aggregation via Dynamic Routing for Sequence Encoding,/paper/information-aggregation-via-dynamic-routing,https://arxiv.org/pdf/1806.01501v1.pdf
SST-2 Binary classification,,# 15,Accuracy,87.2,Reverse DR-AGG,-,Sentiment Analysis,Information Aggregation via Dynamic Routing for Sequence Encoding,/paper/information-aggregation-via-dynamic-routing,https://arxiv.org/pdf/1806.01501v1.pdf
SST-2 Binary classification,,# 13,Accuracy,87.6,Standard DR-AGG,-,Sentiment Analysis,Information Aggregation via Dynamic Routing for Sequence Encoding,/paper/information-aggregation-via-dynamic-routing,https://arxiv.org/pdf/1806.01501v1.pdf
CT-150,,# 1,Dice Score,0.84,Att U-Net,-,Pancreas Segmentation,Attention U-Net: Learning Where to Look for the Pancreas,/paper/attention-u-net-learning-where-to-look-for,https://arxiv.org/pdf/1804.03999v3.pdf
CT-150,,# 1,Precision,0.8490000000000001,Att U-Net,-,Pancreas Segmentation,Attention U-Net: Learning Where to Look for the Pancreas,/paper/attention-u-net-learning-where-to-look-for,https://arxiv.org/pdf/1804.03999v3.pdf
CT-150,,# 1,Recall,0.841,Att U-Net,-,Pancreas Segmentation,Attention U-Net: Learning Where to Look for the Pancreas,/paper/attention-u-net-learning-where-to-look-for,https://arxiv.org/pdf/1804.03999v3.pdf
PASCAL Context,,# 11,mIoU,40.4,ParseNet,-,Semantic Segmentation,ParseNet: Looking Wider to See Better,/paper/parsenet-looking-wider-to-see-better,https://arxiv.org/pdf/1506.04579v2.pdf
PASCAL VOC 2012,,# 15,Mean IoU,69.8%,ParseNet,-,Semantic Segmentation,ParseNet: Looking Wider to See Better,/paper/parsenet-looking-wider-to-see-better,https://arxiv.org/pdf/1506.04579v2.pdf
Labeled Faces in the Wild,,# 4,Accuracy,99.52%,Ring loss,-,Face Verification,Ring loss: Convex Feature Normalization for Face Recognition,/paper/ring-loss-convex-feature-normalization-for,https://arxiv.org/pdf/1803.00130v1.pdf
CIFAR-10,,# 7,Inception score,6.35,SteinGAN,-,Conditional Image Generation,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,/paper/learning-to-draw-samples-with-application-to,https://arxiv.org/pdf/1611.01722v2.pdf
GTAV-to-Cityscapes Labels,,# 7,mIoU,27.1,FCNs in the wild,-,Synthetic-to-Real Translation,FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation,/paper/fcns-in-the-wild-pixel-level-adversarial-and,https://arxiv.org/pdf/1612.02649v1.pdf
SYNTHIA Fall-to-Winter,,# 2,mIoU,59.6,FCNs in the wild,-,Image-to-Image Translation,FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation,/paper/fcns-in-the-wild-pixel-level-adversarial-and,https://arxiv.org/pdf/1612.02649v1.pdf
SYNTHIA-to-Cityscapes,,# 2,mIoU,20.2,FCNs in the wild,-,Image-to-Image Translation,FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation,/paper/fcns-in-the-wild-pixel-level-adversarial-and,https://arxiv.org/pdf/1612.02649v1.pdf
CIFAR-10,,# 21,Percentage correct,94.0,Tree+Max-Avg pooling,-,Image Classification,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",/paper/generalizing-pooling-functions-in,https://arxiv.org/pdf/1509.08985v2.pdf
CIFAR-100,,# 32,Percentage correct,67.6,Tree+Max-Avg pooling,-,Image Classification,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",/paper/generalizing-pooling-functions-in,https://arxiv.org/pdf/1509.08985v2.pdf
MNIST,,# 3,Percentage error,0.3,Tree+Max-Avg pooling,-,Image Classification,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",/paper/generalizing-pooling-functions-in,https://arxiv.org/pdf/1509.08985v2.pdf
SVHN,,# 5,Percentage error,1.69,Tree+Max-Avg pooling,-,Image Classification,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",/paper/generalizing-pooling-functions-in,https://arxiv.org/pdf/1509.08985v2.pdf
BSD100 - 4x upscaling,,# 8,PSNR,27.58,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
BSD100 - 4x upscaling,,# 15,SSIM,0.7349,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
Set14 - 4x upscaling,,# 8,PSNR,28.6,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
Set14 - 4x upscaling,,# 14,SSIM,0.7806,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
Set5 - 4x upscaling,,# 8,PSNR,32.13,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
Set5 - 4x upscaling,,# 12,SSIM,0.8937,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
Urban100 - 4x upscaling,,# 9,PSNR,26.07,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
Urban100 - 4x upscaling,,# 9,SSIM,0.7837,CARN,-,Image Super-Resolution,"Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network",/paper/fast-accurate-and-lightweight-super-1,https://arxiv.org/pdf/1803.08664v5.pdf
CliCR,,# 1,F1,33.9,Gated-Attention Reader,-,Question Answering,CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension,/paper/clicr-a-dataset-of-clinical-case-reports-for-1,https://aclweb.org/anthology/N18-1140
CliCR,,# 2,F1,27.2,Stanford Attentive Reader,-,Question Answering,CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension,/paper/clicr-a-dataset-of-clinical-case-reports-for-1,https://aclweb.org/anthology/N18-1140
GigaWord,,# 2,ROUGE-1,37.04,Re^3 Sum,-,Text Summarization,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",/paper/retrieve-rerank-and-rewrite-soft-template,https://aclweb.org/anthology/P18-1015
GigaWord,,# 1,ROUGE-2,19.03,Re^3 Sum,-,Text Summarization,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",/paper/retrieve-rerank-and-rewrite-soft-template,https://aclweb.org/anthology/P18-1015
GigaWord,,# 2,ROUGE-L,34.46,Re^3 Sum,-,Text Summarization,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",/paper/retrieve-rerank-and-rewrite-soft-template,https://aclweb.org/anthology/P18-1015
RVL-CDIP,,# 3,Accuracy,90.94%,AlexNet + spatial pyramidal pooling + image resizing,-,Document Image Classification,Analysis of Convolutional Neural Networks for Document Image Classification,/paper/analysis-of-convolutional-neural-networks-for,https://arxiv.org/pdf/1708.03273v1.pdf
CIFAR-10,,# 33,Percentage correct,91.8,DSN,-,Image Classification,Deeply-Supervised Nets,/paper/deeply-supervised-nets,https://arxiv.org/pdf/1409.5185v2.pdf
CIFAR-100,,# 37,Percentage correct,65.4,DSN,-,Image Classification,Deeply-Supervised Nets,/paper/deeply-supervised-nets,https://arxiv.org/pdf/1409.5185v2.pdf
MNIST,,# 4,Percentage error,0.4,DSN,-,Image Classification,Deeply-Supervised Nets,/paper/deeply-supervised-nets,https://arxiv.org/pdf/1409.5185v2.pdf
SVHN,,# 11,Percentage error,1.92,DSN,-,Image Classification,Deeply-Supervised Nets,/paper/deeply-supervised-nets,https://arxiv.org/pdf/1409.5185v2.pdf
PubMed 20k RCT,,# 1,F1,92.6,Hierarchical Neural Networks,-,Sentence Classification,Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,/paper/hierarchical-neural-networks-for-sequential,https://arxiv.org/pdf/1808.06161v1.pdf
ImageNet,,# 7,Top 1 Accuracy,80.9%,ResNeXt-101,-,Image Classification,Aggregated Residual Transformations for Deep Neural Networks,/paper/aggregated-residual-transformations-for-deep,https://arxiv.org/pdf/1611.05431v2.pdf
ImageNet,,# 5,Top 5 Accuracy,95.6%,ResNeXt-101,-,Image Classification,Aggregated Residual Transformations for Deep Neural Networks,/paper/aggregated-residual-transformations-for-deep,https://arxiv.org/pdf/1611.05431v2.pdf
RotoWire,,# 1,BLEU,16.50,Neural Content Planning + conditional copy,-,Data-to-Text Generation,Data-to-Text Generation with Content Selection and Planning,/paper/data-to-text-generation-with-content,https://arxiv.org/pdf/1809.00582v2.pdf
RotoWire (Content Ordering),,# 1,DLD,18.58%,Neural Content Planning + conditional copy,-,Data-to-Text Generation,Data-to-Text Generation with Content Selection and Planning,/paper/data-to-text-generation-with-content,https://arxiv.org/pdf/1809.00582v2.pdf
Rotowire (Content Selection),,# 1,Precision,34.18%,Neural Content Planning + conditional copy,-,Data-to-Text Generation,Data-to-Text Generation with Content Selection and Planning,/paper/data-to-text-generation-with-content,https://arxiv.org/pdf/1809.00582v2.pdf
Rotowire (Content Selection),,# 1,Recall,51.22%,Neural Content Planning + conditional copy,-,Data-to-Text Generation,Data-to-Text Generation with Content Selection and Planning,/paper/data-to-text-generation-with-content,https://arxiv.org/pdf/1809.00582v2.pdf
RotoWire (Relation Generation),,# 1,count,34.28,Neural Content Planning + conditional copy,-,Data-to-Text Generation,Data-to-Text Generation with Content Selection and Planning,/paper/data-to-text-generation-with-content,https://arxiv.org/pdf/1809.00582v2.pdf
RotoWire (Relation Generation),,# 1,Precision,87.47%,Neural Content Planning + conditional copy,-,Data-to-Text Generation,Data-to-Text Generation with Content Selection and Planning,/paper/data-to-text-generation-with-content,https://arxiv.org/pdf/1809.00582v2.pdf
Clipart1k,,# 1,MAP,46.0,DT+PL,-,Weakly Supervised Object Detection,Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation,/paper/cross-domain-weakly-supervised-object,https://arxiv.org/pdf/1803.11365v1.pdf
Comic2k,,# 1,MAP,37.2,DT+PL,-,Weakly Supervised Object Detection,Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation,/paper/cross-domain-weakly-supervised-object,https://arxiv.org/pdf/1803.11365v1.pdf
Watercolor2k,,# 2,MAP,54.3,DT+PL,-,Weakly Supervised Object Detection,Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation,/paper/cross-domain-weakly-supervised-object,https://arxiv.org/pdf/1803.11365v1.pdf
Pascal3D+,,# 1,Mean PCK,82.5,ConvNet + deformable shape model,-,Keypoint Detection,6-DoF Object Pose from Semantic Keypoints,/paper/6-dof-object-pose-from-semantic-keypoints,https://arxiv.org/pdf/1703.04670v1.pdf
BUS 2017 Dataset B,,# 1,Dice Score,0.804,Attn U-Net + Multi-Input + FTL,-,Lesion Segmentation,A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,/paper/a-novel-focal-tversky-loss-function-with,https://arxiv.org/pdf/1810.07842v1.pdf
BUS 2017 Dataset B,,# 3,Dice Score,0.615,Attn U-Net + DL,-,Lesion Segmentation,A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,/paper/a-novel-focal-tversky-loss-function-with,https://arxiv.org/pdf/1810.07842v1.pdf
BUS 2017 Dataset B,,# 2,Dice Score,0.669,U-Net + FTL,-,Lesion Segmentation,A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,/paper/a-novel-focal-tversky-loss-function-with,https://arxiv.org/pdf/1810.07842v1.pdf
ISIC 2018,,# 2,Dice Score,0.8290000000000001,U-Net + FTL,-,Lesion Segmentation,A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,/paper/a-novel-focal-tversky-loss-function-with,https://arxiv.org/pdf/1810.07842v1.pdf
ISIC 2018,,# 1,Dice Score,0.856,Attn U-Net + Multi-Input + FTL,-,Lesion Segmentation,A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,/paper/a-novel-focal-tversky-loss-function-with,https://arxiv.org/pdf/1810.07842v1.pdf
ISIC 2018,,# 3,Dice Score,0.8059999999999999,Attn U-Net + DL,-,Lesion Segmentation,A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,/paper/a-novel-focal-tversky-loss-function-with,https://arxiv.org/pdf/1810.07842v1.pdf
MLDoc Zero-Shot English-to-French,,# 4,Accuracy,72.38%,MultiCCA + CNN,-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-French,,# 2,Accuracy,74.52%,BiLSTM (UN),-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-French,,# 3,Accuracy,72.83%,BiLSTM (Europarl),-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-German,,# 2,Accuracy,81.20%,MultiCCA + CNN,-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-German,,# 3,Accuracy,71.83%,BiLSTM (Europarl),-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-Spanish,,# 4,Accuracy,66.65%,BiLSTM (Europarl),-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-Spanish,,# 2,Accuracy,72.50%,MultiCCA + CNN,-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
MLDoc Zero-Shot English-to-Spanish,,# 3,Accuracy,69.50%,BiLSTM (UN),-,Cross-Lingual Document Classification,A Corpus for Multilingual Document Classification in Eight Languages,/paper/a-corpus-for-multilingual-document,https://arxiv.org/pdf/1805.09821v1.pdf
Penn Treebank,,# 10,F1 score,92.1,Semi-supervised LSTM,-,Constituency Parsing,Grammar as a Foreign Language,/paper/grammar-as-a-foreign-language,https://arxiv.org/pdf/1412.7449v3.pdf
MSU-MFSD,,# 2,Equal Error Rate,10.8%,Color LBP,-,Face Anti-Spoofing,face anti-spoofing based on color texture analysis,/paper/face-anti-spoofing-based-on-color-texture,https://arxiv.org/pdf/1511.06316v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 4,Accuracy,52.40%,MAML++,-,Few-Shot Image Classification,How to train your MAML,/paper/how-to-train-your-maml,https://arxiv.org/pdf/1810.09502v3.pdf
Mini-ImageNet - 5-Shot Learning,,# 5,Accuracy,67.15%,MAML++,-,Few-Shot Image Classification,How to train your MAML,/paper/how-to-train-your-maml,https://arxiv.org/pdf/1810.09502v3.pdf
HIV dataset,,# 2,AUC,0.8220000000000001,GraphConv,-,Drug Discovery,Convolutional Networks on Graphs for Learning Molecular Fingerprints,/paper/convolutional-networks-on-graphs-for-learning,https://arxiv.org/pdf/1509.09292v2.pdf
MUV,,# 2,AUC,0.836,GraphConv,-,Drug Discovery,Convolutional Networks on Graphs for Learning Molecular Fingerprints,/paper/convolutional-networks-on-graphs-for-learning,https://arxiv.org/pdf/1509.09292v2.pdf
PCBA,,# 2,AUC,0.855,GraphConv,-,Drug Discovery,Convolutional Networks on Graphs for Learning Molecular Fingerprints,/paper/convolutional-networks-on-graphs-for-learning,https://arxiv.org/pdf/1509.09292v2.pdf
Tox21,,# 2,AUC,0.846,GraphConv,-,Drug Discovery,Convolutional Networks on Graphs for Learning Molecular Fingerprints,/paper/convolutional-networks-on-graphs-for-learning,https://arxiv.org/pdf/1509.09292v2.pdf
ToxCast,,# 2,AUC,0.754,GraphConv,-,Drug Discovery,Convolutional Networks on Graphs for Learning Molecular Fingerprints,/paper/convolutional-networks-on-graphs-for-learning,https://arxiv.org/pdf/1509.09292v2.pdf
WMT2014 English-French,,# 25,BLEU score,27.6,PBSMT + NMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 English-French,,# 4,BLEU,27.6,PBSMT + NMT,-,Unsupervised Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 English-French,,# 28,BLEU score,25.14,Unsupervised NMT + Transformer,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 English-French,,# 24,BLEU score,28.11,Unsupervised PBSMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 English-German,,# 22,BLEU score,17.16,Unsupervised NMT + Transformer,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 English-German,,# 17,BLEU score,20.23,PBSMT + NMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 English-German,,# 20,BLEU score,17.94,Unsupervised PBSMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2014 French-English,,# 4,BLEU,27.7,PBSMT + NMT,-,Unsupervised Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-German,,# 4,BLEU,20.2,PBSMT + NMT,-,Unsupervised Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-Romanian,,# 9,BLEU score,21.33,Unsupervised PBSMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-Romanian,,# 10,BLEU score,21.18,Unsupervised NMT + Transformer,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-Romanian,,# 8,BLEU score,25.13,PBSMT + NMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-Russian,,# 2,BLEU score,13.76,PBSMT + NMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-Russian,,# 3,BLEU score,13.37,Unsupervised PBSMT,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 English-Russian,,# 4,BLEU score,7.98,Unsupervised NMT + Transformer,-,Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
WMT2016 German-English,,# 5,BLEU,25.2,PBSMT,-,Unsupervised Machine Translation,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/pdf/1804.07755v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 22,Restaurant (Acc),77.28,GCAE,-,Aspect-Based Sentiment Analysis,Aspect Based Sentiment Analysis with Gated Convolutional Networks,/paper/aspect-based-sentiment-analysis-with-gated,https://arxiv.org/pdf/1805.07043v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 3,Laptop (Acc),69.14,GCAE,-,Aspect-Based Sentiment Analysis,Aspect Based Sentiment Analysis with Gated Convolutional Networks,/paper/aspect-based-sentiment-analysis-with-gated,https://arxiv.org/pdf/1805.07043v1.pdf
DailyDialog,,# 1,BLEU-1,14.17,AEM+Attention,-,Text Generation,An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation,/paper/an-auto-encoder-matching-model-for-learning,https://arxiv.org/pdf/1808.08795v1.pdf
DailyDialog,,# 1,BLEU-2,5.69,AEM+Attention,-,Text Generation,An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation,/paper/an-auto-encoder-matching-model-for-learning,https://arxiv.org/pdf/1808.08795v1.pdf
DailyDialog,,# 1,BLEU-3,3.78,AEM+Attention,-,Text Generation,An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation,/paper/an-auto-encoder-matching-model-for-learning,https://arxiv.org/pdf/1808.08795v1.pdf
DailyDialog,,# 1,BLEU-4,2.84,AEM+Attention,-,Text Generation,An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation,/paper/an-auto-encoder-matching-model-for-learning,https://arxiv.org/pdf/1808.08795v1.pdf
Amazon,,# 3,AUC,0.8683,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Bing News,,# 3,AUC,0.8376,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Bing News,,# 3,Log Loss,0.2671,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Company*,,# 1,AUC,0.8715,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Company*,,# 1,Log Loss,0.026180000000000002,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Criteo,,# 2,AUC,0.8007,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Criteo,,# 2,Log Loss,0.45083,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Dianping,,# 2,AUC,0.8481,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Dianping,,# 2,Log Loss,0.3333,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
MovieLens 20M,,# 3,AUC,0.7324,DeepFM,-,Click-Through Rate Prediction,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,/paper/deepfm-a-factorization-machine-based-neural,https://arxiv.org/pdf/1703.04247v1.pdf
Caltech,,# 12,Reasonable Miss Rate,9.68,SA-FastRCNN,-,Pedestrian Detection,Scale-aware Fast R-CNN for Pedestrian Detection,/paper/scale-aware-fast-r-cnn-for-pedestrian,https://arxiv.org/pdf/1510.08160v3.pdf
COCO,,# 4,MAP,43.5,ProNet,-,Weakly Supervised Object Detection,ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks,/paper/pronet-learning-to-propose-object-specific,https://arxiv.org/pdf/1511.03776v3.pdf
PASCAL Context,,# 8,mIoU,43.3,Piecewise,-,Semantic Segmentation,Efficient piecewise training of deep structured models for semantic segmentation,/paper/efficient-piecewise-training-of-deep,https://arxiv.org/pdf/1504.01013v4.pdf
E2E NLG Challenge,,# 6,BLEU,64.22,Gong,-,Data-to-Text Generation,Technical Report for E2E NLG Challenge,/paper/technical-report-for-e2e-nlg-challenge,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Gong.pdf
E2E NLG Challenge,,# 5,NIST,8.3453,Gong,-,Data-to-Text Generation,Technical Report for E2E NLG Challenge,/paper/technical-report-for-e2e-nlg-challenge,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Gong.pdf
E2E NLG Challenge,,# 5,METEOR,44.69,Gong,-,Data-to-Text Generation,Technical Report for E2E NLG Challenge,/paper/technical-report-for-e2e-nlg-challenge,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Gong.pdf
E2E NLG Challenge,,# 6,ROUGE-L,66.45,Gong,-,Data-to-Text Generation,Technical Report for E2E NLG Challenge,/paper/technical-report-for-e2e-nlg-challenge,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Gong.pdf
E2E NLG Challenge,,# 2,CIDEr,2.2721,Gong,-,Data-to-Text Generation,Technical Report for E2E NLG Challenge,/paper/technical-report-for-e2e-nlg-challenge,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-Gong.pdf
WIDER Face (Easy),,# 8,AP,0.902,CMS-RCNN,-,Face Detection,CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection,/paper/cms-rcnn-contextual-multi-scale-region-based,https://arxiv.org/pdf/1606.05413v1.pdf
WIDER Face (Hard),,# 10,AP,0.643,CMS-RCNN,-,Face Detection,CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection,/paper/cms-rcnn-contextual-multi-scale-region-based,https://arxiv.org/pdf/1606.05413v1.pdf
WIDER Face (Medium),,# 8,AP,0.8740000000000001,CMS-RCNN,-,Face Detection,CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection,/paper/cms-rcnn-contextual-multi-scale-region-based,https://arxiv.org/pdf/1606.05413v1.pdf
CUB-200-2011,,# 7,Accuracy,85.3%,RA-CNN,-,Fine-Grained Image Classification,Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition,/paper/look-closer-to-see-better-recurrent-attention,https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf
SQuAD1.1,,# 62,EM,76.2,QANet + data augmentation Ã3,-,Question Answering,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,/paper/qanet-combining-local-convolution-with-global,https://arxiv.org/pdf/1804.09541v1.pdf
SQuAD1.1,,# 56,F1,84.6,QANet + data augmentation Ã3,-,Question Answering,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,/paper/qanet-combining-local-convolution-with-global,https://arxiv.org/pdf/1804.09541v1.pdf
Annotated Faces in the Wild,,# 7,AP,0.9835,STN,-,Face Detection,Supervised Transformer Network for Efficient Face Detection,/paper/supervised-transformer-network-for-efficient,https://arxiv.org/pdf/1607.05477v1.pdf
PASCAL Face,,# 6,AP,0.941,STN,-,Face Detection,Supervised Transformer Network for Efficient Face Detection,/paper/supervised-transformer-network-for-efficient,https://arxiv.org/pdf/1607.05477v1.pdf
Set14 - 4x upscaling,,# 34,PSNR,26.22,Deep Mean-Shift Priors,-,Image Super-Resolution,Deep Mean-Shift Priors for Image Restoration,/paper/deep-mean-shift-priors-for-image-restoration,https://arxiv.org/pdf/1709.03749v2.pdf
Set5 - 4x upscaling,,# 31,PSNR,29.16,Deep Mean-Shift Priors,-,Image Super-Resolution,Deep Mean-Shift Priors for Image Restoration,/paper/deep-mean-shift-priors-for-image-restoration,https://arxiv.org/pdf/1709.03749v2.pdf
COCO,,# 2,MAP,48.1%,YOLOv2 608x608,-,Real-Time Object Detection,"YOLO9000: Better, Faster, Stronger",/paper/yolo9000-better-faster-stronger,https://arxiv.org/pdf/1612.08242v1.pdf
COCO,,# 2,FPS,40,YOLOv2 608x608,-,Real-Time Object Detection,"YOLO9000: Better, Faster, Stronger",/paper/yolo9000-better-faster-stronger,https://arxiv.org/pdf/1612.08242v1.pdf
COCO,,# 36,Bounding Box AP,21.6,YOLO v2 + Darknet-19,-,Object Detection,"YOLO9000: Better, Faster, Stronger",/paper/yolo9000-better-faster-stronger,https://arxiv.org/pdf/1612.08242v1.pdf
PASCAL VOC 2007,,# 8,MAP,78.6%,YOLO v2,-,Object Detection,"YOLO9000: Better, Faster, Stronger",/paper/yolo9000-better-faster-stronger,https://arxiv.org/pdf/1612.08242v1.pdf
IWSLT2015 English-German,,# 4,BLEU score,27.01,Denoising autoencoders (non-autoregressive),-,Machine Translation,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://arxiv.org/pdf/1802.06901v3.pdf
IWSLT2015 German-English,,# 7,BLEU score,32.43,Denoising autoencoders (non-autoregressive),-,Machine Translation,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://arxiv.org/pdf/1802.06901v3.pdf
WMT2014 English-German,,# 15,BLEU score,21.54,Denoising autoencoders (non-autoregressive),-,Machine Translation,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://arxiv.org/pdf/1802.06901v3.pdf
WMT2014 German-English,,# 1,BLEU score,25.43,Denoising autoencoders (non-autoregressive),-,Machine Translation,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://arxiv.org/pdf/1802.06901v3.pdf
WMT2016 English-Romanian,,# 3,BLEU score,29.66,Denoising autoencoders (non-autoregressive),-,Machine Translation,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://arxiv.org/pdf/1802.06901v3.pdf
WMT2016 Romanian-English,,# 4,BLEU score,30.3,Denoising autoencoders (non-autoregressive),-,Machine Translation,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://arxiv.org/pdf/1802.06901v3.pdf
Human3.6M,,# 5,Average 3D Error,66.1,Sequence-to-sequence network,-,3D Human Pose Estimation,Exploiting temporal information for 3D pose estimation,/paper/exploiting-temporal-information-for-3d-pose,https://arxiv.org/pdf/1711.08585v4.pdf
CIFAR-10,,# 3,FID,24.8,WGAN-GP + TT Update Rule,-,Image Generation,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,/paper/gans-trained-by-a-two-time-scale-update-rule,https://arxiv.org/pdf/1706.08500v6.pdf
LSUN Bedroom 256 x 256,,# 4,FID,9.5,WGAN-GP + TT Update Rule,-,Image Generation,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,/paper/gans-trained-by-a-two-time-scale-update-rule,https://arxiv.org/pdf/1706.08500v6.pdf
BSD100 - 4x upscaling,,# 17,PSNR,27.4,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
BSD100 - 4x upscaling,,# 22,SSIM,0.7281,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Manga109 - 4x upscaling,,# 6,PSNR,29.42,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Manga109 - 4x upscaling,,# 7,SSIM,0.8942,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Set14 - 4x upscaling,,# 18,PSNR,28.26,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Set14 - 4x upscaling,,# 22,SSIM,0.7723,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Set5 - 4x upscaling,,# 15,PSNR,31.74,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Set5 - 4x upscaling,,# 19,SSIM,0.8893,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Urban100 - 4x upscaling,,# 16,PSNR,25.5,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Urban100 - 4x upscaling,,# 18,SSIM,0.763,MemNet,-,Image Super-Resolution,MemNet: A Persistent Memory Network for Image Restoration,/paper/memnet-a-persistent-memory-network-for-image,https://arxiv.org/pdf/1708.02209v1.pdf
Second dialogue state tracking challenge,,# 2,Request,96.5,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
Second dialogue state tracking challenge,,# 2,Area,90.0,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
Second dialogue state tracking challenge,,# 2,Food,84.0,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
Second dialogue state tracking challenge,,# 1,Price,94.0,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
Second dialogue state tracking challenge,,# 3,Joint,73.4,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
Wizard-of-Oz,,# 2,Request,96.5,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
Wizard-of-Oz,,# 3,Joint,84.4,Neural belief tracker,-,Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,/paper/neural-belief-tracker-data-driven-dialogue,https://arxiv.org/pdf/1606.03777v2.pdf
IEMOCAP,,# 1,F1,55.3%,CHFusion,-,Multimodal Emotion Recognition,Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling,/paper/multimodal-sentiment-analysis-using,https://arxiv.org/pdf/1806.06228v1.pdf
MOSI,,# 4,Accuracy,76.5%,CHFusion,-,Multimodal Sentiment Analysis,Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling,/paper/multimodal-sentiment-analysis-using,https://arxiv.org/pdf/1806.06228v1.pdf
WMT2014 English-French,,# 4,BLEU score,41.5,Transformer (big) + Relative Position Representations,-,Machine Translation,Self-Attention with Relative Position Representations,/paper/self-attention-with-relative-position,https://arxiv.org/pdf/1803.02155v2.pdf
WMT2014 English-German,,# 4,BLEU score,29.2,Transformer (big) + Relative Position Representations,-,Machine Translation,Self-Attention with Relative Position Representations,/paper/self-attention-with-relative-position,https://arxiv.org/pdf/1803.02155v2.pdf
CIFAR-10,,# 10,NLL Test,4.2,Deep Diffusion,-,Image Generation,"10,000+ Times Accelerated Robust Subset Selection (ARSS)",/paper/10000-times-accelerated-robust-subset,https://arxiv.org/pdf/1409.3660v4.pdf
CIFAR-10,,# 17,Percentage correct,94.77,Stochastic Depth,-,Image Classification,Deep Networks with Stochastic Depth,/paper/deep-networks-with-stochastic-depth,https://arxiv.org/pdf/1603.09382v3.pdf
CIFAR-10,,# 12,Percentage error,5.23,Stochastic Depth,-,Image Classification,Deep Networks with Stochastic Depth,/paper/deep-networks-with-stochastic-depth,https://arxiv.org/pdf/1603.09382v3.pdf
CIFAR-100,,# 15,Percentage correct,75.42,Stochastic Depth,-,Image Classification,Deep Networks with Stochastic Depth,/paper/deep-networks-with-stochastic-depth,https://arxiv.org/pdf/1603.09382v3.pdf
CIFAR-100,,# 7,Percentage error,24.58,Stochastic Depth,-,Image Classification,Deep Networks with Stochastic Depth,/paper/deep-networks-with-stochastic-depth,https://arxiv.org/pdf/1603.09382v3.pdf
SVHN,,# 7,Percentage error,1.75,Stochastic Depth,-,Image Classification,Deep Networks with Stochastic Depth,/paper/deep-networks-with-stochastic-depth,https://arxiv.org/pdf/1603.09382v3.pdf
SQuAD1.1,,# 106,EM,70.733,Document Reader (single model),-,Question Answering,Reading Wikipedia to Answer Open-Domain Questions,/paper/reading-wikipedia-to-answer-open-domain,https://arxiv.org/pdf/1704.00051v2.pdf
SQuAD1.1,,# 109,F1,79.35300000000001,Document Reader (single model),-,Question Answering,Reading Wikipedia to Answer Open-Domain Questions,/paper/reading-wikipedia-to-answer-open-domain,https://arxiv.org/pdf/1704.00051v2.pdf
Scan2CAD,,# 1,Average Accuracy,31.68%,Scan2CAD,-,3D Reconstruction,Scan2CAD: Learning CAD Model Alignment in RGB-D Scans,/paper/scan2cad-learning-cad-model-alignment-in-rgb,https://arxiv.org/pdf/1811.11187v1.pdf
DUC 2004 Task 1,,# 4,ROUGE-1,29.33,Seq2seq + selective + MTL + ERAM,-,Text Summarization,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,/paper/ensure-the-correctness-of-the-summary,https://aclweb.org/anthology/C18-1121
DUC 2004 Task 1,,# 4,ROUGE-2,10.24,Seq2seq + selective + MTL + ERAM,-,Text Summarization,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,/paper/ensure-the-correctness-of-the-summary,https://aclweb.org/anthology/C18-1121
DUC 2004 Task 1,,# 5,ROUGE-L,25.24,Seq2seq + selective + MTL + ERAM,-,Text Summarization,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,/paper/ensure-the-correctness-of-the-summary,https://aclweb.org/anthology/C18-1121
GigaWord,,# 9,ROUGE-1,35.33,Seq2seq + selective + MTL + ERAM,-,Text Summarization,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,/paper/ensure-the-correctness-of-the-summary,https://aclweb.org/anthology/C18-1121
GigaWord,,# 10,ROUGE-2,17.27,Seq2seq + selective + MTL + ERAM,-,Text Summarization,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,/paper/ensure-the-correctness-of-the-summary,https://aclweb.org/anthology/C18-1121
GigaWord,,# 10,ROUGE-L,33.19,Seq2seq + selective + MTL + ERAM,-,Text Summarization,Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization,/paper/ensure-the-correctness-of-the-summary,https://aclweb.org/anthology/C18-1121
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 1,Percentage correct,70.1,MCB 7 att.,-,Visual Question Answering,Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,/paper/multimodal-compact-bilinear-pooling-for,https://arxiv.org/pdf/1606.01847v3.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 1,Percentage correct,66.5,MCB 7 att.,-,Visual Question Answering,Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,/paper/multimodal-compact-bilinear-pooling-for,https://arxiv.org/pdf/1606.01847v3.pdf
Visual7W,,# 2,Percentage correct,62.2,MCB+Att.,-,Visual Question Answering,Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,/paper/multimodal-compact-bilinear-pooling-for,https://arxiv.org/pdf/1606.01847v3.pdf
CoQA,,# 8,In-domain,54.5,Vanilla DrQA (single model),-,Question Answering,CoQA: A Conversational Question Answering Challenge,/paper/coqa-a-conversational-question-answering,https://arxiv.org/pdf/1808.07042v2.pdf
CoQA,,# 8,Out-of-domain,47.9,Vanilla DrQA (single model),-,Question Answering,CoQA: A Conversational Question Answering Challenge,/paper/coqa-a-conversational-question-answering,https://arxiv.org/pdf/1808.07042v2.pdf
CoQA,,# 8,Overall,52.6,Vanilla DrQA (single model),-,Question Answering,CoQA: A Conversational Question Answering Challenge,/paper/coqa-a-conversational-question-answering,https://arxiv.org/pdf/1808.07042v2.pdf
CoQA,,# 7,In-domain,67.0,DrQA + seq2seq with copy attention (single model),-,Question Answering,CoQA: A Conversational Question Answering Challenge,/paper/coqa-a-conversational-question-answering,https://arxiv.org/pdf/1808.07042v2.pdf
CoQA,,# 7,Out-of-domain,60.4,DrQA + seq2seq with copy attention (single model),-,Question Answering,CoQA: A Conversational Question Answering Challenge,/paper/coqa-a-conversational-question-answering,https://arxiv.org/pdf/1808.07042v2.pdf
CoQA,,# 7,Overall,65.1,DrQA + seq2seq with copy attention (single model),-,Question Answering,CoQA: A Conversational Question Answering Challenge,/paper/coqa-a-conversational-question-answering,https://arxiv.org/pdf/1808.07042v2.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 7,Percentage correct,58.9,SAN,-,Visual Question Answering,Stacked Attention Networks for Image Question Answering,/paper/stacked-attention-networks-for-image-question,https://arxiv.org/pdf/1511.02274v2.pdf
HPatches,,# 5,Viewpoint I AEPE,36.94,SPyNet,-,Dense Pixel Correspondence Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
HPatches,,# 5,Viewpoint II AEPE,50.92,SPyNet,-,Dense Pixel Correspondence Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
HPatches,,# 5,Viewpoint III AEPE,54.29,SPyNet,-,Dense Pixel Correspondence Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
HPatches,,# 5,Viewpoint IV AEPE,62.6,SPyNet,-,Dense Pixel Correspondence Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
HPatches,,# 5,Viewpoint V AEPE,72.57,SPyNet,-,Dense Pixel Correspondence Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
Sintel-clean,,# 3,Average End-Point Error,6.64,Spynet,-,Optical Flow Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
Sintel-final,,# 3,Average End-Point Error,8.36,Spynet,-,Optical Flow Estimation,Optical Flow Estimation using a Spatial Pyramid Network,/paper/optical-flow-estimation-using-a-spatial,https://arxiv.org/pdf/1611.00850v2.pdf
LineMOD,,# 2,Accuracy,94.5%,Keypoint detector localization,-,6D Pose Estimation,Estimating 6D Pose From Localizing Designated Surface Keypoints,/paper/estimating-6d-pose-from-localizing-designated,https://arxiv.org/pdf/1812.01387v1.pdf
COFW,,# 1,Mean Error Rate,5.55%,DenseU-Net + Dual Transformer,-,Face Alignment,Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment,/paper/stacked-dense-u-nets-with-dual-transformers,https://arxiv.org/pdf/1812.01936v1.pdf
IBUG,,# 1,Mean Error Rate,6.73%,DenseU-Net + Dual Transformer,-,Face Alignment,Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment,/paper/stacked-dense-u-nets-with-dual-transformers,https://arxiv.org/pdf/1812.01936v1.pdf
IJB-A,,# 11,TAR @ FAR=0.01,83.80%,DCNN,-,Face Verification,Unconstrained Face Verification using Deep CNN Features,/paper/unconstrained-face-verification-using-deep,https://arxiv.org/pdf/1508.01722v2.pdf
Medical domain,,# 5,MAP,13.77,EXPR,-,Hypernym Discovery,EXPR at SemEval-2018 Task 9: A Combined Approach for Hypernym Discovery,/paper/expr-at-semeval-2018-task-9-a-combined,https://aclweb.org/anthology/S18-1150
Medical domain,,# 3,MRR,40.76,EXPR,-,Hypernym Discovery,EXPR at SemEval-2018 Task 9: A Combined Approach for Hypernym Discovery,/paper/expr-at-semeval-2018-task-9-a-combined,https://aclweb.org/anthology/S18-1150
Medical domain,,# 5,[emailÂ protected],12.76,EXPR,-,Hypernym Discovery,EXPR at SemEval-2018 Task 9: A Combined Approach for Hypernym Discovery,/paper/expr-at-semeval-2018-task-9-a-combined,https://aclweb.org/anthology/S18-1150
CIFAR-10,,# 12,Percentage correct,96.29,SimpleNetv2,-,Image Classification,Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet,/paper/towards-principled-design-of-deep,https://arxiv.org/pdf/1802.06205v1.pdf
CIFAR-100,,# 10,Percentage correct,80.29,SimpleNetv2,-,Image Classification,Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet,/paper/towards-principled-design-of-deep,https://arxiv.org/pdf/1802.06205v1.pdf
Vid4 - 4x upscaling,,# 3,PSNR,26.69,FRVSR,-,Video Super-Resolution,Frame-Recurrent Video Super-Resolution,/paper/frame-recurrent-video-super-resolution,https://arxiv.org/pdf/1801.04590v4.pdf
Vid4 - 4x upscaling,,# 10,SSIM,0.8220000000000001,FRVSR,-,Video Super-Resolution,Frame-Recurrent Video Super-Resolution,/paper/frame-recurrent-video-super-resolution,https://arxiv.org/pdf/1801.04590v4.pdf
BP4D,,# 1,Avg F1,63.1,AU R-CNN,-,Action Unit Detection,AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection,/paper/au-r-cnn-encoding-expert-prior-knowledge-into,https://arxiv.org/pdf/1812.05788v1.pdf
COCO,,# 31,Bounding Box AP,34.7,Faster R-CNN,-,Object Detection,Speed/accuracy trade-offs for modern convolutional object detectors,/paper/speedaccuracy-trade-offs-for-modern,https://arxiv.org/pdf/1611.10012v3.pdf
Chinese Poems,,# 2,BLEU-2,0.812,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
COCO Captions,,# 2,BLEU-2,0.85,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
COCO Captions,,# 2,BLEU-3,0.672,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
COCO Captions,,# 2,BLEU-4,0.557,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
COCO Captions,,# 2,BLEU-5,0.544,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
EMNLP2017 WMT,,# 3,BLEU-2,0.778,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
EMNLP2017 WMT,,# 3,BLEU-3,0.478,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
EMNLP2017 WMT,,# 3,BLEU-4,0.41100000000000003,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
EMNLP2017 WMT,,# 2,BLEU-5,0.46299999999999997,RankGAN,-,Text Generation,Adversarial Ranking for Language Generation,/paper/adversarial-ranking-for-language-generation,https://arxiv.org/pdf/1705.11001v3.pdf
CUB-200-2011,,# 5,Accuracy,86.87%,PC-DenseNet-161,-,Fine-Grained Image Classification,Pairwise Confusion for Fine-Grained Visual Classification,/paper/pairwise-confusion-for-fine-grained-visual,https://arxiv.org/pdf/1705.08016v3.pdf
NABirds,,# 1,Accuracy,82.79%,PC-DenseNet-161,-,Fine-Grained Image Classification,Pairwise Confusion for Fine-Grained Visual Classification,/paper/pairwise-confusion-for-fine-grained-visual,https://arxiv.org/pdf/1705.08016v3.pdf
Oxford 102 Flowers,,# 1,Accuracy,93.65%,PC Bilinear CNN,-,Fine-Grained Image Classification,Pairwise Confusion for Fine-Grained Visual Classification,/paper/pairwise-confusion-for-fine-grained-visual,https://arxiv.org/pdf/1705.08016v3.pdf
Stanford Cars,,# 3,Accuracy,92.86%,PC-DenseNet-161,-,Fine-Grained Image Classification,Pairwise Confusion for Fine-Grained Visual Classification,/paper/pairwise-confusion-for-fine-grained-visual,https://arxiv.org/pdf/1705.08016v3.pdf
Stanford Dogs,,# 2,Accuracy,83.75%,PC-DenseNet-161,-,Fine-Grained Image Classification,Pairwise Confusion for Fine-Grained Visual Classification,/paper/pairwise-confusion-for-fine-grained-visual,https://arxiv.org/pdf/1705.08016v3.pdf
NewsQA,,# 4,F1,56.1,FastQAExt,-,Question Answering,Making Neural QA as Simple as Possible but not Simpler,/paper/making-neural-qa-as-simple-as-possible-but,https://arxiv.org/pdf/1703.04816v3.pdf
NewsQA,,# 4,EM,43.7,FastQAExt,-,Question Answering,Making Neural QA as Simple as Possible but not Simpler,/paper/making-neural-qa-as-simple-as-possible-but,https://arxiv.org/pdf/1703.04816v3.pdf
SQuAD1.1,,# 105,EM,70.84899999999999,FastQAExt,-,Question Answering,Making Neural QA as Simple as Possible but not Simpler,/paper/making-neural-qa-as-simple-as-possible-but,https://arxiv.org/pdf/1703.04816v3.pdf
SQuAD1.1,,# 110,F1,78.857,FastQAExt,-,Question Answering,Making Neural QA as Simple as Possible but not Simpler,/paper/making-neural-qa-as-simple-as-possible-but,https://arxiv.org/pdf/1703.04816v3.pdf
SQuAD1.1,,# 114,EM,68.436,FastQA,-,Question Answering,Making Neural QA as Simple as Possible but not Simpler,/paper/making-neural-qa-as-simple-as-possible-but,https://arxiv.org/pdf/1703.04816v3.pdf
SQuAD1.1,,# 122,F1,77.07,FastQA,-,Question Answering,Making Neural QA as Simple as Possible but not Simpler,/paper/making-neural-qa-as-simple-as-possible-but,https://arxiv.org/pdf/1703.04816v3.pdf
Cityscapes,,# 12,Mean IoU,70.4%,DeepLab-CRF (ResNet-101),-,Semantic Segmentation,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",/paper/deeplab-semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1606.00915v2.pdf
PASCAL Context,,# 6,mIoU,45.7,DeepLabV2,-,Semantic Segmentation,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",/paper/deeplab-semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1606.00915v2.pdf
PASCAL VOC 2012,,# 10,Mean IoU,79.7%,DeepLab-CRF (ResNet-101),-,Semantic Segmentation,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",/paper/deeplab-semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1606.00915v2.pdf
PASCAL VOC 2007,,# 4,MAP,82.44%,SPP (Overfeat-7),-,Object Detection,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,/paper/spatial-pyramid-pooling-in-deep-convolutional,https://arxiv.org/pdf/1406.4729v4.pdf
CityPersons,,# 5,Reasonable MR^-2,14.4,TLL+MRF,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 3,Heavy MR^-2,52.0,TLL+MRF,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 4,Partial MR^-2,15.9,TLL+MRF,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 5,Bare MR^-2,9.2,TLL+MRF,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 8,Reasonable MR^-2,15.5,TLL,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 4,Heavy MR^-2,53.6,TLL,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 6,Partial MR^-2,17.2,TLL,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
CityPersons,,# 6,Bare MR^-2,10.0,TLL,-,Pedestrian Detection,Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation,/paper/small-scale-pedestrian-detection-based-on,https://arxiv.org/pdf/1807.01438v1.pdf
COCO,,# 28,Bounding Box AP,36.8,Faster R-CNN + TDM,-,Object Detection,Beyond Skip Connections: Top-Down Modulation for Object Detection,/paper/beyond-skip-connections-top-down-modulation,https://arxiv.org/pdf/1612.06851v2.pdf
Multi-Domain Sentiment Dataset,,# 4,DVD,76.17,Asymmetric tri-training,-,Sentiment Analysis,Asymmetric Tri-training for Unsupervised Domain Adaptation,/paper/asymmetric-tri-training-for-unsupervised,https://arxiv.org/pdf/1702.08400v3.pdf
Multi-Domain Sentiment Dataset,,# 4,Books,72.97,Asymmetric tri-training,-,Sentiment Analysis,Asymmetric Tri-training for Unsupervised Domain Adaptation,/paper/asymmetric-tri-training-for-unsupervised,https://arxiv.org/pdf/1702.08400v3.pdf
Multi-Domain Sentiment Dataset,,# 3,Electronics,80.47,Asymmetric tri-training,-,Sentiment Analysis,Asymmetric Tri-training for Unsupervised Domain Adaptation,/paper/asymmetric-tri-training-for-unsupervised,https://arxiv.org/pdf/1702.08400v3.pdf
Multi-Domain Sentiment Dataset,,# 2,Kitchen,83.97,Asymmetric tri-training,-,Sentiment Analysis,Asymmetric Tri-training for Unsupervised Domain Adaptation,/paper/asymmetric-tri-training-for-unsupervised,https://arxiv.org/pdf/1702.08400v3.pdf
Multi-Domain Sentiment Dataset,,# 3,Average,78.39,Asymmetric tri-training,-,Sentiment Analysis,Asymmetric Tri-training for Unsupervised Domain Adaptation,/paper/asymmetric-tri-training-for-unsupervised,https://arxiv.org/pdf/1702.08400v3.pdf
CoNLL 2005,,# 2,F1,96.4,DeepSRL,-,Predicate Detection,Deep Semantic Role Labeling: What Works and Whatâs Next,/paper/deep-semantic-role-labeling-what-works-and,https://aclweb.org/anthology/P17-1044
OntoNotes,,# 8,F1,81.7,He et al.,-,Semantic Role Labeling,Deep Semantic Role Labeling: What Works and Whatâs Next,/paper/deep-semantic-role-labeling-what-works-and,https://aclweb.org/anthology/P17-1044
bAbi,,# 2,Accuracy (trained on 10k),99.5%,EntNet,-,Question Answering,Tracking the World State with Recurrent Entity Networks,/paper/tracking-the-world-state-with-recurrent,https://arxiv.org/pdf/1612.03969v3.pdf
bAbi,,# 2,Accuracy (trained on 1k),89.1%,EntNet,-,Question Answering,Tracking the World State with Recurrent Entity Networks,/paper/tracking-the-world-state-with-recurrent,https://arxiv.org/pdf/1612.03969v3.pdf
bAbi,,# 6,Mean Error Rate,9.7%,EntNet,-,Question Answering,Tracking the World State with Recurrent Entity Networks,/paper/tracking-the-world-state-with-recurrent,https://arxiv.org/pdf/1612.03969v3.pdf
BSD100 - 4x upscaling,,# 5,PSNR,27.71,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
BSD100 - 4x upscaling,,# 6,SSIM,0.742,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Manga109 - 4x upscaling,,# 4,PSNR,31.02,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Manga109 - 4x upscaling,,# 5,SSIM,0.9148,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Set14 - 4x upscaling,,# 7,PSNR,28.8,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Set14 - 4x upscaling,,# 7,SSIM,0.7876,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Set5 - 4x upscaling,,# 5,PSNR,32.46,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Set5 - 4x upscaling,,# 8,SSIM,0.8968,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Urban100 - 4x upscaling,,# 4,PSNR,26.64,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
Urban100 - 4x upscaling,,# 3,SSIM,0.8033,EDSR,-,Image Super-Resolution,Enhanced Deep Residual Networks for Single Image Super-Resolution,/paper/enhanced-deep-residual-networks-for-single,https://arxiv.org/pdf/1707.02921v1.pdf
CIFAR-10,,# 51,Percentage correct,84.4,Improving neural networks by preventing co-adaptation of feature detectors,-,Image Classification,Improving neural networks by preventing co-adaptation of feature detectors,/paper/improving-neural-networks-by-preventing-co,https://arxiv.org/pdf/1207.0580v1.pdf
SNLI,,# 12,% Test Accuracy,88.6,600D ESIM + 300D Syntactic TreeLSTM,-,Natural Language Inference,Enhanced LSTM for Natural Language Inference,/paper/enhanced-lstm-for-natural-language-inference,https://arxiv.org/pdf/1609.06038v3.pdf
SNLI,,# 15,% Train Accuracy,93.5,600D ESIM + 300D Syntactic TreeLSTM,-,Natural Language Inference,Enhanced LSTM for Natural Language Inference,/paper/enhanced-lstm-for-natural-language-inference,https://arxiv.org/pdf/1609.06038v3.pdf
SNLI,,# 1,Parameters,7.7m,600D ESIM + 300D Syntactic TreeLSTM,-,Natural Language Inference,Enhanced LSTM for Natural Language Inference,/paper/enhanced-lstm-for-natural-language-inference,https://arxiv.org/pdf/1609.06038v3.pdf
DeepMind Cheetah Run (Images),,# 1,Return,650,PlaNet,-,Continuous Control,Learning Latent Dynamics for Planning from Pixels,/paper/learning-latent-dynamics-for-planning-from,https://arxiv.org/pdf/1811.04551v3.pdf
DeepMind Cup Catch (Images),,# 1,Return,914,PlaNet,-,Continuous Control,Learning Latent Dynamics for Planning from Pixels,/paper/learning-latent-dynamics-for-planning-from,https://arxiv.org/pdf/1811.04551v3.pdf
DeepMind Walker Walk (Images),,# 1,Return,890,PlaNet,-,Continuous Control,Learning Latent Dynamics for Planning from Pixels,/paper/learning-latent-dynamics-for-planning-from,https://arxiv.org/pdf/1811.04551v3.pdf
BSD100 - 4x upscaling,,# 24,PSNR,27.02,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
BSD100 - 4x upscaling,,# 4,SSIM,0.7442,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
BSD100 - 4x upscaling,,# 4,MOS,2.01,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Set14 - 4x upscaling,,# 28,PSNR,27.66,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Set14 - 4x upscaling,,# 3,SSIM,0.8004,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Set14 - 4x upscaling,,# 4,MOS,2.52,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Set5 - 4x upscaling,,# 25,PSNR,30.76,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Set5 - 4x upscaling,,# 24,SSIM,0.8784,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Set5 - 4x upscaling,,# 4,MOS,2.89,ESPCN,-,Image Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Ultra Video Group HD - 4x upscaling,,# 1,Average PSNR,37.91,ESPCN,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Ultra Video Group HD - 4x upscaling,,# 3,Average PSNR,36.2,bicubic,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Vid4 - 4x upscaling,,# 8,PSNR,25.06,ESPCN,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Vid4 - 4x upscaling,,# 5,SSIM,0.7394,ESPCN,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Vid4 - 4x upscaling,,# 3,MOVIE,6.54,ESPCN,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Xiph HD - 4x upscaling,,# 3,Average PSNR,30.3,bicubic,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Xiph HD - 4x upscaling,,# 1,Average PSNR,31.67,ESPCN,-,Video Super-Resolution,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,/paper/real-time-single-image-and-video-super,https://arxiv.org/pdf/1609.05158v2.pdf
Cats-and-Dogs,,# 3,CIS,0.076,CycleGAN,-,Multimodal Unsupervised Image-To-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cats-and-Dogs,,# 3,IS,0.813,CycleGAN,-,Multimodal Unsupervised Image-To-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cityscapes Labels-to-Photo,,# 2,Class IOU,0.11,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cityscapes Labels-to-Photo,,# 2,Per-class Accuracy,17%,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cityscapes Labels-to-Photo,,# 6,Per-pixel Accuracy,52%,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cityscapes Photo-to-Labels,,# 2,Per-pixel Accuracy,58%,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cityscapes Photo-to-Labels,,# 2,Per-class Accuracy,22%,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Cityscapes Photo-to-Labels,,# 2,Class IOU,0.16,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Edge-to-Handbags,,# 3,Quality,40.8%,CycleGAN,-,Multimodal Unsupervised Image-To-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Edge-to-Handbags,,# 4,Diversity,0.012,CycleGAN,-,Multimodal Unsupervised Image-To-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Edge-to-Shoes,,# 4,Quality,36.0%,CycleGAN,-,Multimodal Unsupervised Image-To-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
Edge-to-Shoes,,# 4,Diversity,0.010,CycleGAN,-,Multimodal Unsupervised Image-To-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
RaFD,,# 3,Classification Error,5.99%,CycleGAN,-,Image-to-Image Translation,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,/paper/unpaired-image-to-image-translation-using,https://arxiv.org/pdf/1703.10593v6.pdf
CoNLL-2014 A1,,# 3,F0.5,22.14,BiLSTM-JOINT (trained on FCE),-,Grammatical Error Detection,Jointly Learning to Label Sentences and Tokens,/paper/jointly-learning-to-label-sentences-and,https://arxiv.org/pdf/1811.05949v1.pdf
CoNLL-2014 A2,,# 4,F0.5,29.65,BiLSTM-JOINT (trained on FCE),-,Grammatical Error Detection,Jointly Learning to Label Sentences and Tokens,/paper/jointly-learning-to-label-sentences-and,https://arxiv.org/pdf/1811.05949v1.pdf
FCE,,# 1,F0.5,52.07,BiLSTM-JOINT,-,Grammatical Error Detection,Jointly Learning to Label Sentences and Tokens,/paper/jointly-learning-to-label-sentences-and,https://arxiv.org/pdf/1811.05949v1.pdf
JFLEG,,# 1,F0.5,52.52,BiLSTM-JOINT (trained on FCE),-,Grammatical Error Detection,Jointly Learning to Label Sentences and Tokens,/paper/jointly-learning-to-label-sentences-and,https://arxiv.org/pdf/1811.05949v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 23,Restaurant (Acc),77.2,ATAE-LSTM,-,Aspect-Based Sentiment Analysis,Attention-based LSTM for Aspect-level Sentiment Classification,/paper/attention-based-lstm-for-aspect-level,https://aclweb.org/anthology/D16-1058
SemEval 2014 Task 4 Sub Task 2,,# 2,Laptop (Acc),68.7,ATAE-LSTM,-,Aspect-Based Sentiment Analysis,Attention-based LSTM for Aspect-level Sentiment Classification,/paper/attention-based-lstm-for-aspect-level,https://aclweb.org/anthology/D16-1058
Bing News,,# 1,AUC,0.84,xDeepFM,-,Click-Through Rate Prediction,xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,/paper/xdeepfm-combining-explicit-and-implicit,https://arxiv.org/pdf/1803.05170v3.pdf
Bing News,,# 1,Log Loss,0.2649,xDeepFM,-,Click-Through Rate Prediction,xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,/paper/xdeepfm-combining-explicit-and-implicit,https://arxiv.org/pdf/1803.05170v3.pdf
Criteo,,# 1,AUC,0.8052,xDeepFM,-,Click-Through Rate Prediction,xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,/paper/xdeepfm-combining-explicit-and-implicit,https://arxiv.org/pdf/1803.05170v3.pdf
Criteo,,# 1,Log Loss,0.4418,xDeepFM,-,Click-Through Rate Prediction,xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,/paper/xdeepfm-combining-explicit-and-implicit,https://arxiv.org/pdf/1803.05170v3.pdf
Dianping,,# 1,AUC,0.8639,xDeepFM,-,Click-Through Rate Prediction,xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,/paper/xdeepfm-combining-explicit-and-implicit,https://arxiv.org/pdf/1803.05170v3.pdf
Dianping,,# 1,Log Loss,0.3156,xDeepFM,-,Click-Through Rate Prediction,xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,/paper/xdeepfm-combining-explicit-and-implicit,https://arxiv.org/pdf/1803.05170v3.pdf
MNIST,,# 4,Percentage error,0.4,HOPE,-,Image Classification,Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks,/paper/hybrid-orthogonal-projection-and-estimation,https://arxiv.org/pdf/1502.00702v2.pdf
LexNorm,,# 1,Accuracy,87.63,MoNoise,-,Lexical Normalization,MoNoise: Modeling Noise Using a Modular Normalization System,/paper/monoise-modeling-noise-using-a-modular,https://arxiv.org/pdf/1710.03476v1.pdf
TrecQA,,# 3,MAP,0.75,aNMM,-,Question Answering,aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model,/paper/anmm-ranking-short-answer-texts-with,https://arxiv.org/pdf/1801.01641v1.pdf
TrecQA,,# 3,MRR,0.8109999999999999,aNMM,-,Question Answering,aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model,/paper/anmm-ranking-short-answer-texts-with,https://arxiv.org/pdf/1801.01641v1.pdf
CUB-200-2011,,# 8,Accuracy,85.1%,Bilinear-CNN,-,Fine-Grained Image Classification,Bilinear CNN Models for Fine-Grained Visual Recognition,/paper/bilinear-cnn-models-for-fine-grained-visual,https://openaccess.thecvf.com/content_iccv_2015/papers/Lin_Bilinear_CNN_Models_ICCV_2015_paper.pdf
Atari-57,,# 2,Medium Human-Normalized Score,191.8%,"IMPALA, deep",-,Atari Games,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,/paper/impala-scalable-distributed-deep-rl-with,https://arxiv.org/pdf/1802.01561v3.pdf
Market-1501,,# 21,Rank-1,77.11,Re-rank,-,Person Re-Identification,Re-ranking Person Re-identification with k-reciprocal Encoding,/paper/re-ranking-person-re-identification-with-k,https://arxiv.org/pdf/1701.08398v4.pdf
Market-1501,,# 16,MAP,63.63,Re-rank,-,Person Re-Identification,Re-ranking Person Re-identification with k-reciprocal Encoding,/paper/re-ranking-person-re-identification-with-k,https://arxiv.org/pdf/1701.08398v4.pdf
300W,,# 1,Mean Error Rate,0.1043,FPN,-,Facial Landmark Detection,FacePoseNet: Making a Case for Landmark-Free Face Alignment,/paper/faceposenet-making-a-case-for-landmark-free,https://arxiv.org/pdf/1708.07517v2.pdf
IJB-A,,# 2,Accuracy,91.4%,FPN,-,Face Identification,FacePoseNet: Making a Case for Landmark-Free Face Alignment,/paper/faceposenet-making-a-case-for-landmark-free,https://arxiv.org/pdf/1708.07517v2.pdf
IJB-A,,# 8,TAR @ FAR=0.01,90.1%,FPN,-,Face Verification,FacePoseNet: Making a Case for Landmark-Free Face Alignment,/paper/faceposenet-making-a-case-for-landmark-free,https://arxiv.org/pdf/1708.07517v2.pdf
IJB-B,,# 1,Accuracy,91.1%,FPN,-,Face Identification,FacePoseNet: Making a Case for Landmark-Free Face Alignment,/paper/faceposenet-making-a-case-for-landmark-free,https://arxiv.org/pdf/1708.07517v2.pdf
IJB-B,,# 1,TAR @ FAR=0.01,96.5%,FPN,-,Face Verification,FacePoseNet: Making a Case for Landmark-Free Face Alignment,/paper/faceposenet-making-a-case-for-landmark-free,https://arxiv.org/pdf/1708.07517v2.pdf
CelebA-HQ 1024x1024,,# 2,FID,5.5,PG-SWGAN,-,Image Generation,Sliced Wasserstein Generative Models,/paper/sliced-wasserstein-generative-models-1,https://arxiv.org/pdf/1904.05408v2.pdf
LSUN Bedroom 256 x 256,,# 2,FID,8.0,PG-SWGAN,-,Image Generation,Sliced Wasserstein Generative Models,/paper/sliced-wasserstein-generative-models-1,https://arxiv.org/pdf/1904.05408v2.pdf
TrailerFaces,,# 1,FID,404.1,PG-SWGAN-3D,-,Video Generation,Sliced Wasserstein Generative Models,/paper/sliced-wasserstein-generative-models-1,https://arxiv.org/pdf/1904.05408v2.pdf
COCO,,# 10,Bounding Box AP,44.6,FSAF,-,Object Detection,Feature Selective Anchor-Free Module for Single-Shot Object Detection,/paper/feature-selective-anchor-free-module-for,https://arxiv.org/pdf/1903.00621v1.pdf
300W,,# 1,NME,3.98,SAN GT,-,Facial Landmark Detection,Style Aggregated Network for Facial Landmark Detection,/paper/style-aggregated-network-for-facial-landmark,https://arxiv.org/pdf/1803.04108v4.pdf
AFLW-Front,,# 1,Mean NME,1.85,SAN,-,Facial Landmark Detection,Style Aggregated Network for Facial Landmark Detection,/paper/style-aggregated-network-for-facial-landmark,https://arxiv.org/pdf/1803.04108v4.pdf
AFLW-Full,,# 1,Mean NME,1.91,SAN,-,Facial Landmark Detection,Style Aggregated Network for Facial Landmark Detection,/paper/style-aggregated-network-for-facial-landmark,https://arxiv.org/pdf/1803.04108v4.pdf
TIMIT,,# 14,Percentage error,20.1,"Soft Monotonic Attention (ours, offline)",-,Speech Recognition,Online and Linear-Time Attention by Enforcing Monotonic Alignments,/paper/online-and-linear-time-attention-by-enforcing,https://arxiv.org/pdf/1704.00784v2.pdf
Oxf105k,,# 4,MAP,77.9%,siaMAC+QE*,-,Image Retrieval,CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples,/paper/cnn-image-retrieval-learns-from-bow,https://arxiv.org/pdf/1604.02426v3.pdf
Oxf5k,,# 4,MAP,82.9%,siaMAC+QE*,-,Image Retrieval,CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples,/paper/cnn-image-retrieval-learns-from-bow,https://arxiv.org/pdf/1604.02426v3.pdf
Par106k,,# 4,Accuracy,78.3%,siaMAC+QE*,-,Image Retrieval,CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples,/paper/cnn-image-retrieval-learns-from-bow,https://arxiv.org/pdf/1604.02426v3.pdf
Par6k,,# 3,Accuracy,85.6%,siaMAC+QE*,-,Image Retrieval,CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples,/paper/cnn-image-retrieval-learns-from-bow,https://arxiv.org/pdf/1604.02426v3.pdf
Cityscapes,,# 3,Mean IoU,81.7%,OCNet,-,Semantic Segmentation,OCNet: Object Context Network for Scene Parsing,/paper/ocnet-object-context-network-for-scene,https://arxiv.org/pdf/1809.00916v3.pdf
Natural Questions,,# 1,F1 (Long),66.2,BERT-joint,-,Question Answering,A BERT Baseline for the Natural Questions,/paper/a-bert-baseline-for-the-natural-questions,https://arxiv.org/pdf/1901.08634v2.pdf
Natural Questions,,# 1,F1 (Short),52.1,BERT-joint,-,Question Answering,A BERT Baseline for the Natural Questions,/paper/a-bert-baseline-for-the-natural-questions,https://arxiv.org/pdf/1901.08634v2.pdf
Supervised:,,# 3,Senseval 2,71.6,ELMo,-,Word Sense Disambiguation,Deep Contextualized Word Representations,/paper/deep-contextualized-word-representations-1,https://aclweb.org/anthology/N18-1202
Supervised:,,# 5,Senseval 3,69.6,ELMo,-,Word Sense Disambiguation,Deep Contextualized Word Representations,/paper/deep-contextualized-word-representations-1,https://aclweb.org/anthology/N18-1202
Supervised:,,# 4,SemEval 2007,62.2,ELMo,-,Word Sense Disambiguation,Deep Contextualized Word Representations,/paper/deep-contextualized-word-representations-1,https://aclweb.org/anthology/N18-1202
Supervised:,,# 4,SemEval 2013,66.2,ELMo,-,Word Sense Disambiguation,Deep Contextualized Word Representations,/paper/deep-contextualized-word-representations-1,https://aclweb.org/anthology/N18-1202
Supervised:,,# 3,SemEval 2015,71.3,ELMo,-,Word Sense Disambiguation,Deep Contextualized Word Representations,/paper/deep-contextualized-word-representations-1,https://aclweb.org/anthology/N18-1202
CamVid,,# 2,mIoU,68.7%,BiSeNet,-,Real-Time Semantic Segmentation,BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation,/paper/bisenet-bilateral-segmentation-network-for,https://arxiv.org/pdf/1808.00897v1.pdf
CamVid,,# 2,Mean IoU,68.7%,BiSeNet,-,Semantic Segmentation,BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation,/paper/bisenet-bilateral-segmentation-network-for,https://arxiv.org/pdf/1808.00897v1.pdf
Cityscapes,,# 3,mIoU,74.7%,BiSeNet,-,Real-Time Semantic Segmentation,BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation,/paper/bisenet-bilateral-segmentation-network-for,https://arxiv.org/pdf/1808.00897v1.pdf
Cityscapes,,# 2,Frame (fps),65.5,BiSeNet,-,Real-Time Semantic Segmentation,BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation,/paper/bisenet-bilateral-segmentation-network-for,https://arxiv.org/pdf/1808.00897v1.pdf
Cityscapes,,# 8,Mean IoU,78.9%,BiSeNet,-,Semantic Segmentation,BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation,/paper/bisenet-bilateral-segmentation-network-for,https://arxiv.org/pdf/1808.00897v1.pdf
300W,,# 1,Mean Error Rate,1.42,DAN-Menpo + bounding box diagonal normalization,-,Face Alignment,Deep Alignment Network: A convolutional neural network for robust face alignment,/paper/deep-alignment-network-a-convolutional-neural,https://arxiv.org/pdf/1706.01789v2.pdf
300W,,# 3,Mean Error Rate,3.44,DAN-Menpo + inter-ocular normalization,-,Face Alignment,Deep Alignment Network: A convolutional neural network for robust face alignment,/paper/deep-alignment-network-a-convolutional-neural,https://arxiv.org/pdf/1706.01789v2.pdf
300W,,# 1,AUC0.08,57.07,DAN-Menpo + inter-ocular normalization,-,Face Alignment,Deep Alignment Network: A convolutional neural network for robust face alignment,/paper/deep-alignment-network-a-convolutional-neural,https://arxiv.org/pdf/1706.01789v2.pdf
300W,,# 1,Failure,0.58%,DAN-Menpo + inter-ocular normalization,-,Face Alignment,Deep Alignment Network: A convolutional neural network for robust face alignment,/paper/deep-alignment-network-a-convolutional-neural,https://arxiv.org/pdf/1706.01789v2.pdf
COCO,,# 2,MAP,55.3,SPNs,-,Weakly Supervised Object Detection,Soft Proposal Networks for Weakly Supervised Object Localization,/paper/soft-proposal-networks-for-weakly-supervised,https://arxiv.org/pdf/1709.01829v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 11,Restaurant (Acc),81.1,BILSTM-ATT-G (TSSVAE),-,Aspect-Based Sentiment Analysis,Variational Semi-supervised Aspect-term Sentiment Analysis via Transformer,/paper/semi-supervised-target-level-sentiment,https://arxiv.org/pdf/1810.10437v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 19,Laptop (Acc),75.34,BILSTM-ATT-G (TSSVAE),-,Aspect-Based Sentiment Analysis,Variational Semi-supervised Aspect-term Sentiment Analysis via Transformer,/paper/semi-supervised-target-level-sentiment,https://arxiv.org/pdf/1810.10437v2.pdf
SQuAD1.1,,# 107,EM,70.639,Ruminating Reader (single model),-,Question Answering,Ruminating Reader: Reasoning with Gated Multi-Hop Attention,/paper/ruminating-reader-reasoning-with-gated-multi,https://arxiv.org/pdf/1704.07415v1.pdf
SQuAD1.1,,# 107,F1,79.456,Ruminating Reader (single model),-,Question Answering,Ruminating Reader: Reasoning with Gated Multi-Hop Attention,/paper/ruminating-reader-reasoning-with-gated-multi,https://arxiv.org/pdf/1704.07415v1.pdf
DBpedia,,# 9,Error,1.07,M-ACNN,-,Text Classification,Learning Context-Sensitive Convolutional Filters for Text Processing,/paper/learning-context-sensitive-convolutional,https://arxiv.org/pdf/1709.08294v3.pdf
Yelp Binary classification,,# 9,Error,3.89,M-ACNN,-,Sentiment Analysis,Learning Context-Sensitive Convolutional Filters for Text Processing,/paper/learning-context-sensitive-convolutional,https://arxiv.org/pdf/1709.08294v3.pdf
Annotated Faces in the Wild,,# 1,AP,0.9987,SRN,-,Face Detection,Selective Refinement Network for High Performance Face Detection,/paper/selective-refinement-network-for-high,https://arxiv.org/pdf/1809.02693v1.pdf
FDDB,,# 3,AP,0.988,SRN,-,Face Detection,Selective Refinement Network for High Performance Face Detection,/paper/selective-refinement-network-for-high,https://arxiv.org/pdf/1809.02693v1.pdf
PASCAL Face,,# 1,AP,0.9909,SRN,-,Face Detection,Selective Refinement Network for High Performance Face Detection,/paper/selective-refinement-network-for-high,https://arxiv.org/pdf/1809.02693v1.pdf
WIDER Face (Easy),,# 2,AP,0.9590000000000001,SRN,-,Face Detection,Selective Refinement Network for High Performance Face Detection,/paper/selective-refinement-network-for-high,https://arxiv.org/pdf/1809.02693v1.pdf
WIDER Face (Hard),,# 2,AP,0.8959999999999999,SRN,-,Face Detection,Selective Refinement Network for High Performance Face Detection,/paper/selective-refinement-network-for-high,https://arxiv.org/pdf/1809.02693v1.pdf
WIDER Face (Medium),,# 2,AP,0.948,SRN,-,Face Detection,Selective Refinement Network for High Performance Face Detection,/paper/selective-refinement-network-for-high,https://arxiv.org/pdf/1809.02693v1.pdf
CHASE_DB1,,# 1,F1 score,0.8031,LadderNet,-,Retinal Vessel Segmentation,LadderNet: Multi-path networks based on U-Net for medical image segmentation,/paper/laddernet-multi-path-networks-based-on-u-net,https://arxiv.org/pdf/1810.07810v3.pdf
CHASE_DB1,,# 1,AUC,0.9839,LadderNet,-,Retinal Vessel Segmentation,LadderNet: Multi-path networks based on U-Net for medical image segmentation,/paper/laddernet-multi-path-networks-based-on-u-net,https://arxiv.org/pdf/1810.07810v3.pdf
DRIVE,,# 1,F1 score,0.8202,LadderNet,-,Retinal Vessel Segmentation,LadderNet: Multi-path networks based on U-Net for medical image segmentation,/paper/laddernet-multi-path-networks-based-on-u-net,https://arxiv.org/pdf/1810.07810v3.pdf
DRIVE,,# 1,AUC,0.9793,LadderNet,-,Retinal Vessel Segmentation,LadderNet: Multi-path networks based on U-Net for medical image segmentation,/paper/laddernet-multi-path-networks-based-on-u-net,https://arxiv.org/pdf/1810.07810v3.pdf
PASCAL Context,,# 10,mIoU,40.5,BoxSup,-,Semantic Segmentation,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,/paper/boxsup-exploiting-bounding-boxes-to-supervise,https://arxiv.org/pdf/1503.01640v2.pdf
CoQA,,# 5,In-domain,76.3,FlowQA (single model),-,Question Answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,/paper/flowqa-grasping-flow-in-history-for,https://arxiv.org/pdf/1810.06683v3.pdf
CoQA,,# 5,Out-of-domain,71.8,FlowQA (single model),-,Question Answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,/paper/flowqa-grasping-flow-in-history-for,https://arxiv.org/pdf/1810.06683v3.pdf
CoQA,,# 5,Overall,75.0,FlowQA (single model),-,Question Answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,/paper/flowqa-grasping-flow-in-history-for,https://arxiv.org/pdf/1810.06683v3.pdf
QuAC,,# 1,F1,64.1,FlowQA (single model),-,Question Answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,/paper/flowqa-grasping-flow-in-history-for,https://arxiv.org/pdf/1810.06683v3.pdf
QuAC,,# 1,HEQQ,59.6,FlowQA (single model),-,Question Answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,/paper/flowqa-grasping-flow-in-history-for,https://arxiv.org/pdf/1810.06683v3.pdf
QuAC,,# 1,HEQD,5.8,FlowQA (single model),-,Question Answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,/paper/flowqa-grasping-flow-in-history-for,https://arxiv.org/pdf/1810.06683v3.pdf
CIFAR-10,,# 8,Inception score,7.17,LR-GAN,-,Image Generation,LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,/paper/lr-gan-layered-recursive-generative,https://arxiv.org/pdf/1703.01560v3.pdf
WIDER Face (Easy),,# 9,AP,0.851,Multitask Cascade CNN,-,Face Detection,Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks,/paper/joint-face-detection-and-alignment-using,https://arxiv.org/pdf/1604.02878v1.pdf
WIDER Face (Hard),,# 11,AP,0.607,Multitask Cascade CNN,-,Face Detection,Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks,/paper/joint-face-detection-and-alignment-using,https://arxiv.org/pdf/1604.02878v1.pdf
WIDER Face (Medium),,# 9,AP,0.82,Multitask Cascade CNN,-,Face Detection,Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks,/paper/joint-face-detection-and-alignment-using,https://arxiv.org/pdf/1604.02878v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 5,Restaurant (Acc),81.49,MGAN,-,Aspect-Based Sentiment Analysis,Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification,/paper/exploiting-coarse-to-fine-task-transfer-for-1,https://arxiv.org/pdf/1811.10999
SemEval 2014 Task 4 Sub Task 2,,# 22,Laptop (Acc),76.21,MGAN,-,Aspect-Based Sentiment Analysis,Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification,/paper/exploiting-coarse-to-fine-task-transfer-for-1,https://arxiv.org/pdf/1811.10999
SciTail,,# 5,Accuracy,83.3,CAFE,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 29,% Test Accuracy,85.9,300D CAFE (no cross-sentence attention),-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 40,% Train Accuracy,87.3,300D CAFE (no cross-sentence attention),-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 1,Parameters,3.7m,300D CAFE (no cross-sentence attention),-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 13,% Test Accuracy,88.5,300D CAFE,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 33,% Train Accuracy,89.8,300D CAFE,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 1,Parameters,4.7m,300D CAFE,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 7,% Test Accuracy,89.3,300D CAFE Ensemble,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 20,% Train Accuracy,92.5,300D CAFE Ensemble,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SNLI,,# 1,Parameters,17.5m,300D CAFE Ensemble,-,Natural Language Inference,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",/paper/compare-compress-and-propagate-enhancing,https://arxiv.org/pdf/1801.00102v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 9,Restaurant (Acc),81.2,AOA,-,Aspect-Based Sentiment Analysis,Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks,/paper/aspect-level-sentiment-classification-with,https://arxiv.org/pdf/1804.06536v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 15,Laptop (Acc),74.5,AOA,-,Aspect-Based Sentiment Analysis,Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks,/paper/aspect-level-sentiment-classification-with,https://arxiv.org/pdf/1804.06536v1.pdf
ICVL Hands,,# 2,Average 3D Error,6.8,Pose Guidance,-,Hand Pose Estimation,Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation,/paper/pose-guided-structured-region-ensemble,https://arxiv.org/pdf/1708.03416v2.pdf
MSRA Hands,,# 3,Average 3D Error,8.6,Pose Guidance,-,Hand Pose Estimation,Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation,/paper/pose-guided-structured-region-ensemble,https://arxiv.org/pdf/1708.03416v2.pdf
NYU Hands,,# 3,Average 3D Error,11.8,Pose Guidance,-,Hand Pose Estimation,Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation,/paper/pose-guided-structured-region-ensemble,https://arxiv.org/pdf/1708.03416v2.pdf
COCO,,# 3,Bounding Box AP,47.0,CenterNet511 (multi-scale),-,Object Detection,CenterNet: Keypoint Triplets for Object Detection,/paper/centernet-object-detection-with-keypoint,https://arxiv.org/pdf/1904.08189v3.pdf
COCO,,# 9,Bounding Box AP,44.9,CenterNet511 (single-scale),-,Object Detection,CenterNet: Keypoint Triplets for Object Detection,/paper/centernet-object-detection-with-keypoint,https://arxiv.org/pdf/1904.08189v3.pdf
LibriSpeech test-clean,,# 6,Word Error Rate (WER),3.6,Model Unit Exploration,-,Speech Recognition,Model Unit Exploration for Sequence-to-Sequence Speech Recognition,/paper/model-unit-exploration-for-sequence-to,https://arxiv.org/pdf/1902.01955v1.pdf
AFLW2000-3D,,# 1,Mean NME,3.53%,2DASL,-,Face Alignment,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D-Assisted Self-Supervised Learning,/paper/joint-3d-face-reconstruction-and-dense-face,https://arxiv.org/pdf/1903.09359v1.pdf
MAFA,,# 1,MAP,88.3%,FAN,-,Occluded Face Detection,Face Attention Network: An Effective Face Detector for the Occluded Faces,/paper/face-attention-network-an-effective-face,https://arxiv.org/pdf/1711.07246v2.pdf
CIFAR-10,,# 41,Percentage correct,90.65,Maxout Network (k=2),-,Image Classification,Maxout Networks,/paper/maxout-networks,https://arxiv.org/pdf/1302.4389v4.pdf
CIFAR-10,,# 18,Percentage error,9.38,Maxout Network (k=2),-,Image Classification,Maxout Networks,/paper/maxout-networks,https://arxiv.org/pdf/1302.4389v4.pdf
CIFAR-100,,# 42,Percentage correct,61.43,Maxout Network (k=2),-,Image Classification,Maxout Networks,/paper/maxout-networks,https://arxiv.org/pdf/1302.4389v4.pdf
CIFAR-100,,# 12,Percentage error,38.57,Maxout Network (k=2),-,Image Classification,Maxout Networks,/paper/maxout-networks,https://arxiv.org/pdf/1302.4389v4.pdf
MNIST,,# 5,Percentage error,0.5,Maxout Networks,-,Image Classification,Maxout Networks,/paper/maxout-networks,https://arxiv.org/pdf/1302.4389v4.pdf
SVHN,,# 19,Percentage error,2.47,Maxout,-,Image Classification,Maxout Networks,/paper/maxout-networks,https://arxiv.org/pdf/1302.4389v4.pdf
ShapeNet-Part,,# 1,Class Average IoU,82.8,PointConv,-,3D Part Segmentation,PointConv: Deep Convolutional Networks on 3D Point Clouds,/paper/pointconv-deep-convolutional-networks-on-3d,https://arxiv.org/pdf/1811.07246v2.pdf
ShapeNet-Part,,# 2,Instance Average IoU,85.7,PointConv,-,3D Part Segmentation,PointConv: Deep Convolutional Networks on 3D Point Clouds,/paper/pointconv-deep-convolutional-networks-on-3d,https://arxiv.org/pdf/1811.07246v2.pdf
KITTI Cars Moderate,,# 2,AP,74.99 %,PointPillars,-,3D Object Detection,PointPillars: Fast Encoders for Object Detection from Point Clouds,/paper/pointpillars-fast-encoders-for-object,https://arxiv.org/pdf/1812.05784v1.pdf
KITTI Cars Moderate,,# 1,AP,86.10 %,PointPillars,-,Birds Eye View Object Detection,PointPillars: Fast Encoders for Object Detection from Point Clouds,/paper/pointpillars-fast-encoders-for-object,https://arxiv.org/pdf/1812.05784v1.pdf
KITTI Cyclists Moderate,,# 1,AP,62.25 %,PointPillars,-,Birds Eye View Object Detection,PointPillars: Fast Encoders for Object Detection from Point Clouds,/paper/pointpillars-fast-encoders-for-object,https://arxiv.org/pdf/1812.05784v1.pdf
KITTI Cyclists Moderate,,# 1,AP,59.07 %,PointPillars,-,3D Object Detection,PointPillars: Fast Encoders for Object Detection from Point Clouds,/paper/pointpillars-fast-encoders-for-object,https://arxiv.org/pdf/1812.05784v1.pdf
KITTI Pedestrians Moderate,,# 1,AP,50.23 %,PointPillars,-,Birds Eye View Object Detection,PointPillars: Fast Encoders for Object Detection from Point Clouds,/paper/pointpillars-fast-encoders-for-object,https://arxiv.org/pdf/1812.05784v1.pdf
KITTI Pedestrians Moderate,,# 3,AP,43.53 %,PointPillars,-,3D Object Detection,PointPillars: Fast Encoders for Object Detection from Point Clouds,/paper/pointpillars-fast-encoders-for-object,https://arxiv.org/pdf/1812.05784v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 3,Accuracy,56.20%,Cosine similarity function + C64F feature extractor,-,Few-Shot Image Classification,Dynamic Few-Shot Visual Learning without Forgetting,/paper/dynamic-few-shot-visual-learning-without,https://arxiv.org/pdf/1804.09458v1.pdf
Mini-ImageNet - 5-Shot Learning,,# 3,Accuracy,73.00%,Cosine similarity function + C128F feature extractor,-,Few-Shot Image Classification,Dynamic Few-Shot Visual Learning without Forgetting,/paper/dynamic-few-shot-visual-learning-without,https://arxiv.org/pdf/1804.09458v1.pdf
SVHN,,# 14,Percentage error,2.01,FractalNet,-,Image Classification,FractalNet: Ultra-Deep Neural Networks without Residuals,/paper/fractalnet-ultra-deep-neural-networks-without,https://arxiv.org/pdf/1605.07648v4.pdf
IWSLT2015 English-German,,# 6,BLEU score,25.36,NPMT + language model,-,Machine Translation,Towards Neural Phrase-based Machine Translation,/paper/towards-neural-phrase-based-machine,https://arxiv.org/pdf/1706.05565v8.pdf
IWSLT2015 German-English,,# 10,BLEU score,30.08,NPMT + language model,-,Machine Translation,Towards Neural Phrase-based Machine Translation,/paper/towards-neural-phrase-based-machine,https://arxiv.org/pdf/1706.05565v8.pdf
SQuAD1.1,,# 64,EM,76.125,KAR (single model),-,Question Answering,Exploring Machine Reading Comprehension with Explicit Knowledge,/paper/exploring-machine-reading-comprehension-with,https://arxiv.org/pdf/1809.03449v1.pdf
SQuAD1.1,,# 67,F1,83.538,KAR (single model),-,Question Answering,Exploring Machine Reading Comprehension with Explicit Knowledge,/paper/exploring-machine-reading-comprehension-with,https://arxiv.org/pdf/1809.03449v1.pdf
MSU-MFSD,,# 1,Equal Error Rate,7.5%,GFA-CNN,-,Face Anti-Spoofing,Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing,/paper/learning-generalizable-and-identity,https://arxiv.org/pdf/1901.05602v1.pdf
Atari-57,,# 1,Medium Human-Normalized Score,434.1%,Ape-X,-,Atari Games,Distributed Prioritized Experience Replay,/paper/distributed-prioritized-experience-replay,https://arxiv.org/pdf/1803.00933v1.pdf
BSD200 sigma10,,# 1,PSNR,33.63,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma10,,# 1,SSIM,0.9319,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma30,,# 1,PSNR,27.95,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma30,,# 1,SSIM,0.8019,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma50,,# 1,PSNR,25.75,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma50,,# 1,SSIM,0.7167,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma70,,# 1,PSNR,24.37,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD200 sigma70,,# 1,SSIM,0.6551,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
Urban100 sigma50,,# 5,PSNR,26.32,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
Urban100 sigma70,,# 3,PSNR,24.63,RED30,-,Image Denoising,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,/paper/image-restoration-using-convolutional-auto,https://arxiv.org/pdf/1606.08921v3.pdf
BSD100 - 4x upscaling,,# 30,PSNR,25.33,SFT-GAN,-,Image Super-Resolution,Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,/paper/recovering-realistic-texture-in-image-super,https://arxiv.org/pdf/1804.02815v1.pdf
BSD100 - 4x upscaling,,# 31,SSIM,0.6509999999999999,SFT-GAN,-,Image Super-Resolution,Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,/paper/recovering-realistic-texture-in-image-super,https://arxiv.org/pdf/1804.02815v1.pdf
Set14 - 4x upscaling,,# 35,PSNR,26.13,SFT-GAN,-,Image Super-Resolution,Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,/paper/recovering-realistic-texture-in-image-super,https://arxiv.org/pdf/1804.02815v1.pdf
Set14 - 4x upscaling,,# 33,SSIM,0.6940000000000001,SFT-GAN,-,Image Super-Resolution,Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,/paper/recovering-realistic-texture-in-image-super,https://arxiv.org/pdf/1804.02815v1.pdf
Set5 - 4x upscaling,,# 29,PSNR,29.82,SFT-GAN,-,Image Super-Resolution,Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,/paper/recovering-realistic-texture-in-image-super,https://arxiv.org/pdf/1804.02815v1.pdf
Set5 - 4x upscaling,,# 30,SSIM,0.84,SFT-GAN,-,Image Super-Resolution,Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,/paper/recovering-realistic-texture-in-image-super,https://arxiv.org/pdf/1804.02815v1.pdf
enwiki8,,# 2,Bit per Character (BPC),0.940,Transformer-XL + RMS dynamic eval + decay,-,Language Modelling,Dynamic Evaluation of Transformer Language Models,/paper/dynamic-evaluation-of-transformer-language,https://arxiv.org/pdf/1904.08378v1.pdf
enwiki8,,# 1,Number of params,277M,Transformer-XL + RMS dynamic eval + decay,-,Language Modelling,Dynamic Evaluation of Transformer Language Models,/paper/dynamic-evaluation-of-transformer-language,https://arxiv.org/pdf/1904.08378v1.pdf
Text8,,# 2,Bit per Character (BPC),1.038,Transformer-XL + RMS dynamic eval + decay,-,Language Modelling,Dynamic Evaluation of Transformer Language Models,/paper/dynamic-evaluation-of-transformer-language,https://arxiv.org/pdf/1904.08378v1.pdf
Text8,,# 1,Number of params,277M,Transformer-XL + RMS dynamic eval + decay,-,Language Modelling,Dynamic Evaluation of Transformer Language Models,/paper/dynamic-evaluation-of-transformer-language,https://arxiv.org/pdf/1904.08378v1.pdf
SNLI,,# 32,% Test Accuracy,85.5,600D (300+300) Deep Gated Attn. BiLSTM encoders,-,Natural Language Inference,Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference,/paper/recurrent-neural-network-based-sentence,https://arxiv.org/pdf/1708.01353v1.pdf
SNLI,,# 31,% Train Accuracy,90.5,600D (300+300) Deep Gated Attn. BiLSTM encoders,-,Natural Language Inference,Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference,/paper/recurrent-neural-network-based-sentence,https://arxiv.org/pdf/1708.01353v1.pdf
SNLI,,# 1,Parameters,12m,600D (300+300) Deep Gated Attn. BiLSTM encoders,-,Natural Language Inference,Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference,/paper/recurrent-neural-network-based-sentence,https://arxiv.org/pdf/1708.01353v1.pdf
SNLI,,# 34,% Test Accuracy,85.1,50D stacked TC-LSTMs,-,Natural Language Inference,Modelling Interaction of Sentence Pair with coupled-LSTMs,/paper/modelling-interaction-of-sentence-pair-with,https://arxiv.org/pdf/1605.05573v2.pdf
SNLI,,# 42,% Train Accuracy,86.7,50D stacked TC-LSTMs,-,Natural Language Inference,Modelling Interaction of Sentence Pair with coupled-LSTMs,/paper/modelling-interaction-of-sentence-pair-with,https://arxiv.org/pdf/1605.05573v2.pdf
SNLI,,# 1,Parameters,190k,50D stacked TC-LSTMs,-,Natural Language Inference,Modelling Interaction of Sentence Pair with coupled-LSTMs,/paper/modelling-interaction-of-sentence-pair-with,https://arxiv.org/pdf/1605.05573v2.pdf
Children's Book Test,,# 5,Accuracy-CN,68.9%,AS reader (avg),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
Children's Book Test,,# 7,Accuracy-NE,70.6%,AS reader (avg),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
Children's Book Test,,# 6,Accuracy-CN,67.5%,AS reader (greedy),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
Children's Book Test,,# 6,Accuracy-NE,71%,AS reader (greedy),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
CNN / Daily Mail,,# 12,CNN,69.5,AS Reader (single model),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
CNN / Daily Mail,,# 7,Daily Mail,73.9,AS Reader (single model),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
CNN / Daily Mail,,# 6,CNN,75.4,AS Reader (ensemble model),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
CNN / Daily Mail,,# 4,Daily Mail,77.7,AS Reader (ensemble model),-,Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
SearchQA,,# 4,Unigram Acc,41.3,ASR,-,Open-Domain Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
SearchQA,,# 5,N-gram F1,22.8,ASR,-,Open-Domain Question Answering,Text Understanding with the Attention Sum Reader Network,/paper/text-understanding-with-the-attention-sum,https://arxiv.org/pdf/1603.01547v2.pdf
ImageNet,,# 15,Top 1 Accuracy,78.8%,Inception V3,-,Image Classification,Rethinking the Inception Architecture for Computer Vision,/paper/rethinking-the-inception-architecture-for,https://arxiv.org/pdf/1512.00567v3.pdf
ImageNet,,# 11,Top 5 Accuracy,94.4%,Inception V3,-,Image Classification,Rethinking the Inception Architecture for Computer Vision,/paper/rethinking-the-inception-architecture-for,https://arxiv.org/pdf/1512.00567v3.pdf
WIDER Face (Easy),,# 7,AP,0.919,Massively-large receptive fields,-,Face Detection,Finding Tiny Faces,/paper/finding-tiny-faces,https://arxiv.org/pdf/1612.04402v2.pdf
WIDER Face (Hard),,# 8,AP,0.823,Massively-large receptive fields,-,Face Detection,Finding Tiny Faces,/paper/finding-tiny-faces,https://arxiv.org/pdf/1612.04402v2.pdf
WIDER Face (Medium),,# 7,AP,0.9079999999999999,Massively-large receptive fields,-,Face Detection,Finding Tiny Faces,/paper/finding-tiny-faces,https://arxiv.org/pdf/1612.04402v2.pdf
SNLI,,# 36,% Test Accuracy,84.6,100D DF-LSTM,-,Natural Language Inference,Deep Fusion LSTMs for Text Semantic Matching,/paper/deep-fusion-lstms-for-text-semantic-matching,https://aclweb.org/anthology/P16-1098
SNLI,,# 48,% Train Accuracy,85.2,100D DF-LSTM,-,Natural Language Inference,Deep Fusion LSTMs for Text Semantic Matching,/paper/deep-fusion-lstms-for-text-semantic-matching,https://aclweb.org/anthology/P16-1098
SNLI,,# 1,Parameters,320k,100D DF-LSTM,-,Natural Language Inference,Deep Fusion LSTMs for Text Semantic Matching,/paper/deep-fusion-lstms-for-text-semantic-matching,https://aclweb.org/anthology/P16-1098
TriviaQA,,# 3,EM,50.56,Reading Twice for NLU,-,Question Answering,Dynamic Integration of Background Knowledge in Neural NLU Systems,/paper/dynamic-integration-of-background-knowledge,https://arxiv.org/pdf/1706.02596v3.pdf
TriviaQA,,# 3,F1,56.73,Reading Twice for NLU,-,Question Answering,Dynamic Integration of Background Knowledge in Neural NLU Systems,/paper/dynamic-integration-of-background-knowledge,https://arxiv.org/pdf/1706.02596v3.pdf
Switchboard + Hub500,,# 14,Percentage error,12.2,"Deep CNN (10 conv, 4 FC layers), multi-scale feature maps",-,Speech Recognition,Very Deep Multilingual Convolutional Neural Networks for LVCSR,/paper/very-deep-multilingual-convolutional-neural,https://arxiv.org/pdf/1509.08967v2.pdf
street2shop - topwear,,# 2,Accuracy,93.69,MILDNet,-,Image Retrieval,MILDNet: A Lightweight Single Scaled Deep Ranking Architecture,/paper/mildnet-a-lightweight-single-scaled-deep,https://arxiv.org/pdf/1903.00905v2.pdf
Wikipedia-Wikidata relations,,# 1,Error rate,0.159,ContextAtt,-,Relation Extraction,Context-Aware Representations for Knowledge Base Relation Extraction,/paper/context-aware-representations-for-knowledge,https://aclweb.org/anthology/D17-1188
CNN / Daily Mail,,# 1,ROUGE-1,43.25,BERTSUM+Transformer,-,Document Summarization,Fine-tune BERT for Extractive Summarization,/paper/fine-tune-bert-for-extractive-summarization-1,https://arxiv.org/pdf/1903.10318v1.pdf
CNN / Daily Mail,,# 1,ROUGE-2,20.24,BERTSUM+Transformer,-,Document Summarization,Fine-tune BERT for Extractive Summarization,/paper/fine-tune-bert-for-extractive-summarization-1,https://arxiv.org/pdf/1903.10318v1.pdf
CNN / Daily Mail,,# 1,ROUGE-L,39.63,BERTSUM+Transformer,-,Document Summarization,Fine-tune BERT for Extractive Summarization,/paper/fine-tune-bert-for-extractive-summarization-1,https://arxiv.org/pdf/1903.10318v1.pdf
CNN / Daily Mail,,# 1,ROUGE-2,20.24,BERTSUM,-,Extractive Document Summarization,Fine-tune BERT for Extractive Summarization,/paper/fine-tune-bert-for-extractive-summarization,https://arxiv.org/pdf/1903.10318v1.pdf
CIFAR-10,,# 14,Percentage correct,95.6,ACN,-,Image Classification,Striving for Simplicity: The All Convolutional Net,/paper/striving-for-simplicity-the-all-convolutional,https://arxiv.org/pdf/1412.6806v3.pdf
CIFAR-100,,# 35,Percentage correct,66.3,ACN,-,Image Classification,Striving for Simplicity: The All Convolutional Net,/paper/striving-for-simplicity-the-all-convolutional,https://arxiv.org/pdf/1412.6806v3.pdf
COLLAB,,# 1,Accuracy,79.62%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
D&D,,# 2,Accuracy,75.38%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
ENZYMES,,# 1,Accuracy,54.67%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
IMDb-B,,# 2,Accuracy,73.10%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
IMDb-M,,# 1,Accuracy,50.27%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
MUTAG,,# 3,Accuracy,86.67%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
NCI1,,# 2,Accuracy,78.35%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
PROTEINS,,# 1,Accuracy,76.28%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
RE-M12K,,# 1,Accuracy,46.62%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
RE-M5K,,# 1,Accuracy,52.88%,CapsGNN,-,Graph Classification,Capsule Graph Neural Network,/paper/capsule-graph-neural-network,https://openreview.net/pdf?id=Byl8BnRcYm
CIFAR-10,,# 35,Percentage correct,91.5,MIM,-,Image Classification,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,/paper/on-the-importance-of-normalisation-layers-in,https://arxiv.org/pdf/1508.00330v2.pdf
CIFAR-100,,# 23,Percentage correct,70.8,MIM,-,Image Classification,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,/paper/on-the-importance-of-normalisation-layers-in,https://arxiv.org/pdf/1508.00330v2.pdf
MNIST,,# 4,Percentage error,0.4,MIM,-,Image Classification,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,/paper/on-the-importance-of-normalisation-layers-in,https://arxiv.org/pdf/1508.00330v2.pdf
SVHN,,# 13,Percentage error,1.97,MIM,-,Image Classification,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,/paper/on-the-importance-of-normalisation-layers-in,https://arxiv.org/pdf/1508.00330v2.pdf
PASCAL-Person-Part,,# 3,AP 0.5,38.80%,MNC,-,Multi-Human Parsing,Instance-aware Semantic Segmentation via Multi-task Network Cascades,/paper/instance-aware-semantic-segmentation-via,https://arxiv.org/pdf/1512.04412v1.pdf
IWSLT2015 German-English,,# 9,BLEU score,30.4,Conv-LSTM (deep+pos),-,Machine Translation,A Convolutional Encoder Model for Neural Machine Translation,/paper/a-convolutional-encoder-model-for-neural,https://arxiv.org/pdf/1611.02344v3.pdf
WMT2014 English-French,,# 20,BLEU score,35.7,Deep Convolutional Encoder; single-layer decoder,-,Machine Translation,A Convolutional Encoder Model for Neural Machine Translation,/paper/a-convolutional-encoder-model-for-neural,https://arxiv.org/pdf/1611.02344v3.pdf
WMT2016 English-Romanian,,# 6,BLEU score,27.8,Deep Convolutional Encoder; single-layer decoder,-,Machine Translation,A Convolutional Encoder Model for Neural Machine Translation,/paper/a-convolutional-encoder-model-for-neural,https://arxiv.org/pdf/1611.02344v3.pdf
WMT2016 English-Romanian,,# 7,BLEU score,27.5,BiLSTM,-,Machine Translation,A Convolutional Encoder Model for Neural Machine Translation,/paper/a-convolutional-encoder-model-for-neural,https://arxiv.org/pdf/1611.02344v3.pdf
AG News,,# 2,Error,5.01,ULMFiT,-,Text Classification,Universal Language Model Fine-tuning for Text Classification,/paper/universal-language-model-fine-tuning-for-text,https://arxiv.org/pdf/1801.06146v5.pdf
DBpedia,,# 3,Error,0.8,ULMFiT,-,Text Classification,Universal Language Model Fine-tuning for Text Classification,/paper/universal-language-model-fine-tuning-for-text,https://arxiv.org/pdf/1801.06146v5.pdf
IMDb,,# 4,Accuracy,95.4,ULMFiT,-,Sentiment Analysis,Universal Language Model Fine-tuning for Text Classification,/paper/universal-language-model-fine-tuning-for-text,https://arxiv.org/pdf/1801.06146v5.pdf
TREC-6,,# 2,Error,3.6,ULMFiT,-,Text Classification,Universal Language Model Fine-tuning for Text Classification,/paper/universal-language-model-fine-tuning-for-text,https://arxiv.org/pdf/1801.06146v5.pdf
Yelp Binary classification,,# 3,Error,2.16,ULMFiT,-,Sentiment Analysis,Universal Language Model Fine-tuning for Text Classification,/paper/universal-language-model-fine-tuning-for-text,https://arxiv.org/pdf/1801.06146v5.pdf
Yelp Fine-grained classification,,# 2,Error,29.98,ULMFiT,-,Sentiment Analysis,Universal Language Model Fine-tuning for Text Classification,/paper/universal-language-model-fine-tuning-for-text,https://arxiv.org/pdf/1801.06146v5.pdf
BP4D,,# 1,Average Accuracy,81.8%,Multi-View,-,Facial Action Unit Detection,Multi-View Dynamic Facial Action Unit Detection,/paper/multi-view-dynamic-facial-action-unit,https://arxiv.org/pdf/1704.07863v2.pdf
BP4D,,# 1,F1,57.7,Multi-View,-,Facial Action Unit Detection,Multi-View Dynamic Facial Action Unit Detection,/paper/multi-view-dynamic-facial-action-unit,https://arxiv.org/pdf/1704.07863v2.pdf
COCO 2014,,# 1,DCG,2.447,Text2Vis,-,Cross-Modal Retrieval,Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions,/paper/picture-it-in-your-mind-generating-high-level,https://arxiv.org/pdf/1606.07287v1.pdf
Caltech,,# 4,Reasonable Miss Rate,4.5,CSP,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
Caltech,,# 1,Reasonable Miss Rate,3.8,CSP + CityPersons dataset,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 1,Reasonable MR^-2,11.0,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 1,Heavy MR^-2,49.3,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 1,Partial MR^-2,10.4,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 2,Bare MR^-2,7.3,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 1,Small MR^-2,16.0,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 1,Medium MR^-2,3.7,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 1,Large MR^-2,6.5,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
CityPersons,,# 2,Test Time,0.33s/img,CSP (with offset) + ResNet-50,-,Pedestrian Detection,High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection,/paper/high-level-semantic-feature-detectiona-new,https://arxiv.org/pdf/1904.02948v2.pdf
MPII Human Pose,,# 4,PCKh-0.5,91.2%,DU-Net,-,Pose Estimation,Quantized Densely Connected U-Nets for Efficient Landmark Localization,/paper/quantized-densely-connected-u-nets-for,https://arxiv.org/pdf/1808.02194v2.pdf
Labeled Faces in the Wild,,# 9,Accuracy,99.15%,DeepId2,-,Face Verification,Deep Learning Face Representation by Joint Identification-Verification,/paper/deep-learning-face-representation-by-joint,https://arxiv.org/pdf/1406.4773v1.pdf
CIFAR-10,,# 30,Percentage correct,92.4,VDN,-,Image Classification,Training Very Deep Networks,/paper/training-very-deep-networks,https://arxiv.org/pdf/1507.06228v2.pdf
CIFAR-10,,# 15,Percentage error,7.6,VDN,-,Image Classification,Training Very Deep Networks,/paper/training-very-deep-networks,https://arxiv.org/pdf/1507.06228v2.pdf
CIFAR-100,,# 30,Percentage correct,67.8,VDN,-,Image Classification,Training Very Deep Networks,/paper/training-very-deep-networks,https://arxiv.org/pdf/1507.06228v2.pdf
CIFAR-100,,# 9,Percentage error,32.34,VDN,-,Image Classification,Training Very Deep Networks,/paper/training-very-deep-networks,https://arxiv.org/pdf/1507.06228v2.pdf
MNIST,,# 5,Percentage error,0.5,VDN,-,Image Classification,Training Very Deep Networks,/paper/training-very-deep-networks,https://arxiv.org/pdf/1507.06228v2.pdf
Penn Treebank,,# 6,F1 score,94.2,In-order,-,Constituency Parsing,In-Order Transition-based Constituent Parsing,/paper/in-order-transition-based-constituent-parsing-1,https://aclweb.org/anthology/Q17-1029
General,,# 4,MAP,8.95,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
General,,# 4,MRR,19.44,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
General,,# 4,[emailÂ protected],8.63,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
Medical domain,,# 3,MAP,20.75,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
Medical domain,,# 4,MRR,40.6,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
Medical domain,,# 3,[emailÂ protected],21.43,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
Music domain,,# 3,MAP,29.54,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
Music domain,,# 3,MRR,46.43,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
Music domain,,# 3,[emailÂ protected],28.86,300-sparsans,-,Hypernym Discovery,300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes,/paper/300-sparsans-at-semeval-2018-task-9-hypernymy,https://aclweb.org/anthology/S18-1152
CIFAR-10,,# 23,Percentage correct,93.6,Tuned CNN,-,Image Classification,Scalable Bayesian Optimization Using Deep Neural Networks,/paper/scalable-bayesian-optimization-using-deep,https://arxiv.org/pdf/1502.05700v2.pdf
CIFAR-100,,# 19,Percentage correct,72.6,Tuned CNN,-,Image Classification,Scalable Bayesian Optimization Using Deep Neural Networks,/paper/scalable-bayesian-optimization-using-deep,https://arxiv.org/pdf/1502.05700v2.pdf
BRATS 2018,,# 1,Dice Score,0.87049,NVDLMED,-,Brain Tumor Segmentation,3D MRI brain tumor segmentation using autoencoder regularization,/paper/3d-mri-brain-tumor-segmentation-using,https://arxiv.org/pdf/1810.11654v3.pdf
Penn Treebank,,# 3,POS,97.3,BIST transition-based parser,-,Dependency Parsing,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,/paper/simple-and-accurate-dependency-parsing-using,https://arxiv.org/pdf/1603.04351v3.pdf
Penn Treebank,,# 7,UAS,93.9,BIST transition-based parser,-,Dependency Parsing,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,/paper/simple-and-accurate-dependency-parsing-using,https://arxiv.org/pdf/1603.04351v3.pdf
Penn Treebank,,# 7,LAS,91.9,BIST transition-based parser,-,Dependency Parsing,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,/paper/simple-and-accurate-dependency-parsing-using,https://arxiv.org/pdf/1603.04351v3.pdf
Penn Treebank,,# 3,POS,97.3,BIST graph-based parser,-,Dependency Parsing,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,/paper/simple-and-accurate-dependency-parsing-using,https://arxiv.org/pdf/1603.04351v3.pdf
Penn Treebank,,# 9,UAS,93.1,BIST graph-based parser,-,Dependency Parsing,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,/paper/simple-and-accurate-dependency-parsing-using,https://arxiv.org/pdf/1603.04351v3.pdf
Penn Treebank,,# 9,LAS,91.0,BIST graph-based parser,-,Dependency Parsing,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,/paper/simple-and-accurate-dependency-parsing-using,https://arxiv.org/pdf/1603.04351v3.pdf
WMT2014 English-French,,# 9,BLEU score,40.6,Evolved Transformer Base,-,Machine Translation,The Evolved Transformer,/paper/the-evolved-transformer,https://arxiv.org/pdf/1901.11117v3.pdf
WMT2014 English-French,,# 6,BLEU score,41.3,Evolved Transformer Big,-,Machine Translation,The Evolved Transformer,/paper/the-evolved-transformer,https://arxiv.org/pdf/1901.11117v3.pdf
WMT2014 English-German,,# 7,BLEU score,28.4,Evolved Transformer Base,-,Machine Translation,The Evolved Transformer,/paper/the-evolved-transformer,https://arxiv.org/pdf/1901.11117v3.pdf
WMT2014 English-German,,# 3,BLEU score,29.3,Evolved Transformer Big,-,Machine Translation,The Evolved Transformer,/paper/the-evolved-transformer,https://arxiv.org/pdf/1901.11117v3.pdf
AG News,,# 1,Error,4.95,L MIXED,-,Text Classification,Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function,/paper/revisiting-lstm-networks-for-semi-supervised,https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf
DBpedia,,# 2,Error,0.7,L MIXED,-,Text Classification,Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function,/paper/revisiting-lstm-networks-for-semi-supervised,https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf
IMDb,,# 2,Accuracy,95.68,L MIXED,-,Sentiment Analysis,Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function,/paper/revisiting-lstm-networks-for-semi-supervised,https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf
Atari 2600 Alien,,# 1,Score,7022.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Amidar,,# 1,Score,2946.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Assault,,# 1,Score,29091.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Asterix,,# 4,Score,342016.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Asteroids,,# 4,Score,2898.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Atlantis,,# 3,Score,978200.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Bank Heist,,# 3,Score,1416.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Battle Zone,,# 1,Score,42244.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Beam Rider,,# 1,Score,42776.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Berzerk,,# 9,Score,1053.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Bowling,,# 2,Score,86.5,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Boxing,,# 1,Score,99.8,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Breakout,,# 4,Score,734.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Centipede,,# 2,Score,11561.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Chopper Command,,# 1,Score,16836.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Crazy Climber,,# 2,Score,179082.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Defender,,# 1,Score,53537.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Demon Attack,,# 2,Score,128580.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Double Dunk,,# 3,Score,5.6,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Enduro,,# 2,Score,2359.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Fishing Derby,,# 5,Score,33.8,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Freeway,,# 1,Score,34.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Frostbite,,# 5,Score,4324.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Gopher,,# 1,Score,118365.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Gravitar,,# 2,Score,911.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 HERO,,# 5,Score,28386.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Ice Hockey,,# 3,Score,0.2,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 James Bond,,# 1,Score,35108.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Kangaroo,,# 2,Score,15487.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Krull,,# 2,Score,10707.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Kung-Fu Master,,# 1,Score,73512.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Ms. Pacman,,# 2,Score,6349.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Name This Game,,# 1,Score,22682.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Phoenix,,# 2,Score,56599.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Pitfall!,,# 1,Score,0.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Pong,,# 1,Score,21.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Private Eye,,# 16,Score,200.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Q*Bert,,# 1,Score,25750.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 River Raid,,# 3,Score,17765.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Road Runner,,# 5,Score,57900.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Robotank,,# 7,Score,62.5,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Seaquest,,# 4,Score,30140.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Skiing,,# 1,Score,-9289.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Solaris,,# 1,Score,8007.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Space Invaders,,# 1,Score,28888.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Star Gunner,,# 7,Score,74677.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Surround,,# 1,Score,9.4,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Tennis,,# 1,Score,23.6,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Time Pilot,,# 3,Score,12236.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Tutankham,,# 1,Score,293.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Up and Down,,# 2,Score,88148.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Venture,,# 3,Score,1318.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Video Pinball,,# 3,Score,698045.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Wizard of Wor,,# 1,Score,31190.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Yars Revenge,,# 1,Score,28379.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
Atari 2600 Zaxxon,,# 3,Score,21772.0,IQN,-,Atari Games,Implicit Quantile Networks for Distributional Reinforcement Learning,/paper/implicit-quantile-networks-for-distributional,https://arxiv.org/pdf/1806.06923v1.pdf
CIFAR-10,,# 16,Percentage correct,95.4,ResNet-1001,-,Image Classification,Identity Mappings in Deep Residual Networks,/paper/identity-mappings-in-deep-residual-networks,https://arxiv.org/pdf/1603.05027v3.pdf
CIFAR-10,,# 11,Percentage error,4.62,ResNet-1001,-,Image Classification,Identity Mappings in Deep Residual Networks,/paper/identity-mappings-in-deep-residual-networks,https://arxiv.org/pdf/1603.05027v3.pdf
CIFAR-100,,# 12,Percentage correct,77.3,ResNet-1001,-,Image Classification,Identity Mappings in Deep Residual Networks,/paper/identity-mappings-in-deep-residual-networks,https://arxiv.org/pdf/1603.05027v3.pdf
CIFAR-100,,# 6,Percentage error,22.71,ResNet-1001,-,Image Classification,Identity Mappings in Deep Residual Networks,/paper/identity-mappings-in-deep-residual-networks,https://arxiv.org/pdf/1603.05027v3.pdf
BlogCatalog,,# 1,Accuracy,23.20%,GraphGAN,-,Node Classification,GraphGAN: Graph Representation Learning with Generative Adversarial Nets,/paper/graphgan-graph-representation-learning-with,https://arxiv.org/pdf/1711.08267v1.pdf
BlogCatalog,,# 1,Macro-F1,0.221,GraphGAN,-,Node Classification,GraphGAN: Graph Representation Learning with Generative Adversarial Nets,/paper/graphgan-graph-representation-learning-with,https://arxiv.org/pdf/1711.08267v1.pdf
Wikipedia,,# 1,Accuracy,21.30%,GraphGAN,-,Node Classification,GraphGAN: Graph Representation Learning with Generative Adversarial Nets,/paper/graphgan-graph-representation-learning-with,https://arxiv.org/pdf/1711.08267v1.pdf
Wikipedia,,# 1,Macro-F1,0.194,GraphGAN,-,Node Classification,GraphGAN: Graph Representation Learning with Generative Adversarial Nets,/paper/graphgan-graph-representation-learning-with,https://arxiv.org/pdf/1711.08267v1.pdf
ICSI Meeting Recorder Dialog Act (MRDA) corpus,,# 2,Accuracy,90.9,Bi-LSTM-CRF,-,Dialogue Act Classification,Dialogue Act Sequence Labeling using Hierarchical encoder with CRF,/paper/dialogue-act-sequence-labeling-using,https://arxiv.org/pdf/1709.04250v2.pdf
Switchboard corpus,,# 2,Accuracy,79.2,Bi-LSTM-CRF,-,Dialogue Act Classification,Dialogue Act Sequence Labeling using Hierarchical encoder with CRF,/paper/dialogue-act-sequence-labeling-using,https://arxiv.org/pdf/1709.04250v2.pdf
BSD100 - 4x upscaling,,# 22,PSNR,27.16,IA,-,Image Super-Resolution,Seven ways to improve example-based single image super resolution,/paper/seven-ways-to-improve-example-based-single,https://arxiv.org/pdf/1511.02228v1.pdf
Set14 - 4x upscaling,,# 25,PSNR,27.88,IA,-,Image Super-Resolution,Seven ways to improve example-based single image super resolution,/paper/seven-ways-to-improve-example-based-single,https://arxiv.org/pdf/1511.02228v1.pdf
Set5 - 4x upscaling,,# 21,PSNR,31.1,IA,-,Image Super-Resolution,Seven ways to improve example-based single image super resolution,/paper/seven-ways-to-improve-example-based-single,https://arxiv.org/pdf/1511.02228v1.pdf
WikiBio,,# 3,BLEU,34.7,Table NLM,-,Table-to-text Generation,Neural Text Generation from Structured Data with Application to the Biography Domain,/paper/neural-text-generation-from-structured-data,https://arxiv.org/pdf/1603.07771v3.pdf
WikiBio,,# 3,ROUGE,25.8,Table NLM,-,Table-to-text Generation,Neural Text Generation from Structured Data with Application to the Biography Domain,/paper/neural-text-generation-from-structured-data,https://arxiv.org/pdf/1603.07771v3.pdf
SNLI,,# 21,% Test Accuracy,86.8,600D Dynamic Self-Attention Model,-,Natural Language Inference,Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding,/paper/dynamic-self-attention-computing-attention,https://arxiv.org/pdf/1808.07383v1.pdf
SNLI,,# 40,% Train Accuracy,87.3,600D Dynamic Self-Attention Model,-,Natural Language Inference,Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding,/paper/dynamic-self-attention-computing-attention,https://arxiv.org/pdf/1808.07383v1.pdf
SNLI,,# 1,Parameters,2.1m,600D Dynamic Self-Attention Model,-,Natural Language Inference,Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding,/paper/dynamic-self-attention-computing-attention,https://arxiv.org/pdf/1808.07383v1.pdf
SNLI,,# 18,% Test Accuracy,87.4,2400D Multiple-Dynamic Self-Attention Model,-,Natural Language Inference,Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding,/paper/dynamic-self-attention-computing-attention,https://arxiv.org/pdf/1808.07383v1.pdf
SNLI,,# 38,% Train Accuracy,89.0,2400D Multiple-Dynamic Self-Attention Model,-,Natural Language Inference,Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding,/paper/dynamic-self-attention-computing-attention,https://arxiv.org/pdf/1808.07383v1.pdf
SNLI,,# 1,Parameters,7.0m,2400D Multiple-Dynamic Self-Attention Model,-,Natural Language Inference,Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding,/paper/dynamic-self-attention-computing-attention,https://arxiv.org/pdf/1808.07383v1.pdf
IWSLT2015 German-English,,# 3,BLEU score,33.97,Transformer with FRAGE,-,Machine Translation,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
Penn Treebank (Word Level),,# 1,Validation perplexity,47.38,FRAGE + AWD-LSTM-MoS + dynamic eval,-,Language Modelling,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
Penn Treebank (Word Level),,# 2,Test perplexity,46.54,FRAGE + AWD-LSTM-MoS + dynamic eval,-,Language Modelling,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
Penn Treebank (Word Level),,# 1,Params,22M,FRAGE + AWD-LSTM-MoS + dynamic eval,-,Language Modelling,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
WikiText-2,,# 1,Validation perplexity,40.85,FRAGE + AWD-LSTM-MoS + dynamic eval,-,Language Modelling,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
WikiText-2,,# 2,Test perplexity,39.14,FRAGE + AWD-LSTM-MoS + dynamic eval,-,Language Modelling,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
WikiText-2,,# 1,Number of params,35M,FRAGE + AWD-LSTM-MoS + dynamic eval,-,Language Modelling,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
WMT2014 English-German,,# 5,BLEU score,29.11,Transformer Big with FRAGE,-,Machine Translation,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,https://arxiv.org/pdf/1809.06858v1.pdf
Cityscapes,,# 17,Mean IoU,62.5%,CRF-RNN,-,Semantic Segmentation,Conditional Random Fields as Recurrent Neural Networks,/paper/conditional-random-fields-as-recurrent-neural-1,https://arxiv.org/pdf/1502.03240v3.pdf
Cityscapes,,# 9,mIoU,62.5%,CRF-RNN,-,Real-Time Semantic Segmentation,Conditional Random Fields as Recurrent Neural Networks,/paper/conditional-random-fields-as-recurrent-neural-1,https://arxiv.org/pdf/1502.03240v3.pdf
Cityscapes,,# 6,Time (ms),700,CRF-RNN,-,Real-Time Semantic Segmentation,Conditional Random Fields as Recurrent Neural Networks,/paper/conditional-random-fields-as-recurrent-neural-1,https://arxiv.org/pdf/1502.03240v3.pdf
Cityscapes,,# 8,Frame (fps),1.4,CRF-RNN,-,Real-Time Semantic Segmentation,Conditional Random Fields as Recurrent Neural Networks,/paper/conditional-random-fields-as-recurrent-neural-1,https://arxiv.org/pdf/1502.03240v3.pdf
PASCAL Context,,# 12,mIoU,39.3,CRF-RNN,-,Semantic Segmentation,Conditional Random Fields as Recurrent Neural Networks,/paper/conditional-random-fields-as-recurrent-neural-1,https://arxiv.org/pdf/1502.03240v3.pdf
PASCAL VOC 2012,,# 13,Mean IoU,74.7%,CRF-RNN,-,Semantic Segmentation,Conditional Random Fields as Recurrent Neural Networks,/paper/conditional-random-fields-as-recurrent-neural-1,https://arxiv.org/pdf/1502.03240v3.pdf
SemEval 2014 Task 4 Sub Task 2,,# 10,Restaurant (Acc),81.16,PBAN,-,Aspect-Based Sentiment Analysis,A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis,/paper/a-position-aware-bidirectional-attention,https://aclweb.org/anthology/C18-1066
SemEval 2014 Task 4 Sub Task 2,,# 13,Laptop (Acc),74.12,PBAN,-,Aspect-Based Sentiment Analysis,A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis,/paper/a-position-aware-bidirectional-attention,https://aclweb.org/anthology/C18-1066
Winograd Schema Challenge,,# 2,Score,62.6,Word-LM-partial,-,Common Sense Reasoning,A Simple Method for Commonsense Reasoning,/paper/a-simple-method-for-commonsense-reasoning,https://arxiv.org/pdf/1806.02847v1.pdf
Winograd Schema Challenge,,# 3,Score,57.9,Char-LM-partial,-,Common Sense Reasoning,A Simple Method for Commonsense Reasoning,/paper/a-simple-method-for-commonsense-reasoning,https://arxiv.org/pdf/1806.02847v1.pdf
ELO Ratings,,# 1,ELO Rating,5185,AlphaGo Zero,-,Game of Go,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,/paper/mastering-chess-and-shogi-by-self-play-with-a,https://arxiv.org/pdf/1712.01815v1.pdf
ELO Ratings,,# 1,ELO Rating,4650,AlphaZero,-,Game of Shogi,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,/paper/mastering-chess-and-shogi-by-self-play-with-a,https://arxiv.org/pdf/1712.01815v1.pdf
ImageNet VID,,# 1,MAP,80.1%,FGFA + Seq-NMS,-,Video Object Detection,Flow-Guided Feature Aggregation for Video Object Detection,/paper/flow-guided-feature-aggregation-for-video,https://arxiv.org/pdf/1703.10025v2.pdf
ImageNet VID,,# 1,runtime (ms),954,FGFA + Seq-NMS,-,Video Object Detection,Flow-Guided Feature Aggregation for Video Object Detection,/paper/flow-guided-feature-aggregation-for-video,https://arxiv.org/pdf/1703.10025v2.pdf
GigaWord,,# 4,ROUGE-1,36.3,CGU,-,Text Summarization,Global Encoding for Abstractive Summarization,/paper/global-encoding-for-abstractive-summarization-1,https://aclweb.org/anthology/P18-2027
GigaWord,,# 2,ROUGE-2,18.0,CGU,-,Text Summarization,Global Encoding for Abstractive Summarization,/paper/global-encoding-for-abstractive-summarization-1,https://aclweb.org/anthology/P18-2027
GigaWord,,# 5,ROUGE-L,33.8,CGU,-,Text Summarization,Global Encoding for Abstractive Summarization,/paper/global-encoding-for-abstractive-summarization-1,https://aclweb.org/anthology/P18-2027
VOT2017/18,,# 3,Expected Average Overlap (EAO),0.345,STRCF,-,Visual Object Tracking,Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking,/paper/learning-spatial-temporal-regularized,https://arxiv.org/pdf/1803.08679v1.pdf
OccludedLINEMOD,,# 3,Accuracy,76.3%,Point Pair Features,-,6D Pose Estimation,Going Further with Point Pair Features,/paper/going-further-with-point-pair-features,https://arxiv.org/pdf/1711.04061v1.pdf
ADE20K,,# 2,Validation mIoU,44.65,EncNet,-,Semantic Segmentation,Context Encoding for Semantic Segmentation,/paper/context-encoding-for-semantic-segmentation,https://arxiv.org/pdf/1803.08904v1.pdf
ADE20K,,# 2,Test Score,0.5567,EncNet,-,Semantic Segmentation,Context Encoding for Semantic Segmentation,/paper/context-encoding-for-semantic-segmentation,https://arxiv.org/pdf/1803.08904v1.pdf
PASCAL Context,,# 4,mIoU,51.7,EncNet,-,Semantic Segmentation,Context Encoding for Semantic Segmentation,/paper/context-encoding-for-semantic-segmentation,https://arxiv.org/pdf/1803.08904v1.pdf
PASCAL VOC 2012,,# 4,Mean IoU,85.9%,EncNet,-,Semantic Segmentation,Context Encoding for Semantic Segmentation,/paper/context-encoding-for-semantic-segmentation,https://arxiv.org/pdf/1803.08904v1.pdf
CIFAR-100,,# 4,Percentage correct,83.44,Res2NeXt-29,-,Image Classification,Res2Net: A New Multi-scale Backbone Architecture,/paper/res2net-a-new-multi-scale-backbone,https://arxiv.org/pdf/1904.01169v1.pdf
CIFAR-100,,# 2,Percentage error,16.56,Res2NeXt-29,-,Image Classification,Res2Net: A New Multi-scale Backbone Architecture,/paper/res2net-a-new-multi-scale-backbone,https://arxiv.org/pdf/1904.01169v1.pdf
ImageNet,,# 9,Top 1 Accuracy,79.47%,Res2Net-DLA-60,-,Image Classification,Res2Net: A New Multi-scale Backbone Architecture,/paper/res2net-a-new-multi-scale-backbone,https://arxiv.org/pdf/1904.01169v1.pdf
SQuAD1.1,,# 127,EM,64.083,OTF dict+spelling (single),-,Question Answering,Learning to Compute Word Embeddings On the Fly,/paper/learning-to-compute-word-embeddings-on-the,https://arxiv.org/pdf/1706.00286v3.pdf
SQuAD1.1,,# 131,F1,73.056,OTF dict+spelling (single),-,Question Answering,Learning to Compute Word Embeddings On the Fly,/paper/learning-to-compute-word-embeddings-on-the,https://arxiv.org/pdf/1706.00286v3.pdf
SQuAD1.1,,# 129,EM,62.897,OTF spelling (single),-,Question Answering,Learning to Compute Word Embeddings On the Fly,/paper/learning-to-compute-word-embeddings-on-the,https://arxiv.org/pdf/1706.00286v3.pdf
SQuAD1.1,,# 132,F1,72.016,OTF spelling (single),-,Question Answering,Learning to Compute Word Embeddings On the Fly,/paper/learning-to-compute-word-embeddings-on-the,https://arxiv.org/pdf/1706.00286v3.pdf
SQuAD1.1,,# 130,EM,62.604,OTF spelling+lemma (single),-,Question Answering,Learning to Compute Word Embeddings On the Fly,/paper/learning-to-compute-word-embeddings-on-the,https://arxiv.org/pdf/1706.00286v3.pdf
SQuAD1.1,,# 133,F1,71.968,OTF spelling+lemma (single),-,Question Answering,Learning to Compute Word Embeddings On the Fly,/paper/learning-to-compute-word-embeddings-on-the,https://arxiv.org/pdf/1706.00286v3.pdf
SNLI,,# 14,% Test Accuracy,88.3,150D Multiway Attention Network,-,Natural Language Inference,Multiway Attention Networks for Modeling Sentence Pairs,/paper/multiway-attention-networks-for-modeling,https://www.ijcai.org/proceedings/2018/0613.pdf
SNLI,,# 12,% Train Accuracy,94.5,150D Multiway Attention Network,-,Natural Language Inference,Multiway Attention Networks for Modeling Sentence Pairs,/paper/multiway-attention-networks-for-modeling,https://www.ijcai.org/proceedings/2018/0613.pdf
SNLI,,# 1,Parameters,14m,150D Multiway Attention Network,-,Natural Language Inference,Multiway Attention Networks for Modeling Sentence Pairs,/paper/multiway-attention-networks-for-modeling,https://www.ijcai.org/proceedings/2018/0613.pdf
SNLI,,# 6,% Test Accuracy,89.4,150D Multiway Attention Network Ensemble,-,Natural Language Inference,Multiway Attention Networks for Modeling Sentence Pairs,/paper/multiway-attention-networks-for-modeling,https://www.ijcai.org/proceedings/2018/0613.pdf
SNLI,,# 7,% Train Accuracy,95.5,150D Multiway Attention Network Ensemble,-,Natural Language Inference,Multiway Attention Networks for Modeling Sentence Pairs,/paper/multiway-attention-networks-for-modeling,https://www.ijcai.org/proceedings/2018/0613.pdf
SNLI,,# 1,Parameters,58m,150D Multiway Attention Network Ensemble,-,Natural Language Inference,Multiway Attention Networks for Modeling Sentence Pairs,/paper/multiway-attention-networks-for-modeling,https://www.ijcai.org/proceedings/2018/0613.pdf
CIFAR-10,,# 18,Percentage correct,94.6,RL+NT,-,Image Classification,Efficient Architecture Search by Network Transformation,/paper/efficient-architecture-search-by-network,https://arxiv.org/pdf/1707.04873v2.pdf
Atari 2600 Alien,,# 5,Score,3747.7,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Alien,,# 17,Score,823.7,Prior+Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Alien,,# 2,Score,4461.4,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Alien,,# 11,Score,1486.5,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Alien,,# 4,Score,3941.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Amidar,,# 13,Score,238.4,Prior+Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Amidar,,# 2,Score,2354.5,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Amidar,,# 5,Score,1793.3,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Amidar,,# 18,Score,172.7,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Amidar,,# 3,Score,2296.8,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Assault,,# 3,Score,11477.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Assault,,# 15,Score,3994.8,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Assault,,# 4,Score,10950.6,Prior+Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Assault,,# 12,Score,5393.2,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Assault,,# 13,Score,4621.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asterix,,# 3,Score,364200.0,Prior+Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asterix,,# 6,Score,28188.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asterix,,# 11,Score,17356.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asterix,,# 14,Score,15840.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asterix,,# 2,Score,375080.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asteroids,,# 6,Score,2837.7,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asteroids,,# 8,Score,2035.4,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asteroids,,# 21,Score,734.7,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Asteroids,,# 16,Score,1192.7,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Atlantis,,# 9,Score,445360.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Atlantis,,# 12,Score,382572.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Atlantis,,# 19,Score,106056.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Atlantis,,# 11,Score,395762.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bank Heist,,# 2,Score,1503.1,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bank Heist,,# 8,Score,1030.6,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bank Heist,,# 5,Score,1129.3,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bank Heist,,# 1,Score,1611.9,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Battle Zone,,# 3,Score,37150.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Battle Zone,,# 5,Score,31700.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Battle Zone,,# 7,Score,31320.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Battle Zone,,# 4,Score,35520.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Beam Rider,,# 10,Score,14591.3,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Beam Rider,,# 12,Score,13772.8,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Beam Rider,,# 14,Score,12164.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Beam Rider,,# 4,Score,30276.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Berzerk,,# 4,Score,1472.6,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Berzerk,,# 7,Score,1225.4,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Berzerk,,# 11,Score,910.6,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Berzerk,,# 1,Score,3409.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bowling,,# 5,Score,68.1,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bowling,,# 6,Score,65.7,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bowling,,# 7,Score,65.5,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Bowling,,# 14,Score,46.7,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Boxing,,# 2,Score,99.4,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Boxing,,# 8,Score,91.6,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Boxing,,# 11,Score,77.3,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Boxing,,# 4,Score,98.9,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Breakout,,# 13,Score,366.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Breakout,,# 16,Score,345.3,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Breakout,,# 8,Score,411.6,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Breakout,,# 7,Score,418.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Centipede,,# 7,Score,7687.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Centipede,,# 8,Score,7561.4,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Centipede,,# 11,Score,5409.4,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Centipede,,# 12,Score,4881.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Chopper Command,,# 4,Score,11215.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Chopper Command,,# 11,Score,5809.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Chopper Command,,# 16,Score,3784.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Chopper Command,,# 3,Score,13185.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Crazy Climber,,# 10,Score,124566.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Crazy Climber,,# 12,Score,117282.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Crazy Climber,,# 4,Score,143570.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Crazy Climber,,# 3,Score,162224.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Defender,,# 2,Score,42214.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Defender,,# 3,Score,41324.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Defender,,# 4,Score,34415.0,Prior+Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Demon Attack,,# 15,Score,56322.8,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Demon Attack,,# 13,Score,60813.3,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Demon Attack,,# 14,Score,58044.2,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Demon Attack,,# 8,Score,72878.6,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Double Dunk,,# 17,Score,-12.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Double Dunk,,# 7,Score,0.1,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Double Dunk,,# 10,Score,-0.8,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Double Dunk,,# 11,Score,-5.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Enduro,,# 3,Score,2306.4,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Enduro,,# 12,Score,1211.8,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Enduro,,# 7,Score,2077.4,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Enduro,,# 4,Score,2258.2,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Fishing Derby,,# 18,Score,-4.1,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Fishing Derby,,# 3,Score,41.3,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Fishing Derby,,# 1,Score,46.4,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Fishing Derby,,# 10,Score,15.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Freeway,,# 6,Score,33.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Freeway,,# 5,Score,33.3,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Freeway,,# 21,Score,0.2,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Freeway,,# 23,Score,0.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Frostbite,,# 1,Score,7413.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Frostbite,,# 3,Score,4672.8,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Frostbite,,# 11,Score,2332.4,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Frostbite,,# 13,Score,1683.3,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gopher,,# 3,Score,104368.2,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gopher,,# 8,Score,20051.4,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gopher,,# 11,Score,15718.4,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gopher,,# 13,Score,14840.8,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gravitar,,# 22,Score,238.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gravitar,,# 4,Score,588.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gravitar,,# 12,Score,412.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Gravitar,,# 18,Score,297.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 HERO,,# 15,Score,15207.9,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 HERO,,# 10,Score,20818.2,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 HERO,,# 12,Score,20130.2,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 HERO,,# 7,Score,21036.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ice Hockey,,# 2,Score,0.5,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ice Hockey,,# 5,Score,-0.4,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ice Hockey,,# 11,Score,-2.7,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ice Hockey,,# 6,Score,-1.3,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 James Bond,,# 6,Score,1358.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 James Bond,,# 7,Score,1312.5,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 James Bond,,# 8,Score,835.5,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 James Bond,,# 9,Score,812.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kangaroo,,# 4,Score,14854.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kangaroo,,# 15,Score,1792.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kangaroo,,# 11,Score,10334.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kangaroo,,# 6,Score,12992.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Krull,,# 12,Score,7920.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Krull,,# 1,Score,11451.9,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Krull,,# 3,Score,10374.4,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Krull,,# 11,Score,8051.6,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kung-Fu Master,,# 9,Score,34294.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kung-Fu Master,,# 16,Score,24288.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kung-Fu Master,,# 12,Score,29710.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Kung-Fu Master,,# 2,Score,48375.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Montezuma's Revenge,,# 21,Score,22.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ms. Pacman,,# 6,Score,3327.3,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ms. Pacman,,# 3,Score,6283.5,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ms. Pacman,,# 9,Score,2711.4,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Ms. Pacman,,# 11,Score,2250.6,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Name This Game,,# 3,Score,15572.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Name This Game,,# 11,Score,10616.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Name This Game,,# 10,Score,11185.1,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Name This Game,,# 8,Score,11971.1,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Phoenix,,# 1,Score,63597.0,Prior+Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Pong,,# 7,Score,18.8,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Pong,,# 2,Score,20.9,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Pong,,# 1,Score,21.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Pong,,# 2,Score,20.9,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Private Eye,,# 15,Score,206.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Private Eye,,# 11,Score,292.6,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Private Eye,,# 19,Score,129.7,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Private Eye,,# 20,Score,103.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Q*Bert,,# 10,Score,15088.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Q*Bert,,# 11,Score,14175.8,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Q*Bert,,# 5,Score,18760.3,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Q*Bert,,# 4,Score,19220.3,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 River Raid,,# 2,Score,20607.6,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 River Raid,,# 7,Score,14884.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 River Raid,,# 5,Score,16569.4,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 River Raid,,# 1,Score,21162.6,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Road Runner,,# 2,Score,69524.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Road Runner,,# 4,Score,58549.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Road Runner,,# 12,Score,44127.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Road Runner,,# 3,Score,62151.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Robotank,,# 2,Score,65.3,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Robotank,,# 17,Score,27.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Robotank,,# 8,Score,62.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Robotank,,# 3,Score,65.1,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Seaquest,,# 21,Score,931.6,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Seaquest,,# 7,Score,16452.7,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Seaquest,,# 3,Score,37361.6,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Seaquest,,# 2,Score,50254.2,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Space Invaders,,# 7,Score,5993.1,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Space Invaders,,# 6,Score,6427.3,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Space Invaders,,# 14,Score,2525.5,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Space Invaders,,# 4,Score,15311.5,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Star Gunner,,# 4,Score,125117.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Star Gunner,,# 5,Score,90804.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Star Gunner,,# 6,Score,89238.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Star Gunner,,# 11,Score,60142.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tennis,,# 6,Score,5.1,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tennis,,# 7,Score,4.4,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tennis,,# 19,Score,-22.8,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tennis,,# 8,Score,0.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Time Pilot,,# 4,Score,11666.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Time Pilot,,# 7,Score,8339.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Time Pilot,,# 12,Score,6601.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Time Pilot,,# 10,Score,7553.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tutankham,,# 3,Score,245.9,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tutankham,,# 4,Score,218.4,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tutankham,,# 6,Score,211.4,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Tutankham,,# 20,Score,48.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Up and Down,,# 6,Score,44939.6,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Up and Down,,# 9,Score,24759.2,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Up and Down,,# 10,Score,22972.2,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Up and Down,,# 7,Score,33879.1,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Venture,,# 8,Score,497.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Venture,,# 12,Score,200.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Venture,,# 15,Score,98.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Venture,,# 20,Score,48.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Video Pinball,,# 9,Score,309941.9,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Video Pinball,,# 16,Score,110976.2,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Video Pinball,,# 4,Score,479197.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Video Pinball,,# 17,Score,98209.5,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Wizard of Wor,,# 9,Score,7492.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Wizard of Wor,,# 4,Score,12352.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Wizard of Wor,,# 10,Score,7054.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Wizard of Wor,,# 8,Score,7855.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Zaxxon,,# 6,Score,12944.0,Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Zaxxon,,# 5,Score,13886.0,Prior+Duel noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Zaxxon,,# 12,Score,10163.0,DDQN (tuned) noop,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
Atari 2600 Zaxxon,,# 11,Score,10164.0,Duel hs,-,Atari Games,Dueling Network Architectures for Deep Reinforcement Learning,/paper/dueling-network-architectures-for-deep,https://arxiv.org/pdf/1511.06581v3.pdf
CIFAR-10 Image Classification,,# 6,Percentage error,2.89,ENAS + c/o,-,Architecture Search,Efficient Neural Architecture Search via Parameter Sharing,/paper/efficient-neural-architecture-search-via-1,https://arxiv.org/pdf/1802.03268v2.pdf
CIFAR-10 Image Classification,,# 1,Params,4.6M,ENAS + c/o,-,Architecture Search,Efficient Neural Architecture Search via Parameter Sharing,/paper/efficient-neural-architecture-search-via-1,https://arxiv.org/pdf/1802.03268v2.pdf
Penn Treebank (Word Level),,# 16,Validation perplexity,60.8,Efficient NAS,-,Language Modelling,Efficient Neural Architecture Search via Parameter Sharing,/paper/efficient-neural-architecture-search-via-1,https://arxiv.org/pdf/1802.03268v2.pdf
Penn Treebank (Word Level),,# 18,Test perplexity,58.6,Efficient NAS,-,Language Modelling,Efficient Neural Architecture Search via Parameter Sharing,/paper/efficient-neural-architecture-search-via-1,https://arxiv.org/pdf/1802.03268v2.pdf
Penn Treebank (Word Level),,# 1,Params,24M,Efficient NAS,-,Language Modelling,Efficient Neural Architecture Search via Parameter Sharing,/paper/efficient-neural-architecture-search-via-1,https://arxiv.org/pdf/1802.03268v2.pdf
CIFAR-10,,# 2,NLL Test,2.92,PixelCNN+,-,Image Generation,PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications,/paper/pixelcnn-improving-the-pixelcnn-with,https://arxiv.org/pdf/1701.05517v1.pdf
Monologue,,# 1,Accuracy,74.10%,bc-LSTM,-,Multimodal Emotion Recognition,Context-Dependent Sentiment Analysis in User-Generated Videos,/paper/context-dependent-sentiment-analysis-in-user,https://aclweb.org/anthology/P17-1081
MOSI,,# 2,Accuracy,80.3%,bc-LSTM,-,Multimodal Sentiment Analysis,Context-Dependent Sentiment Analysis in User-Generated Videos,/paper/context-dependent-sentiment-analysis-in-user,https://aclweb.org/anthology/P17-1081
Quasar,,# 1,EM (Quasar-T),42.2,Denoising QA,-,Open-Domain Question Answering,Denoising Distantly Supervised Open-Domain Question Answering,/paper/denoising-distantly-supervised-open-domain,https://aclweb.org/anthology/P18-1161
Quasar,,# 1,F1 (Quasar-T),49.3,Denoising QA,-,Open-Domain Question Answering,Denoising Distantly Supervised Open-Domain Question Answering,/paper/denoising-distantly-supervised-open-domain,https://aclweb.org/anthology/P18-1161
SearchQA,,# 5,Unigram Acc,-,Denoising QA,-,Open-Domain Question Answering,Denoising Distantly Supervised Open-Domain Question Answering,/paper/denoising-distantly-supervised-open-domain,https://aclweb.org/anthology/P18-1161
SearchQA,,# 6,N-gram F1,-,Denoising QA,-,Open-Domain Question Answering,Denoising Distantly Supervised Open-Domain Question Answering,/paper/denoising-distantly-supervised-open-domain,https://aclweb.org/anthology/P18-1161
SearchQA,,# 1,EM,58.8,Denoising QA,-,Open-Domain Question Answering,Denoising Distantly Supervised Open-Domain Question Answering,/paper/denoising-distantly-supervised-open-domain,https://aclweb.org/anthology/P18-1161
SearchQA,,# 1,F1,64.5,Denoising QA,-,Open-Domain Question Answering,Denoising Distantly Supervised Open-Domain Question Answering,/paper/denoising-distantly-supervised-open-domain,https://aclweb.org/anthology/P18-1161
Caltech Lanes Cordova,,# 1,F1,0.884,VPGNet,-,Lane Detection,VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition,/paper/vpgnet-vanishing-point-guided-network-for,https://arxiv.org/pdf/1710.06288v1.pdf
Caltech Lanes Washington,,# 1,F1,0.8690000000000001,VPGNet,-,Lane Detection,VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition,/paper/vpgnet-vanishing-point-guided-network-for,https://arxiv.org/pdf/1710.06288v1.pdf
QASent,,# 6,MAP,0.5693,Bigram-CNN,-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
QASent,,# 6,MRR,0.6613,Bigram-CNN,-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
QASent,,# 3,MAP,0.7113,Bigram-CNN (lexical overlap + dist output),-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
QASent,,# 3,MRR,0.7846,Bigram-CNN (lexical overlap + dist output),-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
TrecQA,,# 4,MAP,0.711,CNN,-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
TrecQA,,# 4,MRR,0.785,CNN,-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
WikiQA,,# 11,MAP,0.619,Bigram-CNN,-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
WikiQA,,# 12,MRR,0.6281,Bigram-CNN,-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
WikiQA,,# 10,MAP,0.652,Bigram-CNN (lexical overlap + dist output),-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
WikiQA,,# 11,MRR,0.6652,Bigram-CNN (lexical overlap + dist output),-,Question Answering,Deep Learning for Answer Sentence Selection,/paper/deep-learning-for-answer-sentence-selection,https://arxiv.org/pdf/1412.1632v1.pdf
CoNLL 2012,,# 3,Avg F1,67.2,Lee et al.,-,Coreference Resolution,End-to-end Neural Coreference Resolution,/paper/end-to-end-neural-coreference-resolution,https://arxiv.org/pdf/1707.07045v2.pdf
CIFAR-10,,# 13,Inception score,5.34,ALI,-,Image Generation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
Cityscapes Labels-to-Photo,,# 5,Class IOU,0.02,BiGAN,-,Image-to-Image Translation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
Cityscapes Labels-to-Photo,,# 4,Per-class Accuracy,6%,BiGAN,-,Image-to-Image Translation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
Cityscapes Labels-to-Photo,,# 9,Per-pixel Accuracy,19%,BiGAN,-,Image-to-Image Translation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
Cityscapes Photo-to-Labels,,# 5,Per-pixel Accuracy,41%,BiGAN,-,Image-to-Image Translation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
Cityscapes Photo-to-Labels,,# 3,Per-class Accuracy,13%,BiGAN,-,Image-to-Image Translation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
Cityscapes Photo-to-Labels,,# 4,Class IOU,0.07,BiGAN,-,Image-to-Image Translation,Adversarially Learned Inference,/paper/adversarially-learned-inference,https://arxiv.org/pdf/1606.00704v3.pdf
WIDER Face (Easy),,# 14,AP,0.657,Two-stage CNN,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Easy),,# 11,AP,0.716,Faceness-WIDER,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Easy),,# 12,AP,0.711,Multiscale Cascade CNN,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Hard),,# 15,AP,0.304,Two-stage CNN,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Hard),,# 13,AP,0.4,Multiscale Cascade CNN,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Hard),,# 14,AP,0.315,Faceness-WIDER,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Medium),,# 12,AP,0.604,Faceness-WIDER,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Medium),,# 13,AP,0.589,Two-stage CNN,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
WIDER Face (Medium),,# 11,AP,0.636,Multiscale Cascade CNN,-,Face Detection,WIDER FACE: A Face Detection Benchmark,/paper/wider-face-a-face-detection-benchmark,https://arxiv.org/pdf/1511.06523v1.pdf
spider,,# 1,Accuracy,19.7,Exact Set Matching,-,Semantic Parsing,Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task,/paper/spider-a-large-scale-human-labeled-dataset,https://arxiv.org/pdf/1809.08887v5.pdf
SemEval 2013 Task 12,,# 6,F1,67.2,GASext (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2013 Task 12,,# 9,F1,66.7,GAS (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2013 Task 12,,# 8,F1,67.0,GAS (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2013 Task 12,,# 7,F1,67.1,GASext (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2015 Task 13,,# 3,F1,72.1,GASext (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2015 Task 13,,# 2,F1,72.6,GASext (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2015 Task 13,,# 5,F1,71.6,GAS (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SemEval 2015 Task 13,,# 4,F1,71.8,GAS (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 2,,# 9,F1,72.0,GAS (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 2,,# 8,F1,72.1,GAS (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 2,,# 6,F1,72.4,GASext (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 2,,# 7,F1,72.2,GASext (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 3 Task 1,,# 4,F1,70.5,GASext (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 3 Task 1,,# 7,F1,70.1,GASext (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 3 Task 1,,# 5,F1,70.2,GAS (Concatenation),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
SensEval 3 Task 1,,# 8,F1,70.0,GAS (Linear),-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense,https://aclweb.org/anthology/P18-1230
Supervised:,,# 5,Senseval 2,72.0,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 6,Senseval 3,70.0,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 7,SemEval 2007,--*,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 6,SemEval 2013,66.7,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 5,SemEval 2015,71.6,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 6,Senseval 2,72.1,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 9,Senseval 3,70.2,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 7,SemEval 2007,--*,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 9,SemEval 2013,67,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 6,SemEval 2015,71.8,GAS,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 8,Senseval 2,72.4,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 7,Senseval 3,70.1,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 7,SemEval 2007,--*,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 10,SemEval 2013,67.1,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 8,SemEval 2015,72.1,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 7,Senseval 2,72.2,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 10,Senseval 3,70.5,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 7,SemEval 2007,--*,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 11,SemEval 2013,67.2,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
Supervised:,,# 10,SemEval 2015,72.6,GAS<sub>ext</sub>,-,Word Sense Disambiguation,Incorporating Glosses into Neural Word Sense Disambiguation,/paper/incorporating-glosses-into-neural-word-sense-1,https://aclweb.org/anthology/P18-1230
CoNLL 2003 (English),,# 2,F1,93.09,Flair embeddings,-,Named Entity Recognition (NER),Contextual String Embeddings for Sequence Labeling,/paper/contextual-string-embeddings-for-sequence,https://aclweb.org/anthology/C18-1139
Long-tail emerging entities,,# 1,F1,50.2,Flair embeddings,-,Named Entity Recognition (NER),Contextual String Embeddings for Sequence Labeling,/paper/contextual-string-embeddings-for-sequence,https://aclweb.org/anthology/C18-1139
Ontonotes v5 (English),,# 1,F1,89.3,Flair embeddings,-,Named Entity Recognition (NER),Contextual String Embeddings for Sequence Labeling,/paper/contextual-string-embeddings-for-sequence,https://aclweb.org/anthology/C18-1139
Penn Treebank,,# 2,Accuracy,97.85,Flair embeddings,-,Part-Of-Speech Tagging,Contextual String Embeddings for Sequence Labeling,/paper/contextual-string-embeddings-for-sequence,https://aclweb.org/anthology/C18-1139
Penn Treebank,,# 1,F1 score,96.72,Flair embeddings,-,Chunking,Contextual String Embeddings for Sequence Labeling,/paper/contextual-string-embeddings-for-sequence,https://aclweb.org/anthology/C18-1139
COCO,,# 1,FID,33.35,AttnGAN + OP,-,Text-to-Image Generation,Generating Multiple Objects at Spatially Distinct Locations,/paper/generating-multiple-objects-at-spatially,https://arxiv.org/pdf/1901.00686v1.pdf
COCO,,# 2,Inception score,24.76,AttnGAN + OP,-,Text-to-Image Generation,Generating Multiple Objects at Spatially Distinct Locations,/paper/generating-multiple-objects-at-spatially,https://arxiv.org/pdf/1901.00686v1.pdf
COCO,,# 2,FID,55.3,StackGAN + OP,-,Text-to-Image Generation,Generating Multiple Objects at Spatially Distinct Locations,/paper/generating-multiple-objects-at-spatially,https://arxiv.org/pdf/1901.00686v1.pdf
COCO,,# 3,Inception score,12.12,StackGAN + OP,-,Text-to-Image Generation,Generating Multiple Objects at Spatially Distinct Locations,/paper/generating-multiple-objects-at-spatially,https://arxiv.org/pdf/1901.00686v1.pdf
MS-COCO,,# 1,Inception score,24.76,AttnGAN+OP,-,Text-to-Image Generation,Generating Multiple Objects at Spatially Distinct Locations,/paper/generating-multiple-objects-at-spatially,https://arxiv.org/pdf/1901.00686v1.pdf
MS-COCO,,# 2,Inception score,12.12,StackGAN+OP,-,Text-to-Image Generation,Generating Multiple Objects at Spatially Distinct Locations,/paper/generating-multiple-objects-at-spatially,https://arxiv.org/pdf/1901.00686v1.pdf
WMT2016 English-German,,# 5,BLEU score,9.64,Unsupervised S2S with attention,-,Machine Translation,Unsupervised Machine Translation Using Monolingual Corpora Only,/paper/unsupervised-machine-translation-using,https://arxiv.org/pdf/1711.00043v2.pdf
WMT2016 German-English,,# 5,BLEU score,13.33,Unsupervised S2S with attention,-,Machine Translation,Unsupervised Machine Translation Using Monolingual Corpora Only,/paper/unsupervised-machine-translation-using,https://arxiv.org/pdf/1711.00043v2.pdf
SNLI,,# 13,% Test Accuracy,88.5,450D DR-BiLSTM,-,Natural Language Inference,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,/paper/dr-bilstm-dependent-reading-bidirectional,https://arxiv.org/pdf/1802.05577v2.pdf
SNLI,,# 13,% Train Accuracy,94.1,450D DR-BiLSTM,-,Natural Language Inference,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,/paper/dr-bilstm-dependent-reading-bidirectional,https://arxiv.org/pdf/1802.05577v2.pdf
SNLI,,# 1,Parameters,7.5m,450D DR-BiLSTM,-,Natural Language Inference,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,/paper/dr-bilstm-dependent-reading-bidirectional,https://arxiv.org/pdf/1802.05577v2.pdf
SNLI,,# 7,% Test Accuracy,89.3,450D DR-BiLSTM Ensemble,-,Natural Language Inference,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,/paper/dr-bilstm-dependent-reading-bidirectional,https://arxiv.org/pdf/1802.05577v2.pdf
SNLI,,# 11,% Train Accuracy,94.8,450D DR-BiLSTM Ensemble,-,Natural Language Inference,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,/paper/dr-bilstm-dependent-reading-bidirectional,https://arxiv.org/pdf/1802.05577v2.pdf
SNLI,,# 1,Parameters,45m,450D DR-BiLSTM Ensemble,-,Natural Language Inference,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,/paper/dr-bilstm-dependent-reading-bidirectional,https://arxiv.org/pdf/1802.05577v2.pdf
IEMOCAP,,# 1,F1,64.5%,DialogueRNN,-,Emotion Recognition in Conversation,DialogueRNN: An Attentive RNN for Emotion Detection in Conversations,/paper/dialoguernn-an-attentive-rnn-for-emotion,https://arxiv.org/pdf/1811.00405v3.pdf
ImageNet 128x128,,# 4,FID,18.65,Self-attention,-,Conditional Image Generation,Self-Attention Generative Adversarial Networks,/paper/self-attention-generative-adversarial,https://arxiv.org/pdf/1805.08318v1.pdf
ImageNet 128x128,,# 4,Inception score,52.52,Self-attention,-,Conditional Image Generation,Self-Attention Generative Adversarial Networks,/paper/self-attention-generative-adversarial,https://arxiv.org/pdf/1805.08318v1.pdf
Penn Treebank,,# 4,Accuracy,97.59,Adversarial Bi-LSTM,-,Part-Of-Speech Tagging,Robust Multilingual Part-of-Speech Tagging via Adversarial Training,/paper/robust-multilingual-part-of-speech-tagging,https://arxiv.org/pdf/1711.04903v2.pdf
UD,,# 1,Avg accuracy,96.73,Adversarial Bi-LSTM,-,Part-Of-Speech Tagging,Robust Multilingual Part-of-Speech Tagging via Adversarial Training,/paper/robust-multilingual-part-of-speech-tagging,https://arxiv.org/pdf/1711.04903v2.pdf
Supervised:,,# 5,Senseval 2,72.0,Bi-LSTM<sub>att+LEX</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 4,Senseval 3,69.4,Bi-LSTM<sub>att+LEX</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 7,SemEval 2007,63.7*,Bi-LSTM<sub>att+LEX</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 5,SemEval 2013,66.4,Bi-LSTM<sub>att+LEX</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 9,SemEval 2015,72.4,Bi-LSTM<sub>att+LEX</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 5,Senseval 2,72.0,Bi-LSTM<sub>att+LEX+POS</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 3,Senseval 3,69.1,Bi-LSTM<sub>att+LEX+POS</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 7,SemEval 2007,64.8*,Bi-LSTM<sub>att+LEX+POS</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 8,SemEval 2013,66.9,Bi-LSTM<sub>att+LEX+POS</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
Supervised:,,# 4,SemEval 2015,71.5,Bi-LSTM<sub>att+LEX+POS</sub>,-,Word Sense Disambiguation,Neural Sequence Learning Models for Word Sense Disambiguation,/paper/neural-sequence-learning-models-for-word,https://aclweb.org/anthology/D17-1120
CIFAR-10,,# 2,Model Entropy,4.1,DRAW,-,Image Generation,DRAW: A Recurrent Neural Network For Image Generation,/paper/draw-a-recurrent-neural-network-for-image,https://arxiv.org/pdf/1502.04623v2.pdf
MNIST,,# 8,Percentage error,0.8,Sparse Activity and Sparse Connectivity in Supervised Learning,-,Image Classification,Sparse Activity and Sparse Connectivity in Supervised Learning,/paper/sparse-activity-and-sparse-connectivity-in,https://arxiv.org/pdf/1603.08367v1.pdf
QASent,,# 7,MAP,0.5213,Paragraph vector,-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
QASent,,# 7,MRR,0.6023,Paragraph vector,-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
QASent,,# 4,MAP,0.6762,Paragraph vector (lexical overlap + dist output),-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
QASent,,# 4,MRR,0.7514,Paragraph vector (lexical overlap + dist output),-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
WikiQA,,# 12,MAP,0.5976,Paragraph vector (lexical overlap + dist output),-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
WikiQA,,# 13,MRR,0.6058,Paragraph vector (lexical overlap + dist output),-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
WikiQA,,# 13,MAP,0.511,Paragraph vector,-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
WikiQA,,# 14,MRR,0.516,Paragraph vector,-,Question Answering,Distributed Representations of Sentences and Documents,/paper/distributed-representations-of-sentences-and,https://arxiv.org/pdf/1405.4053v2.pdf
SNLI,,# 42,% Test Accuracy,83.2,300D SPINN-PI encoders,-,Natural Language Inference,A Fast Unified Model for Parsing and Sentence Understanding,/paper/a-fast-unified-model-for-parsing-and-sentence,https://arxiv.org/pdf/1603.06021v3.pdf
SNLI,,# 36,% Train Accuracy,89.2,300D SPINN-PI encoders,-,Natural Language Inference,A Fast Unified Model for Parsing and Sentence Understanding,/paper/a-fast-unified-model-for-parsing-and-sentence,https://arxiv.org/pdf/1603.06021v3.pdf
SNLI,,# 1,Parameters,3.7m,300D SPINN-PI encoders,-,Natural Language Inference,A Fast Unified Model for Parsing and Sentence Understanding,/paper/a-fast-unified-model-for-parsing-and-sentence,https://arxiv.org/pdf/1603.06021v3.pdf
SNLI,,# 45,% Test Accuracy,80.6,300D LSTM encoders,-,Natural Language Inference,A Fast Unified Model for Parsing and Sentence Understanding,/paper/a-fast-unified-model-for-parsing-and-sentence,https://arxiv.org/pdf/1603.06021v3.pdf
SNLI,,# 51,% Train Accuracy,83.9,300D LSTM encoders,-,Natural Language Inference,A Fast Unified Model for Parsing and Sentence Understanding,/paper/a-fast-unified-model-for-parsing-and-sentence,https://arxiv.org/pdf/1603.06021v3.pdf
SNLI,,# 1,Parameters,3.0m,300D LSTM encoders,-,Natural Language Inference,A Fast Unified Model for Parsing and Sentence Understanding,/paper/a-fast-unified-model-for-parsing-and-sentence,https://arxiv.org/pdf/1603.06021v3.pdf
SST-2 Binary classification,,# 21,Accuracy,81.2,Emo2Vec,-,Sentiment Analysis,Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training,/paper/emo2vec-learning-generalized-emotion,https://arxiv.org/pdf/1809.04505v1.pdf
SST-2 Binary classification,,# 20,Accuracy,82.3,GloVe+Emo2Vec,-,Sentiment Analysis,Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training,/paper/emo2vec-learning-generalized-emotion,https://arxiv.org/pdf/1809.04505v1.pdf
SST-5 Fine-grained classification,,# 17,Accuracy,41.6,Emo2Vec,-,Sentiment Analysis,Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training,/paper/emo2vec-learning-generalized-emotion,https://arxiv.org/pdf/1809.04505v1.pdf
SST-5 Fine-grained classification,,# 16,Accuracy,43.6,GloVe+Emo2Vec,-,Sentiment Analysis,Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training,/paper/emo2vec-learning-generalized-emotion,https://arxiv.org/pdf/1809.04505v1.pdf
YCB-Video,,# 3,Mean AUC,83.9%,PointFusion,-,6D Pose Estimation,PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation,/paper/pointfusion-deep-sensor-fusion-for-3d,https://arxiv.org/pdf/1711.10871v2.pdf
CINIC-10,,# 4,Accuracy,87.77,VGG-16,-,Image Classification,CINIC-10 is not ImageNet or CIFAR-10,/paper/cinic-10-is-not-imagenet-or-cifar-10,https://arxiv.org/pdf/1810.03505v1.pdf
CINIC-10,,# 3,Accuracy,90.27,ResNet-18,-,Image Classification,CINIC-10 is not ImageNet or CIFAR-10,/paper/cinic-10-is-not-imagenet-or-cifar-10,https://arxiv.org/pdf/1810.03505v1.pdf
CINIC-10,,# 2,Accuracy,91.26,DenseNet-121,-,Image Classification,CINIC-10 is not ImageNet or CIFAR-10,/paper/cinic-10-is-not-imagenet-or-cifar-10,https://arxiv.org/pdf/1810.03505v1.pdf
CINIC-10,,# 1,Accuracy,91.45,ResNeXt29_2x64d,-,Image Classification,CINIC-10 is not ImageNet or CIFAR-10,/paper/cinic-10-is-not-imagenet-or-cifar-10,https://arxiv.org/pdf/1810.03505v1.pdf
DukeMTMC-reID,,# 6,Rank-1,79.3,SVDNet + Random Erasing,-,Person Re-Identification,Random Erasing Data Augmentation,/paper/random-erasing-data-augmentation,https://arxiv.org/pdf/1708.04896v2.pdf
DukeMTMC-reID,,# 5,MAP,62.4,SVDNet + Random Erasing,-,Person Re-Identification,Random Erasing Data Augmentation,/paper/random-erasing-data-augmentation,https://arxiv.org/pdf/1708.04896v2.pdf
DukeMTMC-reID,,# 10,Rank-1,73.0,TriNet + Random Erasing,-,Person Re-Identification,Random Erasing Data Augmentation,/paper/random-erasing-data-augmentation,https://arxiv.org/pdf/1708.04896v2.pdf
DukeMTMC-reID,,# 9,MAP,56.6,TriNet + Random Erasing,-,Person Re-Identification,Random Erasing Data Augmentation,/paper/random-erasing-data-augmentation,https://arxiv.org/pdf/1708.04896v2.pdf
Fashion-MNIST,,# 1,Percentage error,3.65,Random Erasing,-,Image Classification,Random Erasing Data Augmentation,/paper/random-erasing-data-augmentation,https://arxiv.org/pdf/1708.04896v2.pdf
PASCAL VOC 2007,,# 11,MAP,76.2%,I+ORE,-,Object Detection,Random Erasing Data Augmentation,/paper/random-erasing-data-augmentation,https://arxiv.org/pdf/1708.04896v2.pdf
BSD100 - 4x upscaling,,# 20,PSNR,27.25,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
BSD100 - 4x upscaling,,# 24,SSIM,0.7240000000000001,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
Set14 - 4x upscaling,,# 21,PSNR,28.07,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
Set14 - 4x upscaling,,# 23,SSIM,0.77,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
Set5 - 4x upscaling,,# 19,PSNR,31.4,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
Set5 - 4x upscaling,,# 22,SSIM,0.883,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
Urban100 - 4x upscaling,,# 21,PSNR,25.08,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
Urban100 - 4x upscaling,,# 20,SSIM,0.747,DSRN,-,Image Super-Resolution,Image Super-Resolution via Dual-State Recurrent Networks,/paper/image-super-resolution-via-dual-state,https://arxiv.org/pdf/1805.02704v1.pdf
COCO,,# 16,Bounding Box AP,42.6,RetinaMask + ResNeXt-101-FPN-GN,-,Object Detection,RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free,/paper/retinamask-learning-to-predict-masks-improves,https://arxiv.org/pdf/1901.03353v1.pdf
PASCAL VOC 2007,,# 6,MAP,80.5%,R-FCN,-,Object Detection,R-FCN: Object Detection via Region-based Fully Convolutional Networks,/paper/r-fcn-object-detection-via-region-based-fully,https://arxiv.org/pdf/1605.06409v2.pdf
PASCAL VOC 2007,,# 2,MAP,80.5%,R-FCN,-,Real-Time Object Detection,R-FCN: Object Detection via Region-based Fully Convolutional Networks,/paper/r-fcn-object-detection-via-region-based-fully,https://arxiv.org/pdf/1605.06409v2.pdf
PASCAL VOC 2007,,# 5,FPS,9,R-FCN,-,Real-Time Object Detection,R-FCN: Object Detection via Region-based Fully Convolutional Networks,/paper/r-fcn-object-detection-via-region-based-fully,https://arxiv.org/pdf/1605.06409v2.pdf
ImageNet,,# 1,MAP,19.6,PCL-OB-G-Ens + FRCNN,-,Weakly Supervised Object Detection,PCL: Proposal Cluster Learning for Weakly Supervised Object Detection,/paper/pcl-proposal-cluster-learning-for-weakly,https://arxiv.org/pdf/1807.03342v2.pdf
PASCAL VOC 2007,,# 4,MAP,48.8,PCL-OB-G-Ens + FRCNN,-,Weakly Supervised Object Detection,PCL: Proposal Cluster Learning for Weakly Supervised Object Detection,/paper/pcl-proposal-cluster-learning-for-weakly,https://arxiv.org/pdf/1807.03342v2.pdf
PASCAL VOC 2012,,# 3,MAP,44.2,PCL-OB-G-Ens + FRCNN,-,Weakly Supervised Object Detection,PCL: Proposal Cluster Learning for Weakly Supervised Object Detection,/paper/pcl-proposal-cluster-learning-for-weakly,https://arxiv.org/pdf/1807.03342v2.pdf
Florence,,# 4,Average 3D Error,1.93,3DMM-CNN,-,3D Face Reconstruction,Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network,/paper/regressing-robust-and-discriminative-3d,https://arxiv.org/pdf/1612.04904v1.pdf
Labeled Faces in the Wild,,# 12,Accuracy,92.35%,3DMM face shape parameters + CNN,-,Face Verification,Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network,/paper/regressing-robust-and-discriminative-3d,https://arxiv.org/pdf/1612.04904v1.pdf
YouTube Faces DB,,# 10,Accuracy,88.80%,3DMM face shape parameters + CNN,-,Face Verification,Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network,/paper/regressing-robust-and-discriminative-3d,https://arxiv.org/pdf/1612.04904v1.pdf
TIMIT,,# 9,Percentage error,16.7,Light Gated Recurrent Units,-,Speech Recognition,Light Gated Recurrent Units for Speech Recognition,/paper/light-gated-recurrent-units-for-speech,https://arxiv.org/pdf/1803.10225v1.pdf
TIMIT,,# 3,Percentage error,14.9,Li-GRU + fMLLR features,-,Speech Recognition,Light Gated Recurrent Units for Speech Recognition,/paper/light-gated-recurrent-units-for-speech,https://arxiv.org/pdf/1803.10225v1.pdf
SemEvalCQA,,# 3,[emailÂ protected],0.753,ARC-II,-,Question Answering,Convolutional Neural Network Architectures for Matching Natural Language Sentences,/paper/convolutional-neural-network-architectures-1,https://arxiv.org/pdf/1503.03244v1.pdf
SemEvalCQA,,# 3,MAP,0.78,ARC-II,-,Question Answering,Convolutional Neural Network Architectures for Matching Natural Language Sentences,/paper/convolutional-neural-network-architectures-1,https://arxiv.org/pdf/1503.03244v1.pdf
COCO Visual Question Answering (VQA) abstract 1.0 multiple choice,,# 4,Percentage correct,61.41,LSTM blind,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) abstract 1.0 multiple choice,,# 3,Percentage correct,69.21,LSTM + global features,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) abstract 1.0 multiple choice,,# 2,Percentage correct,71.18,Dualnet ensemble,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) abstract images 1.0 open ended,,# 3,Percentage correct,65.02,LSTM + global features,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) abstract images 1.0 open ended,,# 2,Percentage correct,69.73,Dualnet ensemble,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) abstract images 1.0 open ended,,# 4,Percentage correct,57.19,LSTM blind,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 6,Percentage correct,63.1,LSTM Q+I,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 9,Percentage correct,58.2,LSTM Q+I,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) real images 2.0 open ended,,# 2,Percentage correct,68.16,HDU-USYD-UNCC,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
COCO Visual Question Answering (VQA) real images 2.0 open ended,,# 3,Percentage correct,68.07,DLAIT,-,Visual Question Answering,VQA: Visual Question Answering,/paper/vqa-visual-question-answering,https://arxiv.org/pdf/1505.00468v7.pdf
CIFAR-10,,# 39,Percentage correct,90.8,Deep Networks with Internal Selective Attention through Feedback Connections,-,Image Classification,Deep Networks with Internal Selective Attention through Feedback Connections,/paper/deep-networks-with-internal-selective-1,https://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf
CIFAR-100,,# 36,Percentage correct,66.2,Deep Networks with Internal Selective Attention through Feedback Connections,-,Image Classification,Deep Networks with Internal Selective Attention through Feedback Connections,/paper/deep-networks-with-internal-selective-1,https://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf
CIFAR-10,,# 47,Percentage correct,88.8,RReLU,-,Image Classification,Empirical Evaluation of Rectified Activations in Convolutional Network,/paper/empirical-evaluation-of-rectified-activations,https://arxiv.org/pdf/1505.00853v2.pdf
CIFAR-100,,# 44,Percentage correct,59.8,RReLU,-,Image Classification,Empirical Evaluation of Rectified Activations in Convolutional Network,/paper/empirical-evaluation-of-rectified-activations,https://arxiv.org/pdf/1505.00853v2.pdf
ContactDB,,# 1,Error rate,8.72,DiverseNet-VoxNet,-,Human Grasp Contact Prediction,ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging,/paper/contactdb-analyzing-and-predicting-grasp,https://arxiv.org/pdf/1904.06830v1.pdf
ContactDB,,# 2,Error rate,17.27,sMCL-VoxNet,-,Human Grasp Contact Prediction,ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging,/paper/contactdb-analyzing-and-predicting-grasp,https://arxiv.org/pdf/1904.06830v1.pdf
ContactDB,,# 3,Error rate,21.82,DiverseNet-PointNet,-,Human Grasp Contact Prediction,ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging,/paper/contactdb-analyzing-and-predicting-grasp,https://arxiv.org/pdf/1904.06830v1.pdf
ContactDB,,# 4,Error rate,29.89,sMCL-PointNet,-,Human Grasp Contact Prediction,ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging,/paper/contactdb-analyzing-and-predicting-grasp,https://arxiv.org/pdf/1904.06830v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 9,Accuracy,43.60%,Matching Nets + C64F feature extractor,-,Few-Shot Image Classification,Matching Networks for One Shot Learning,/paper/matching-networks-for-one-shot-learning,https://arxiv.org/pdf/1606.04080v2.pdf
Mini-ImageNet - 5-Shot Learning,,# 9,Accuracy,55.30%,Matching Nets + C64F feature extractor,-,Few-Shot Image Classification,Matching Networks for One Shot Learning,/paper/matching-networks-for-one-shot-learning,https://arxiv.org/pdf/1606.04080v2.pdf
OMNIGLOT - 1-Shot Learning,,# 4,Accuracy,98.1%,Matching Nets,-,Few-Shot Image Classification,Matching Networks for One Shot Learning,/paper/matching-networks-for-one-shot-learning,https://arxiv.org/pdf/1606.04080v2.pdf
OMNIGLOT - 5-Shot Learning,,# 6,Accuracy,98.9%,Matching Nets,-,Few-Shot Image Classification,Matching Networks for One Shot Learning,/paper/matching-networks-for-one-shot-learning,https://arxiv.org/pdf/1606.04080v2.pdf
General,,# 2,MAP,10.6,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
General,,# 2,MRR,23.83,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
General,,# 2,[emailÂ protected],9.91,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
Medical domain,,# 4,MAP,18.84,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
Medical domain,,# 2,MRR,41.07,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
Medical domain,,# 4,[emailÂ protected],20.71,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
Music domain,,# 4,MAP,12.99,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
Music domain,,# 4,MRR,39.36,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
Music domain,,# 4,[emailÂ protected],12.41,vTE,-,Hypernym Discovery,Supervised Distributional Hypernym Discovery via Domain Adaptation,/paper/supervised-distributional-hypernym-discovery,https://aclweb.org/anthology/D16-1041
PASCAL-Person-Part,,# 2,AP 0.5,40.60%,Holistic instance-level,-,Multi-Human Parsing,"Holistic, Instance-Level Human Parsing",/paper/holistic-instance-level-human-parsing,https://arxiv.org/pdf/1709.03612v1.pdf
COCO Visual Question Answering (VQA) abstract 1.0 multiple choice,,# 1,Percentage correct,74.37,Graph VQA,-,Visual Question Answering,Graph-Structured Representations for Visual Question Answering,/paper/graph-structured-representations-for-visual,https://arxiv.org/pdf/1609.05600v2.pdf
COCO Visual Question Answering (VQA) abstract images 1.0 open ended,,# 1,Percentage correct,70.42,Graph VQA,-,Visual Question Answering,Graph-Structured Representations for Visual Question Answering,/paper/graph-structured-representations-for-visual,https://arxiv.org/pdf/1609.05600v2.pdf
BSD100 - 4x upscaling,,# 26,PSNR,26.9,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
BSD100 - 4x upscaling,,# 26,SSIM,0.7101,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Manga109 - 4x upscaling,,# 7,PSNR,27.58,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Manga109 - 4x upscaling,,# 8,SSIM,0.8555,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Set14 - 4x upscaling,,# 31,PSNR,27.5,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Set14 - 4x upscaling,,# 28,SSIM,0.7513,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Set5 - 4x upscaling,,# 26,PSNR,30.49,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Set5 - 4x upscaling,,# 28,SSIM,0.8628,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Ultra Video Group HD - 4x upscaling,,# 2,Average PSNR,37.52,SRCNN,-,Video Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Urban100 - 4x upscaling,,# 22,PSNR,24.52,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Urban100 - 4x upscaling,,# 21,SSIM,0.7221,SRCNN,-,Image Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Vid4 - 4x upscaling,,# 9,PSNR,24.68,SRCNN,-,Video Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Vid4 - 4x upscaling,,# 3,SSIM,0.7158,SRCNN,-,Video Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Vid4 - 4x upscaling,,# 4,MOVIE,6.9,SRCNN,-,Video Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
Xiph HD - 4x upscaling,,# 2,Average PSNR,31.47,SRCNN,-,Video Super-Resolution,Image Super-Resolution Using Deep Convolutional Networks,/paper/image-super-resolution-using-deep,https://arxiv.org/pdf/1501.00092v3.pdf
WMT2014 English-French,,# 15,BLEU score,37.5,LSTM6 + PosUnk,-,Machine Translation,Addressing the Rare Word Problem in Neural Machine Translation,/paper/addressing-the-rare-word-problem-in-neural,https://arxiv.org/pdf/1410.8206v4.pdf
Penn Treebank,,# 7,F1 score,93.8,Semi-supervised LSTM-LM,-,Constituency Parsing,Parsing as Language Modeling,/paper/parsing-as-language-modeling,https://aclweb.org/anthology/D16-1257
Multi-Domain Sentiment Dataset,,# 5,DVD,75.4,DANN,-,Sentiment Analysis,Domain-Adversarial Training of Neural Networks,/paper/domain-adversarial-training-of-neural,https://arxiv.org/pdf/1505.07818v4.pdf
Multi-Domain Sentiment Dataset,,# 5,Books,71.43,DANN,-,Sentiment Analysis,Domain-Adversarial Training of Neural Networks,/paper/domain-adversarial-training-of-neural,https://arxiv.org/pdf/1505.07818v4.pdf
Multi-Domain Sentiment Dataset,,# 4,Electronics,77.67,DANN,-,Sentiment Analysis,Domain-Adversarial Training of Neural Networks,/paper/domain-adversarial-training-of-neural,https://arxiv.org/pdf/1505.07818v4.pdf
Multi-Domain Sentiment Dataset,,# 5,Kitchen,80.53,DANN,-,Sentiment Analysis,Domain-Adversarial Training of Neural Networks,/paper/domain-adversarial-training-of-neural,https://arxiv.org/pdf/1505.07818v4.pdf
Multi-Domain Sentiment Dataset,,# 5,Average,76.26,DANN,-,Sentiment Analysis,Domain-Adversarial Training of Neural Networks,/paper/domain-adversarial-training-of-neural,https://arxiv.org/pdf/1505.07818v4.pdf
CAT 256x256,,# 1,FID,32.11,RaSGAN,-,Image Generation,The relativistic discriminator: a key element missing from standard GAN,/paper/the-relativistic-discriminator-a-key-element,https://arxiv.org/pdf/1807.00734v3.pdf
CIFAR-10,,# 5,FID,25.6,RSGAN-GP,-,Image Generation,The relativistic discriminator: a key element missing from standard GAN,/paper/the-relativistic-discriminator-a-key-element,https://arxiv.org/pdf/1807.00734v3.pdf
CCGBank,,# 4,Accuracy,93.26,Low supervision,-,CCG Supertagging,Deep multi-task learning with low level tasks supervised at lower layers,/paper/deep-multi-task-learning-with-low-level-tasks,https://aclweb.org/anthology/P16-2038
Penn Treebank,,# 3,F1 score,95.57,Low supervision,-,Chunking,Deep multi-task learning with low level tasks supervised at lower layers,/paper/deep-multi-task-learning-with-low-level-tasks,https://aclweb.org/anthology/P16-2038
VQA v2,,# 1,Accuracy,70.34%,"Image features from bottom-up attention, adaptive K, ensemble",-,Visual Question Answering,Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge,/paper/tips-and-tricks-for-visual-question-answering,https://arxiv.org/pdf/1708.02711v1.pdf
BlogCatalog,,# 3,Accuracy,22.50%,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
BlogCatalog,,# 3,Macro-F1,0.214,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
Citeseer,,# 8,Accuracy,43.2%,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
Cora,,# 9,Accuracy,67.2%,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
Cora,,# 7,Accuracy,67.2%,DeepWalk,-,Document Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
NELL,,# 3,Accuracy,58.1%,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
Pubmed,,# 8,Accuracy,65.3%,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
Wikipedia,,# 3,Accuracy,19.40%,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
Wikipedia,,# 3,Macro-F1,0.183,DeepWalk,-,Node Classification,DeepWalk: Online Learning of Social Representations,/paper/deepwalk-online-learning-of-social,https://arxiv.org/pdf/1403.6652v2.pdf
PROMISE 2012,,# 1,Dice Score,0.8690000000000001,V-Net + Dice-based loss,-,Volumetric Medical Image Segmentation,V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,/paper/v-net-fully-convolutional-neural-networks-for,https://arxiv.org/pdf/1606.04797v1.pdf
FB15k,,# 2,MRR,0.795,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k,,# 2,[emailÂ protected],0.892,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k,,# 1,[emailÂ protected],0.833,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k,,# 1,[emailÂ protected],0.741,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k-237,,# 2,MRR,0.358,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k-237,,# 2,[emailÂ protected],0.544,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k-237,,# 1,[emailÂ protected],0.39399999999999996,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
FB15k-237,,# 1,[emailÂ protected],0.266,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18,,# 2,MRR,0.953,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18,,# 3,[emailÂ protected],0.958,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18,,# 2,[emailÂ protected],0.955,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18,,# 2,[emailÂ protected],0.9490000000000001,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18RR,,# 3,MRR,0.47,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18RR,,# 3,[emailÂ protected],0.526,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18RR,,# 1,[emailÂ protected],0.48200000000000004,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
WN18RR,,# 2,[emailÂ protected],0.44299999999999995,TuckER,-,Link Prediction,TuckER: Tensor Factorization for Knowledge Graph Completion,/paper/tucker-tensor-factorization-for-knowledge,https://arxiv.org/pdf/1901.09590v1.pdf
Sintel-clean,,# 1,Average End-Point Error,3.45,PWC-Net,-,Optical Flow Estimation,"Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation",/paper/models-matter-so-does-training-an-empirical,https://arxiv.org/pdf/1809.05571v1.pdf
Sintel-final,,# 1,Average End-Point Error,4.6,PWC-Net,-,Optical Flow Estimation,"Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation",/paper/models-matter-so-does-training-an-empirical,https://arxiv.org/pdf/1809.05571v1.pdf
Penn Treebank,,# 3,POS,97.3,Distilled neural FOG,-,Dependency Parsing,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,/paper/distilling-an-ensemble-of-greedy-dependency,https://arxiv.org/pdf/1609.07561v1.pdf
Penn Treebank,,# 5,UAS,94.26,Distilled neural FOG,-,Dependency Parsing,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,/paper/distilling-an-ensemble-of-greedy-dependency,https://arxiv.org/pdf/1609.07561v1.pdf
Penn Treebank,,# 5,LAS,92.06,Distilled neural FOG,-,Dependency Parsing,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,/paper/distilling-an-ensemble-of-greedy-dependency,https://arxiv.org/pdf/1609.07561v1.pdf
WIDER Face (Easy),,# 10,AP,0.797,LDCF+,-,Face Detection,To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection,/paper/to-boost-or-not-to-boost-on-the-limits-of,https://arxiv.org/pdf/1701.01692v1.pdf
WIDER Face (Hard),,# 12,AP,0.564,LDCF+,-,Face Detection,To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection,/paper/to-boost-or-not-to-boost-on-the-limits-of,https://arxiv.org/pdf/1701.01692v1.pdf
WIDER Face (Medium),,# 10,AP,0.772,LDCF+,-,Face Detection,To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection,/paper/to-boost-or-not-to-boost-on-the-limits-of,https://arxiv.org/pdf/1701.01692v1.pdf
BSD100 - 4x upscaling,,# 18,PSNR,27.32,LapSRN,-,Image Super-Resolution,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,/paper/deep-laplacian-pyramid-networks-for-fast-and,https://arxiv.org/pdf/1704.03915v2.pdf
Set14 - 4x upscaling,,# 20,PSNR,28.19,LapSR,-,Image Super-Resolution,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,/paper/deep-laplacian-pyramid-networks-for-fast-and,https://arxiv.org/pdf/1704.03915v2.pdf
Urban100 - 4x upscaling,,# 19,PSNR,25.21,LapSRN,-,Image Super-Resolution,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,/paper/deep-laplacian-pyramid-networks-for-fast-and,https://arxiv.org/pdf/1704.03915v2.pdf
Annotated Faces in the Wild,,# 2,AP,0.9985,S3FD,-,Face Detection,S$^3$FD: Single Shot Scale-invariant Face Detector,/paper/s3fd-single-shot-scale-invariant-face,https://arxiv.org/pdf/1708.05237v3.pdf
FDDB,,# 5,AP,0.983,S3FD,-,Face Detection,S$^3$FD: Single Shot Scale-invariant Face Detector,/paper/s3fd-single-shot-scale-invariant-face,https://arxiv.org/pdf/1708.05237v3.pdf
PASCAL Face,,# 3,AP,0.9849,S3FD,-,Face Detection,S$^3$FD: Single Shot Scale-invariant Face Detector,/paper/s3fd-single-shot-scale-invariant-face,https://arxiv.org/pdf/1708.05237v3.pdf
WIDER Face (Easy),,# 6,AP,0.9279999999999999,S3FD,-,Face Detection,S$^3$FD: Single Shot Scale-invariant Face Detector,/paper/s3fd-single-shot-scale-invariant-face,https://arxiv.org/pdf/1708.05237v3.pdf
WIDER Face (Hard),,# 7,AP,0.84,S3FD,-,Face Detection,S$^3$FD: Single Shot Scale-invariant Face Detector,/paper/s3fd-single-shot-scale-invariant-face,https://arxiv.org/pdf/1708.05237v3.pdf
WIDER Face (Medium),,# 6,AP,0.9129999999999999,S3FD,-,Face Detection,S$^3$FD: Single Shot Scale-invariant Face Detector,/paper/s3fd-single-shot-scale-invariant-face,https://arxiv.org/pdf/1708.05237v3.pdf
MNIST,,# 1,Accuracy,95,ProjectionNet,-,Image Classification,ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections,/paper/projectionnet-learning-efficient-on-device,https://arxiv.org/pdf/1708.00630v2.pdf
DukeMTMC-reID,,# 17,Rank-1,65.22,IDE,-,Person Re-Identification,"Person Re-identification: Past, Present and Future",/paper/person-re-identification-past-present-and,https://arxiv.org/pdf/1610.02984v1.pdf
DukeMTMC-reID,,# 17,MAP,44.99,IDE,-,Person Re-Identification,"Person Re-identification: Past, Present and Future",/paper/person-re-identification-past-present-and,https://arxiv.org/pdf/1610.02984v1.pdf
Market-1501,,# 22,Rank-1,72.54,IDE,-,Person Re-Identification,"Person Re-identification: Past, Present and Future",/paper/person-re-identification-past-present-and,https://arxiv.org/pdf/1610.02984v1.pdf
Market-1501,,# 22,MAP,46.0,IDE,-,Person Re-Identification,"Person Re-identification: Past, Present and Future",/paper/person-re-identification-past-present-and,https://arxiv.org/pdf/1610.02984v1.pdf
Leeds Sports Poses,,# 1,PCK,93.9%,Pyramid Residual Modules (PRMs),-,Pose Estimation,Learning Feature Pyramids for Human Pose Estimation,/paper/learning-feature-pyramids-for-human-pose,https://arxiv.org/pdf/1708.01101v1.pdf
MPII Human Pose,,# 2,PCKh-0.5,92.0%,Pyramid Residual Modules (PRMs),-,Pose Estimation,Learning Feature Pyramids for Human Pose Estimation,/paper/learning-feature-pyramids-for-human-pose,https://arxiv.org/pdf/1708.01101v1.pdf
TCIA Pancreas-CT,,# 1,Dice Score,81.3,Holistic-nested CNN,-,3D Medical Imaging Segmentation,Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation,/paper/spatial-aggregation-of-holistically-nested-1,https://arxiv.org/pdf/1702.00045v1.pdf
AG News,,# 12,Error,7.9,Balanced+bi-leaf-RNN,-,Text Classification,On Tree-Based Neural Sentence Modeling,/paper/on-tree-based-neural-sentence-modeling,https://arxiv.org/pdf/1808.09644v1.pdf
Amazon Review Full,,# 9,Accuracy,49.7,Gumbel+bi-leaf-RNN,-,Sentiment Analysis,On Tree-Based Neural Sentence Modeling,/paper/on-tree-based-neural-sentence-modeling,https://arxiv.org/pdf/1808.09644v1.pdf
Amazon Review Polarity,,# 9,Accuracy,88.1,Gumbel+bi-leaf-RNN,-,Sentiment Analysis,On Tree-Based Neural Sentence Modeling,/paper/on-tree-based-neural-sentence-modeling,https://arxiv.org/pdf/1808.09644v1.pdf
DBpedia,,# 11,Error,1.2,Balanced+bi-leaf-RNN,-,Text Classification,On Tree-Based Neural Sentence Modeling,/paper/on-tree-based-neural-sentence-modeling,https://arxiv.org/pdf/1808.09644v1.pdf
COCO,,# 3,MAP,47.9,Deep Feature Maps,-,Weakly Supervised Object Detection,Weakly Supervised Localization using Deep Feature Maps,/paper/weakly-supervised-localization-using-deep,https://arxiv.org/pdf/1603.00489v2.pdf
CUB-200 - 0-Shot Learning,,# 3,Accuracy,44.3%,Sample Clustering,-,Few-Shot Image Classification,Learning Deep Parsimonious Representations,/paper/learning-deep-parsimonious-representations,https://papers.nips.cc/paper/6263-learning-deep-parsimonious-representations.pdf
CNN / Daily Mail,,# 4,CNN,76.9,BiDAF,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
CNN / Daily Mail,,# 2,Daily Mail,79.6,BiDAF,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
MS MARCO,,# 4,Rouge-L,23.96,BiDaF Baseline,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
MS MARCO,,# 4,BLEU-1,10.64,BiDaF Baseline,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
NarrativeQA,,# 5,BLEU-1,33.45,BiDAF,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
NarrativeQA,,# 5,BLEU-4,15.69,BiDAF,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
NarrativeQA,,# 5,METEOR,15.68,BiDAF,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
NarrativeQA,,# 5,Rouge-L,36.74,BiDAF,-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
Quasar,,# 5,EM (Quasar-T),25.9,BiDAF,-,Open-Domain Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
Quasar,,# 4,F1 (Quasar-T),28.5,BiDAF,-,Open-Domain Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
SQuAD1.1,,# 118,EM,67.97399999999999,BiDAF (single model),-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
SQuAD1.1,,# 120,F1,77.32300000000001,BiDAF (single model),-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
SQuAD1.1,,# 83,EM,73.744,BiDAF (ensemble),-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
SQuAD1.1,,# 87,F1,81.525,BiDAF (ensemble),-,Question Answering,Bidirectional Attention Flow for Machine Comprehension,/paper/bidirectional-attention-flow-for-machine,https://arxiv.org/pdf/1611.01603v6.pdf
Second dialogue state tracking challenge,,# 4,Request,-,Liu et al.,-,Dialogue State Tracking,Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems,/paper/dialogue-learning-with-human-teaching-and,https://arxiv.org/pdf/1804.06512v1.pdf
Second dialogue state tracking challenge,,# 2,Area,90,Liu et al.,-,Dialogue State Tracking,Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems,/paper/dialogue-learning-with-human-teaching-and,https://arxiv.org/pdf/1804.06512v1.pdf
Second dialogue state tracking challenge,,# 2,Food,84,Liu et al.,-,Dialogue State Tracking,Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems,/paper/dialogue-learning-with-human-teaching-and,https://arxiv.org/pdf/1804.06512v1.pdf
Second dialogue state tracking challenge,,# 2,Price,92,Liu et al.,-,Dialogue State Tracking,Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems,/paper/dialogue-learning-with-human-teaching-and,https://arxiv.org/pdf/1804.06512v1.pdf
Second dialogue state tracking challenge,,# 4,Joint,72,Liu et al.,-,Dialogue State Tracking,Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems,/paper/dialogue-learning-with-human-teaching-and,https://arxiv.org/pdf/1804.06512v1.pdf
CR,,# 3,Accuracy,86.5,SuBiLSTM-Tied,-,Sentiment Analysis,Improved Sentence Modeling using Suffix Bidirectional LSTM,/paper/improved-sentence-modeling-using-suffix,https://arxiv.org/pdf/1805.07340v2.pdf
MR,,# 4,Accuracy,81.6,SuBiLSTM-Tied,-,Sentiment Analysis,Improved Sentence Modeling using Suffix Bidirectional LSTM,/paper/improved-sentence-modeling-using-suffix,https://arxiv.org/pdf/1805.07340v2.pdf
SST-2 Binary classification,,# 6,Accuracy,91.2,Suffix BiLSTM,-,Sentiment Analysis,Improved Sentence Modeling using Suffix Bidirectional LSTM,/paper/improved-sentence-modeling-using-suffix,https://arxiv.org/pdf/1805.07340v2.pdf
SST-5 Fine-grained classification,,# 2,Accuracy,56.2,Suffix BiLSTM,-,Sentiment Analysis,Improved Sentence Modeling using Suffix Bidirectional LSTM,/paper/improved-sentence-modeling-using-suffix,https://arxiv.org/pdf/1805.07340v2.pdf
YCB-Video,,# 1,Mean AUC,93.1%,DenseFusion,-,6D Pose Estimation,DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion,/paper/densefusion-6d-object-pose-estimation-by,https://arxiv.org/pdf/1901.04780v1.pdf
AFAD,,# 1,MAE,3.48,CORAL,-,Age Estimation,Consistent Rank Logits for Ordinal Regression with Convolutional Neural Networks,/paper/consistent-rank-logits-for-ordinal-regression,https://arxiv.org/pdf/1901.07884v2.pdf
CACD,,# 1,MAE,5.35,CORAL,-,Age Estimation,Consistent Rank Logits for Ordinal Regression with Convolutional Neural Networks,/paper/consistent-rank-logits-for-ordinal-regression,https://arxiv.org/pdf/1901.07884v2.pdf
MORPH Album2,,# 1,MAE,2.59,CORAL,-,Age Estimation,Consistent Rank Logits for Ordinal Regression with Convolutional Neural Networks,/paper/consistent-rank-logits-for-ordinal-regression,https://arxiv.org/pdf/1901.07884v2.pdf
UTKFace,,# 1,MAE,5.39,CORAL,-,Age Estimation,Consistent Rank Logits for Ordinal Regression with Convolutional Neural Networks,/paper/consistent-rank-logits-for-ordinal-regression,https://arxiv.org/pdf/1901.07884v2.pdf
Annotated Faces in the Wild,,# 6,AP,0.9891,FaceBoxes,-,Face Detection,FaceBoxes: A CPU Real-time Face Detector with High Accuracy,/paper/faceboxes-a-cpu-real-time-face-detector-with,https://arxiv.org/pdf/1708.05234v4.pdf
FDDB,,# 6,AP,0.96,FaceBoxes,-,Face Detection,FaceBoxes: A CPU Real-time Face Detector with High Accuracy,/paper/faceboxes-a-cpu-real-time-face-detector-with,https://arxiv.org/pdf/1708.05234v4.pdf
PASCAL Face,,# 4,AP,0.963,FaceBoxes,-,Face Detection,FaceBoxes: A CPU Real-time Face Detector with High Accuracy,/paper/faceboxes-a-cpu-real-time-face-detector-with,https://arxiv.org/pdf/1708.05234v4.pdf
Quora Question Pairs,,# 2,Accuracy,89.06,DIIN,-,Paraphrase Identification,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
SNLI,,# 16,% Test Accuracy,88.0,"448D Densely Interactive Inference Network (DIIN, code)",-,Natural Language Inference,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
SNLI,,# 26,% Train Accuracy,91.2,"448D Densely Interactive Inference Network (DIIN, code)",-,Natural Language Inference,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
SNLI,,# 1,Parameters,4.4m,"448D Densely Interactive Inference Network (DIIN, code)",-,Natural Language Inference,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
SNLI,,# 9,% Test Accuracy,88.9,"448D Densely Interactive Inference Network (DIIN, code) Ensemble",-,Natural Language Inference,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
SNLI,,# 21,% Train Accuracy,92.3,"448D Densely Interactive Inference Network (DIIN, code) Ensemble",-,Natural Language Inference,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
SNLI,,# 1,Parameters,17m,"448D Densely Interactive Inference Network (DIIN, code) Ensemble",-,Natural Language Inference,Natural Language Inference over Interaction Space,/paper/natural-language-inference-over-interaction-1,https://arxiv.org/pdf/1709.04348v2.pdf
Market-1501,,# 14,Rank-1,84.14,PDF,-,Person Re-Identification,Pose-driven Deep Convolutional Model for Person Re-identification,/paper/pose-driven-deep-convolutional-model-for,https://arxiv.org/pdf/1709.08325v1.pdf
Market-1501,,# 17,MAP,63.41,PDF,-,Person Re-Identification,Pose-driven Deep Convolutional Model for Person Re-identification,/paper/pose-driven-deep-convolutional-model-for,https://arxiv.org/pdf/1709.08325v1.pdf
IJB-A,,# 4,TAR @ FAR=0.01,94.40%,Deep Residual Equivariant Mapping,-,Face Verification,Pose-Robust Face Recognition via Deep Residual Equivariant Mapping,/paper/pose-robust-face-recognition-via-deep,https://arxiv.org/pdf/1803.00839v1.pdf
IJB-A,,# 1,Accuracy,94.60%,Deep Residual Equivariant Mapping,-,Face Identification,Pose-Robust Face Recognition via Deep Residual Equivariant Mapping,/paper/pose-robust-face-recognition-via-deep,https://arxiv.org/pdf/1803.00839v1.pdf
Vid4 - 4x upscaling,,# 4,PSNR,26.01,SOF-VSR,-,Video Super-Resolution,Learning for Video Super-Resolution through HR Optical Flow Estimation,/paper/learning-for-video-super-resolution-through,https://arxiv.org/pdf/1809.08573v2.pdf
Vid4 - 4x upscaling,,# 7,SSIM,0.7709999999999999,SOF-VSR,-,Video Super-Resolution,Learning for Video Super-Resolution through HR Optical Flow Estimation,/paper/learning-for-video-super-resolution-through,https://arxiv.org/pdf/1809.08573v2.pdf
Vid4 - 4x upscaling,,# 1,MOVIE,4.32,SOF-VSR,-,Video Super-Resolution,Learning for Video Super-Resolution through HR Optical Flow Estimation,/paper/learning-for-video-super-resolution-through,https://arxiv.org/pdf/1809.08573v2.pdf
ShapeNet,,# 2,Mean IoU,84.6%,PointNet++,-,Semantic Segmentation,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,/paper/pointnet-deep-hierarchical-feature-learning,https://arxiv.org/pdf/1706.02413v1.pdf
ShapeNet-Part,,# 4,Class Average IoU,81.9,PointNet++,-,3D Part Segmentation,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,/paper/pointnet-deep-hierarchical-feature-learning,https://arxiv.org/pdf/1706.02413v1.pdf
ShapeNet-Part,,# 4,Instance Average IoU,85.1,PointNet++,-,3D Part Segmentation,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,/paper/pointnet-deep-hierarchical-feature-learning,https://arxiv.org/pdf/1706.02413v1.pdf
DukeMTMC-reID,,# 15,Rank-1,68.1,OIM,-,Person Re-Identification,Joint Detection and Identification Feature Learning for Person Search,/paper/joint-detection-and-identification-feature,https://arxiv.org/pdf/1604.01850v3.pdf
DukeMTMC-reID,,# 15,MAP,47.4,OIM,-,Person Re-Identification,Joint Detection and Identification Feature Learning for Person Search,/paper/joint-detection-and-identification-feature,https://arxiv.org/pdf/1604.01850v3.pdf
VOT2017/18,,# 5,Expected Average Overlap (EAO),0.326,DaSiamRPN,-,Visual Object Tracking,Distractor-aware Siamese Networks for Visual Object Tracking,/paper/distractor-aware-siamese-networks-for-visual,https://arxiv.org/pdf/1808.06048v1.pdf
FDDB,,# 4,AP,0.987,PyramidBox,-,Face Detection,PyramidBox: A Context-assisted Single Shot Face Detector,/paper/pyramidbox-a-context-assisted-single-shot,https://arxiv.org/pdf/1803.07737v2.pdf
WIDER Face (Easy),,# 3,AP,0.956,PyramidBox,-,Face Detection,PyramidBox: A Context-assisted Single Shot Face Detector,/paper/pyramidbox-a-context-assisted-single-shot,https://arxiv.org/pdf/1803.07737v2.pdf
WIDER Face (Hard),,# 4,AP,0.887,PyramidBox,-,Face Detection,PyramidBox: A Context-assisted Single Shot Face Detector,/paper/pyramidbox-a-context-assisted-single-shot,https://arxiv.org/pdf/1803.07737v2.pdf
WIDER Face (Medium),,# 3,AP,0.946,PyramidBox,-,Face Detection,PyramidBox: A Context-assisted Single Shot Face Detector,/paper/pyramidbox-a-context-assisted-single-shot,https://arxiv.org/pdf/1803.07737v2.pdf
GTAV-to-Cityscapes Labels,,# 5,mIoU,31.4,superpixel + color constancy,-,Synthetic-to-Real Translation,A Curriculum Domain Adaptation Approach to the Semantic Segmentation of Urban Scenes,/paper/a-curriculum-domain-adaptation-approach-to,https://arxiv.org/pdf/1812.09953v3.pdf
SYNTHIA-to-Cityscapes,,# 1,mIoU,29.7,superpixel + color constancy,-,Image-to-Image Translation,A Curriculum Domain Adaptation Approach to the Semantic Segmentation of Urban Scenes,/paper/a-curriculum-domain-adaptation-approach-to,https://arxiv.org/pdf/1812.09953v3.pdf
IC15,,# 7,F-Measure,80.72%,EAST,-,Scene Text Detection,EAST: An Efficient and Accurate Scene Text Detector,/paper/east-an-efficient-and-accurate-scene-text,https://arxiv.org/pdf/1704.03155v2.pdf
SCUT-CTW1500,,# 4,F-Measure,60.4%,EAST,-,Curved Text Detection,EAST: An Efficient and Accurate Scene Text Detector,/paper/east-an-efficient-and-accurate-scene-text,https://arxiv.org/pdf/1704.03155v2.pdf
Amazon,,# 4,AUC,0.8679,PNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Bing News,,# 4,AUC,0.8321,PNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Bing News,,# 4,Log Loss,0.2775,PNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Company*,,# 7,AUC,0.8658,OPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Company*,,# 7,Log Loss,0.026410000000000003,OPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Company*,,# 5,AUC,0.8664,IPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Company*,,# 5,Log Loss,0.026369999999999998,IPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Company*,,# 4,AUC,0.8672,PNN*,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Company*,,# 4,Log Loss,0.02636,PNN*,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Criteo,,# 4,AUC,0.7982,OPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Criteo,,# 4,Log Loss,0.45256,OPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Criteo,,# 3,AUC,0.7987,PNN*,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Criteo,,# 3,Log Loss,0.45214,PNN*,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Criteo,,# 6,AUC,0.7972,IPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Criteo,,# 5,Log Loss,0.45323,IPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Dianping,,# 3,AUC,0.8445,PNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
Dianping,,# 4,Log Loss,0.3424,PNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
iPinYou,,# 3,AUC,0.7661,PNN*,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
iPinYou,,# 1,AUC,0.8174,OPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
iPinYou,,# 2,AUC,0.7914,IPNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
MovieLens 20M,,# 4,AUC,0.7321,PNN,-,Click-Through Rate Prediction,Product-based Neural Networks for User Response Prediction,/paper/product-based-neural-networks-for-user,https://arxiv.org/pdf/1611.00144v1.pdf
BSD100 - 4x upscaling,,# 6,PSNR,27.66,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
BSD100 - 4x upscaling,,# 10,SSIM,0.738,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
Set14 - 4x upscaling,,# 7,PSNR,28.8,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
Set14 - 4x upscaling,,# 11,SSIM,0.7856,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
Set5 - 4x upscaling,,# 6,PSNR,32.23,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
Set5 - 4x upscaling,,# 9,SSIM,0.8952,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
Urban100 - 4x upscaling,,# 7,PSNR,26.42,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
Urban100 - 4x upscaling,,# 7,SSIM,0.794,Manifold Simplification,-,Image Super-Resolution,Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification,/paper/beyond-deep-residual-learning-for-image,https://arxiv.org/pdf/1611.06345v4.pdf
CHASE_DB1,,# 3,F1 score,0.78,Residual U-Net,-,Retinal Vessel Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
CHASE_DB1,,# 3,AUC,0.9779,Residual U-Net,-,Retinal Vessel Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
DRIVE,,# 3,F1 score,0.8149,Residual U-Net,-,Retinal Vessel Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
DRIVE,,# 3,AUC,0.9779,Residual U-Net,-,Retinal Vessel Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
Kaggle Skin Lesion Segmentation,,# 2,F1 score,0.8799,Residual U-Net,-,Skin Cancer Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
Kaggle Skin Lesion Segmentation,,# 2,AUC,0.9396,Residual U-Net,-,Skin Cancer Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
LUNA,,# 2,F1 score,0.969,Residual U-Net,-,Lung Nodule Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
LUNA,,# 2,AUC,0.9849,Residual U-Net,-,Lung Nodule Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
STARE,,# 2,F1 score,0.8388,Residual U-Net,-,Retinal Vessel Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
STARE,,# 2,AUC,0.9904,Residual U-Net,-,Retinal Vessel Segmentation,Road Extraction by Deep Residual U-Net,/paper/road-extraction-by-deep-residual-u-net,https://arxiv.org/pdf/1711.10684v1.pdf
CoQA,,# 4,In-domain,78.0,SDNet (single model),-,Question Answering,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,/paper/sdnet-contextualized-attention-based-deep,https://arxiv.org/pdf/1812.03593v5.pdf
CoQA,,# 4,Out-of-domain,73.1,SDNet (single model),-,Question Answering,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,/paper/sdnet-contextualized-attention-based-deep,https://arxiv.org/pdf/1812.03593v5.pdf
CoQA,,# 4,Overall,76.6,SDNet (single model),-,Question Answering,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,/paper/sdnet-contextualized-attention-based-deep,https://arxiv.org/pdf/1812.03593v5.pdf
CoQA,,# 2,In-domain,80.7,SDNet (ensemble),-,Question Answering,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,/paper/sdnet-contextualized-attention-based-deep,https://arxiv.org/pdf/1812.03593v5.pdf
CoQA,,# 2,Out-of-domain,75.9,SDNet (ensemble),-,Question Answering,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,/paper/sdnet-contextualized-attention-based-deep,https://arxiv.org/pdf/1812.03593v5.pdf
CoQA,,# 2,Overall,79.3,SDNet (ensemble),-,Question Answering,SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,/paper/sdnet-contextualized-attention-based-deep,https://arxiv.org/pdf/1812.03593v5.pdf
ACL-ARC,,# 5,F1,51.8,BiLSTM-Attention,-,Citation Intent Classification,Hierarchical Attention Networks for Document Classification,/paper/hierarchical-attention-networks-for-document,https://aclweb.org/anthology/N16-1174
"PASCAL VOC 2012, 60 proposals per image",,# 2,Average Recall,0.667,inst-DML,-,Object Proposal Generation,Semantic Instance Segmentation via Deep Metric Learning,/paper/semantic-instance-segmentation-via-deep,https://arxiv.org/pdf/1703.10277v1.pdf
AI2 Kaggle Dataset,,# 4,[emailÂ protected],54.0,OUR APPROACH,-,Question Answering,Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification,/paper/tell-me-why-using-question-answering-as,https://aclweb.org/anthology/K17-1009
AI2 Kaggle Dataset,,# 1,[emailÂ protected],47.2,IR Baseline,-,Question Answering,Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification,/paper/tell-me-why-using-question-answering-as,https://aclweb.org/anthology/K17-1009
AI2 Kaggle Dataset,,# 3,[emailÂ protected],50.7,IR++,-,Question Answering,Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification,/paper/tell-me-why-using-question-answering-as,https://aclweb.org/anthology/K17-1009
AI2 Kaggle Dataset,,# 2,[emailÂ protected],50.54,Our Approach w/o IR,-,Question Answering,Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification,/paper/tell-me-why-using-question-answering-as,https://aclweb.org/anthology/K17-1009
SNLI,,# 23,% Test Accuracy,86.7,512D Dynamic Meta-Embeddings,-,Natural Language Inference,Dynamic Meta-Embeddings for Improved Sentence Representations,/paper/dynamic-meta-embeddings-for-improved-sentence,https://arxiv.org/pdf/1804.07983v2.pdf
SNLI,,# 24,% Train Accuracy,91.6,512D Dynamic Meta-Embeddings,-,Natural Language Inference,Dynamic Meta-Embeddings for Improved Sentence Representations,/paper/dynamic-meta-embeddings-for-improved-sentence,https://arxiv.org/pdf/1804.07983v2.pdf
SNLI,,# 1,Parameters,9m,512D Dynamic Meta-Embeddings,-,Natural Language Inference,Dynamic Meta-Embeddings for Improved Sentence Representations,/paper/dynamic-meta-embeddings-for-improved-sentence,https://arxiv.org/pdf/1804.07983v2.pdf
CIFAR-10,,# 10,Inception score,6.86,Improved GAN,-,Image Generation,Improved Techniques for Training GANs,/paper/improved-techniques-for-training-gans,https://arxiv.org/pdf/1606.03498v1.pdf
CIFAR-10,,# 5,Inception score,8.09,Improved GAN,-,Conditional Image Generation,Improved Techniques for Training GANs,/paper/improved-techniques-for-training-gans,https://arxiv.org/pdf/1606.03498v1.pdf
"CIFAR-10, 4000 Labels",,# 6,Accuracy,84.41,GAN,-,Semi-Supervised Image Classification,Improved Techniques for Training GANs,/paper/improved-techniques-for-training-gans,https://arxiv.org/pdf/1606.03498v1.pdf
"SVHN, 1000 labels",,# 5,Accuracy,91.89,GAN,-,Semi-Supervised Image Classification,Improved Techniques for Training GANs,/paper/improved-techniques-for-training-gans,https://arxiv.org/pdf/1606.03498v1.pdf
PASCAL VOC 2007,,# 2,MAP,52.4,WSD+PGE+PGA+FSD2,-,Weakly Supervised Object Detection,W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection,/paper/w2f-a-weakly-supervised-to-fully-supervised,https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_W2F_A_Weakly-Supervised_CVPR_2018_paper.pdf
PASCAL VOC 2012,,# 2,MAP,47.8,WSD+PGE+PGA+FSD2,-,Weakly Supervised Object Detection,W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection,/paper/w2f-a-weakly-supervised-to-fully-supervised,https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_W2F_A_Weakly-Supervised_CVPR_2018_paper.pdf
Aerial-to-Map,,# 1,Per-pixel Accuracy,70%,cGAN,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Aerial-to-Map,,# 1,Per-class Accuracy,46%,cGAN,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Aerial-to-Map,,# 1,Class IOU,0.26,cGAN,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Cityscapes Labels-to-Photo,,# 1,Class IOU,0.18,pix2pix,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Cityscapes Labels-to-Photo,,# 1,Per-class Accuracy,25%,pix2pix,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Cityscapes Labels-to-Photo,,# 5,Per-pixel Accuracy,71%,pix2pix,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Cityscapes Photo-to-Labels,,# 1,Per-pixel Accuracy,85%,pix2pix,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Cityscapes Photo-to-Labels,,# 1,Per-class Accuracy,40%,pix2pix,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
Cityscapes Photo-to-Labels,,# 1,Class IOU,0.32,pix2pix,-,Image-to-Image Translation,Image-to-Image Translation with Conditional Adversarial Networks,/paper/image-to-image-translation-with-conditional,https://arxiv.org/pdf/1611.07004v3.pdf
GTSRB,,# 2,Accuracy,98.9%,MicronNet (fp16),-,Traffic Sign Recognition,MicronNet: A Highly Compact Deep Convolutional Neural Network Architecture for Real-time Embedded Traffic Sign Classification,/paper/micronnet-a-highly-compact-deep-convolutional,https://arxiv.org/pdf/1804.00497v3.pdf
VQA v2,,# 3,Accuracy,65.71%,UPMC-LIP6,-,Visual Question Answering,MUTAN: Multimodal Tucker Fusion for Visual Question Answering,/paper/mutan-multimodal-tucker-fusion-for-visual,https://arxiv.org/pdf/1705.06676v1.pdf
GTAV-to-Cityscapes Labels,,# 2,mIoU,41.4,Single-level Adaptation,-,Synthetic-to-Real Translation,Learning to Adapt Structured Output Space for Semantic Segmentation,/paper/learning-to-adapt-structured-output-space-for,https://arxiv.org/pdf/1802.10349v1.pdf
CIFAR-10,,# 36,Percentage correct,91.4,Spectral Representations for Convolutional Neural Networks,-,Image Classification,Spectral Representations for Convolutional Neural Networks,/paper/spectral-representations-for-convolutional-1,https://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf
CIFAR-100,,# 28,Percentage correct,68.4,Spectral Representations for Convolutional Neural Networks,-,Image Classification,Spectral Representations for Convolutional Neural Networks,/paper/spectral-representations-for-convolutional-1,https://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf
CamVid,,# 5,Mean IoU,61.6%,DeepLab-MSc-CRF-LargeFOV,-,Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
CamVid,,# 5,mIoU,61.6%,DeepLab,-,Real-Time Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
CamVid,,# 3,Time (ms),203,DeepLab,-,Real-Time Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
CamVid,,# 3,Frame (fps),4.9,DeepLab,-,Real-Time Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
Cityscapes,,# 15,Mean IoU,63.1%,DeepLab,-,Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
Cityscapes,,# 8,mIoU,63.1%,DeepLab,-,Real-Time Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
Cityscapes,,# 8,Time (ms),4000,DeepLab,-,Real-Time Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
Cityscapes,,# 10,Frame (fps),0.25,DeepLab,-,Real-Time Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
PASCAL VOC 2012,,# 14,Mean IoU,71.6%,DeepLab-MSc-CRF-LargeFOV,-,Semantic Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
SUN-RGBD,,# 1,Mean IoU,32.08,DeepLab-LargeFOV,-,Scene Segmentation,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,/paper/semantic-image-segmentation-with-deep,https://arxiv.org/pdf/1412.7062v4.pdf
MovieLens 10M,,# 2,RMSE,0.7709999999999999,CF-NADE,-,Collaborative Filtering,A Neural Autoregressive Approach to Collaborative Filtering,/paper/a-neural-autoregressive-approach-to,https://arxiv.org/pdf/1605.09477v1.pdf
MovieLens 1M,,# 2,RMSE,0.8290000000000001,CF-NADE,-,Collaborative Filtering,A Neural Autoregressive Approach to Collaborative Filtering,/paper/a-neural-autoregressive-approach-to,https://arxiv.org/pdf/1605.09477v1.pdf
IJB-A,,# 12,TAR @ FAR=0.01,78.70%,Deep multi-pose representations,-,Face Verification,Face Recognition Using Deep Multi-Pose Representations,/paper/face-recognition-using-deep-multi-pose,https://arxiv.org/pdf/1603.07388v1.pdf
COCO,,# 18,Bounding Box AP,42.1,CornerNet,-,Object Detection,CornerNet: Detecting Objects as Paired Keypoints,/paper/cornernet-detecting-objects-as-paired,https://arxiv.org/pdf/1808.01244v2.pdf
MovieLens 100K,,# 4,RMSE,0.996,GMC,-,Collaborative Filtering,Matrix Completion on Graphs,/paper/matrix-completion-on-graphs,https://arxiv.org/pdf/1408.1717v3.pdf
METR-LA,,# 1,MAE @ 12 step,3.6,DCRNN,-,Traffic Prediction,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,/paper/diffusion-convolutional-recurrent-neural,https://arxiv.org/pdf/1707.01926v3.pdf
enwiki8,,# 8,Bit per Character (BPC),1.25,Large FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
enwiki8,,# 1,Number of params,47M,Large FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Hutter Prize,,# 8,Bit per Character (BPC),1.245,Large FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Hutter Prize,,# 1,Number of params,47M,Large FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Hutter Prize,,# 10,Bit per Character (BPC),1.277,FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Hutter Prize,,# 1,Number of params,27M,FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Penn Treebank (Character Level),,# 5,Bit per Character (BPC),1.190,FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Penn Treebank (Character Level),,# 1,Number of params,27M,FS-LSTM-4,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Penn Treebank (Character Level),,# 6,Bit per Character (BPC),1.193,FS-LSTM-2,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
Penn Treebank (Character Level),,# 1,Number of params,27M,FS-LSTM-2,-,Language Modelling,Fast-Slow Recurrent Neural Networks,/paper/fast-slow-recurrent-neural-networks,https://arxiv.org/pdf/1705.08639v2.pdf
COCO,,# 1,Bounding Box AP,48.4,X101 + DCN + Cascade + GC r4,-,Object Detection,GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond,/paper/gcnet-non-local-networks-meet-squeeze,https://arxiv.org/pdf/1904.11492v1.pdf
SQuAD1.1,,# 51,EM,77.583,RaSoR + TR + LM (single model),-,Question Answering,Contextualized Word Representations for Reading Comprehension,/paper/contextualized-word-representations-for,https://arxiv.org/pdf/1712.03609v4.pdf
SQuAD1.1,,# 62,F1,84.163,RaSoR + TR + LM (single model),-,Question Answering,Contextualized Word Representations for Reading Comprehension,/paper/contextualized-word-representations-for,https://arxiv.org/pdf/1712.03609v4.pdf
SQuAD1.1,,# 69,EM,75.789,RaSoR + TR (single model),-,Question Answering,Contextualized Word Representations for Reading Comprehension,/paper/contextualized-word-representations-for,https://arxiv.org/pdf/1712.03609v4.pdf
SQuAD1.1,,# 70,F1,83.26100000000001,RaSoR + TR (single model),-,Question Answering,Contextualized Word Representations for Reading Comprehension,/paper/contextualized-word-representations-for,https://arxiv.org/pdf/1712.03609v4.pdf
CIFAR-10,,# 4,Percentage correct,97.7,WRN + fixup init + mixup + cutout,-,Image Classification,Fixup Initialization: Residual Learning Without Normalization,/paper/fixup-initialization-residual-learning,https://arxiv.org/pdf/1901.09321v2.pdf
CIFAR-10,,# 4,Percentage error,2.3,WRN + fixup init + mixup + cutout,-,Image Classification,Fixup Initialization: Residual Learning Without Normalization,/paper/fixup-initialization-residual-learning,https://arxiv.org/pdf/1901.09321v2.pdf
SVHN,,# 1,Percentage error,1.4,WRN + fixup init + mixup + cutout,-,Image Classification,Fixup Initialization: Residual Learning Without Normalization,/paper/fixup-initialization-residual-learning,https://arxiv.org/pdf/1901.09321v2.pdf
SQuAD1.1,,# 119,EM,67.90100000000001,Match-LSTM with Ans-Ptr (Boundary) (ensemble),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 123,F1,77.02199999999999,Match-LSTM with Ans-Ptr (Boundary) (ensemble),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 125,EM,64.744,Match-LSTM with Bi-Ans-Ptr (Boundary),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 128,F1,73.743,Match-LSTM with Bi-Ans-Ptr (Boundary),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 133,EM,60.474,Match-LSTM with Ans-Ptr (Boundary),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 135,F1,70.695,Match-LSTM with Ans-Ptr (Boundary),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 135,EM,54.505,Match-LSTM with Ans-Ptr (Sentence),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
SQuAD1.1,,# 137,F1,67.748,Match-LSTM with Ans-Ptr (Sentence),-,Question Answering,Machine Comprehension Using Match-LSTM and Answer Pointer,/paper/machine-comprehension-using-match-lstm-and,https://arxiv.org/pdf/1608.07905v2.pdf
FB15k,,# 1,MR,2501.0,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k,,# 5,MRR,0.66,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k,,# 5,[emailÂ protected],0.66,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k,,# 4,[emailÂ protected],0.659,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k,,# 4,[emailÂ protected],0.6579999999999999,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k-237,,# 4,MRR,0.325,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k-237,,# 4,[emailÂ protected],0.501,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k-237,,# 3,[emailÂ protected],0.35600000000000004,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
FB15k-237,,# 3,[emailÂ protected],0.237,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18,,# 1,MRR,0.963,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18,,# 1,[emailÂ protected],0.9640000000000001,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18,,# 1,[emailÂ protected],0.9640000000000001,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18,,# 1,[emailÂ protected],0.953,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18,,# 2,MR,740.0,Inverse Model,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18RR,,# 6,MRR,0.43,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18RR,,# 5,[emailÂ protected],0.52,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18RR,,# 4,[emailÂ protected],0.44,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
WN18RR,,# 5,[emailÂ protected],0.4,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
YAGO3-10,,# 2,MRR,0.44,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
YAGO3-10,,# 2,[emailÂ protected],0.62,ConvE,-,Link Prediction,Convolutional 2D Knowledge Graph Embeddings,/paper/convolutional-2d-knowledge-graph-embeddings,https://arxiv.org/pdf/1707.01476v6.pdf
Labeled Faces in the Wild,,# 11,Accuracy,98.52%,GaussianFace,-,Face Verification,Surpassing Human-Level Face Verification Performance on LFW with GaussianFace,/paper/surpassing-human-level-face-verification,https://arxiv.org/pdf/1404.3840v3.pdf
Penn Treebank,,# 4,F1 score,94.47,LSTM Encoder-Decoder + LSTM-LM,-,Constituency Parsing,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
Penn Treebank (Word Level),,# 8,Validation perplexity,54.12,AWD-LSTM-DOC,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
Penn Treebank (Word Level),,# 8,Test perplexity,52.38,AWD-LSTM-DOC,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
Penn Treebank (Word Level),,# 1,Params,23M,AWD-LSTM-DOC,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
Penn Treebank (Word Level),,# 4,Validation perplexity,48.63,AWD-LSTM-DOC x5,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
Penn Treebank (Word Level),,# 3,Test perplexity,47.17,AWD-LSTM-DOC x5,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
Penn Treebank (Word Level),,# 1,Params,185M,AWD-LSTM-DOC x5,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
WikiText-2,,# 6,Validation perplexity,54.19,AWD-LSTM-DOC x5,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
WikiText-2,,# 7,Test perplexity,53.09,AWD-LSTM-DOC x5,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
WikiText-2,,# 1,Number of params,185M,AWD-LSTM-DOC x5,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
WikiText-2,,# 8,Validation perplexity,60.29,AWD-LSTM-DOC,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
WikiText-2,,# 9,Test perplexity,58.03,AWD-LSTM-DOC,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
WikiText-2,,# 1,Number of params,37M,AWD-LSTM-DOC,-,Language Modelling,Direct Output Connection for a High-Rank Language Model,/paper/direct-output-connection-for-a-high-rank,https://arxiv.org/pdf/1808.10143v2.pdf
20NEWS,,# 2,Accuracy,86.34,Text GCN,-,Text Classification,Graph Convolutional Networks for Text Classification,/paper/graph-convolutional-networks-for-text,https://arxiv.org/pdf/1809.05679v3.pdf
MR,,# 8,Accuracy,76.74,Text GCN,-,Sentiment Analysis,Graph Convolutional Networks for Text Classification,/paper/graph-convolutional-networks-for-text,https://arxiv.org/pdf/1809.05679v3.pdf
Ohsumed,,# 2,Accuracy,68.36,Text GCN,-,Text Classification,Graph Convolutional Networks for Text Classification,/paper/graph-convolutional-networks-for-text,https://arxiv.org/pdf/1809.05679v3.pdf
R52,,# 2,Accuracy,93.56,Text GCN,-,Text Classification,Graph Convolutional Networks for Text Classification,/paper/graph-convolutional-networks-for-text,https://arxiv.org/pdf/1809.05679v3.pdf
R8,,# 2,Accuracy,97.07,Text GCN,-,Text Classification,Graph Convolutional Networks for Text Classification,/paper/graph-convolutional-networks-for-text,https://arxiv.org/pdf/1809.05679v3.pdf
CIFAR-10,,# 3,NLL Test,3.0,PixelRNN,-,Image Generation,Pixel Recurrent Neural Networks,/paper/pixel-recurrent-neural-networks,https://arxiv.org/pdf/1601.06759v3.pdf
CIFAR-10,,# 5,NLL Test,3.14,PixelCNN,-,Image Generation,Pixel Recurrent Neural Networks,/paper/pixel-recurrent-neural-networks,https://arxiv.org/pdf/1601.06759v3.pdf
CIFAR-10,,# 11,NLL Test,4.48,NICE,-,Image Generation,Pixel Recurrent Neural Networks,/paper/pixel-recurrent-neural-networks,https://arxiv.org/pdf/1601.06759v3.pdf
Chinese Poems,,# 1,BLEU-2,0.8809999999999999,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
COCO Captions,,# 1,BLEU-2,0.95,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
COCO Captions,,# 1,BLEU-3,0.88,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
COCO Captions,,# 1,BLEU-4,0.778,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
COCO Captions,,# 1,BLEU-5,0.6859999999999999,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
EMNLP2017 WMT,,# 1,BLEU-2,0.956,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
EMNLP2017 WMT,,# 1,BLEU-3,0.8190000000000001,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
EMNLP2017 WMT,,# 1,BLEU-4,0.627,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
EMNLP2017 WMT,,# 1,BLEU-5,0.498,LeakGAN,-,Text Generation,Long Text Generation via Adversarial Training with Leaked Information,/paper/long-text-generation-via-adversarial-training,https://arxiv.org/pdf/1709.08624v2.pdf
WMT2015 English-German,,# 3,BLEU score,23.45,Enc-Dec Att (char),-,Machine Translation,A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation,/paper/a-character-level-decoder-without-explicit,https://arxiv.org/pdf/1603.06147v4.pdf
WMT2015 English-German,,# 5,BLEU score,21.72,Enc-Dec Att (BPE),-,Machine Translation,A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation,/paper/a-character-level-decoder-without-explicit,https://arxiv.org/pdf/1603.06147v4.pdf
OntoNotes,,# 6,F1,82.7,Tan et al.,-,Semantic Role Labeling,Deep Semantic Role Labeling with Self-Attention,/paper/deep-semantic-role-labeling-with-self,https://arxiv.org/pdf/1712.01586v1.pdf
IJB-A,,# 13,TAR @ FAR=0.01,73.30%,Deep CNN + COTS matcher,-,Face Verification,Face Search at Scale: 80 Million Gallery,/paper/face-search-at-scale-80-million-gallery,https://arxiv.org/pdf/1507.07242v2.pdf
FGNET,,# 1,MAE,3.62,CMAAE-OR,-,Age Estimation,Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression,/paper/facial-aging-and-rejuvenation-by-conditional,https://arxiv.org/pdf/1804.02740v1.pdf
MORPH,,# 1,MAE,1.48,CMAAE-OR,-,Age Estimation,Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression,/paper/facial-aging-and-rejuvenation-by-conditional,https://arxiv.org/pdf/1804.02740v1.pdf
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 2,Percentage correct,67.3,joint-loss,-,Visual Question Answering,Training Recurrent Answering Units with Joint Loss Minimization for VQA,/paper/training-recurrent-answering-units-with-joint,https://arxiv.org/pdf/1606.03647v2.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 3,Percentage correct,63.2,joint-loss,-,Visual Question Answering,Training Recurrent Answering Units with Joint Loss Minimization for VQA,/paper/training-recurrent-answering-units-with-joint,https://arxiv.org/pdf/1606.03647v2.pdf
Penn Treebank,,# 2,POS,97.44,Andor et al.,-,Dependency Parsing,Globally Normalized Transition-Based Neural Networks,/paper/globally-normalized-transition-based-neural,https://arxiv.org/pdf/1603.06042v2.pdf
Penn Treebank,,# 3,UAS,94.61,Andor et al.,-,Dependency Parsing,Globally Normalized Transition-Based Neural Networks,/paper/globally-normalized-transition-based-neural,https://arxiv.org/pdf/1603.06042v2.pdf
Penn Treebank,,# 4,LAS,92.79,Andor et al.,-,Dependency Parsing,Globally Normalized Transition-Based Neural Networks,/paper/globally-normalized-transition-based-neural,https://arxiv.org/pdf/1603.06042v2.pdf
SK-LARGE,,# 2,F-Measure,0.7240000000000001,Hi-Fi,-,Object Skeleton Detection,Hi-Fi: Hierarchical Feature Integration for Skeleton Detection,/paper/hi-fi-hierarchical-feature-integration-for,https://arxiv.org/pdf/1801.01849v4.pdf
ImageNet,,# 19,Top 1 Accuracy,74.8%,Inception V2,-,Image Classification,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,/paper/batch-normalization-accelerating-deep-network,https://arxiv.org/pdf/1502.03167v3.pdf
ImageNet,,# 14,Top 5 Accuracy,92.2%,Inception V2,-,Image Classification,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,/paper/batch-normalization-accelerating-deep-network,https://arxiv.org/pdf/1502.03167v3.pdf
CoNLL 2003 (English),,# 17,F1,91.21,Ma and Hovy,-,Named Entity Recognition (NER),End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,/paper/end-to-end-sequence-labeling-via-bi,https://arxiv.org/pdf/1603.01354v5.pdf
Penn Treebank,,# 5,Accuracy,97.55,Ma and Hovy,-,Part-Of-Speech Tagging,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,/paper/end-to-end-sequence-labeling-via-bi,https://arxiv.org/pdf/1603.01354v5.pdf
KITTI Cars Easy,,# 6,AP,79.75%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Cars Hard,,# 3,AP,66.33%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Cars Moderate,,# 5,AP,72.57%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Cyclists Easy,,# 2,AP,71.40%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Cyclists Hard,,# 2,AP,48.34%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Cyclists Moderate,,# 3,AP,53.46%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Pedestrians Easy,,# 1,AP,56.92%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Pedestrians Hard,,# 1,AP,42.39%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
KITTI Pedestrians Moderate,,# 2,AP,44.68%,IPOD,-,3D Object Detection,IPOD: Intensive Point-based Object Detector for Point Cloud,/paper/ipod-intensive-point-based-object-detector,https://arxiv.org/pdf/1812.05276v1.pdf
CIFAR-10,,# 5,Inception score,7.9,Splitting GAN,-,Image Generation,Class-Splitting Generative Adversarial Networks,/paper/class-splitting-generative-adversarial,https://arxiv.org/pdf/1709.07359v2.pdf
CIFAR-10,,# 1,Inception score,8.87,Splitting GAN,-,Conditional Image Generation,Class-Splitting Generative Adversarial Networks,/paper/class-splitting-generative-adversarial,https://arxiv.org/pdf/1709.07359v2.pdf
Charades,,# 4,MAP,25.2,CoViAR+optical flow,-,Action Recognition In Videos,Compressed Video Action Recognition,/paper/compressed-video-action-recognition,https://arxiv.org/pdf/1712.00636v2.pdf
Sequential MNIST,,# 4,Unpermuted Accuracy,97%,iRNN,-,Sequential Image Classification,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,/paper/a-simple-way-to-initialize-recurrent-networks,https://arxiv.org/pdf/1504.00941v2.pdf
Sequential MNIST,,# 5,Permuted Accuracy,82%,iRNN,-,Sequential Image Classification,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,/paper/a-simple-way-to-initialize-recurrent-networks,https://arxiv.org/pdf/1504.00941v2.pdf
FLIC Elbows,,# 1,[emailÂ protected],99.0%,Stacked Hourglass Networks,-,Pose Estimation,Stacked Hourglass Networks for Human Pose Estimation,/paper/stacked-hourglass-networks-for-human-pose,https://arxiv.org/pdf/1603.06937v2.pdf
FLIC Wrists,,# 1,[emailÂ protected],97.0%,Stacked Hourglass Networks,-,Pose Estimation,Stacked Hourglass Networks for Human Pose Estimation,/paper/stacked-hourglass-networks-for-human-pose,https://arxiv.org/pdf/1603.06937v2.pdf
MPII Human Pose,,# 6,PCKh-0.5,90.9%,Stacked Hourglass Networks,-,Pose Estimation,Stacked Hourglass Networks for Human Pose Estimation,/paper/stacked-hourglass-networks-for-human-pose,https://arxiv.org/pdf/1603.06937v2.pdf
SNLI,,# 10,% Test Accuracy,88.8,300D DMAN,-,Natural Language Inference,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,/paper/discourse-marker-augmented-network-with,https://aclweb.org/anthology/P18-1091
SNLI,,# 8,% Train Accuracy,95.4,300D DMAN,-,Natural Language Inference,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,/paper/discourse-marker-augmented-network-with,https://aclweb.org/anthology/P18-1091
SNLI,,# 1,Parameters,9.2m,300D DMAN,-,Natural Language Inference,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,/paper/discourse-marker-augmented-network-with,https://aclweb.org/anthology/P18-1091
SNLI,,# 5,% Test Accuracy,89.6,300D DMAN Ensemble,-,Natural Language Inference,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,/paper/discourse-marker-augmented-network-with,https://aclweb.org/anthology/P18-1091
SNLI,,# 5,% Train Accuracy,96.1,300D DMAN Ensemble,-,Natural Language Inference,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,/paper/discourse-marker-augmented-network-with,https://aclweb.org/anthology/P18-1091
SNLI,,# 1,Parameters,79m,300D DMAN Ensemble,-,Natural Language Inference,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,/paper/discourse-marker-augmented-network-with,https://aclweb.org/anthology/P18-1091
BSD100 - 4x upscaling,,# 2,PSNR,27.79,ProSR,-,Image Super-Resolution,A Fully Progressive Approach to Single-Image Super-Resolution,/paper/a-fully-progressive-approach-to-single-image,https://arxiv.org/pdf/1804.02900v2.pdf
Set14 - 4x upscaling,,# 3,PSNR,28.94,ProSR,-,Image Super-Resolution,A Fully Progressive Approach to Single-Image Super-Resolution,/paper/a-fully-progressive-approach-to-single-image,https://arxiv.org/pdf/1804.02900v2.pdf
Urban100 - 4x upscaling,,# 2,PSNR,26.89,ProSR,-,Image Super-Resolution,A Fully Progressive Approach to Single-Image Super-Resolution,/paper/a-fully-progressive-approach-to-single-image,https://arxiv.org/pdf/1804.02900v2.pdf
WMT2014 English-French,,# 2,BLEU,33.4,MLM pretraining for encoder and decoder,-,Unsupervised Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
WMT2014 French-English,,# 2,BLEU,33.3,MLM pretraining for encoder and decoder,-,Unsupervised Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
WMT2016 English-German,,# 2,BLEU,26.4,MLM pretraining for encoder and decoder,-,Unsupervised Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
WMT2016 English-Romanian,,# 1,BLEU,33.3,MLM pretraining for encoder and decoder,-,Unsupervised Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
WMT2016 German-English,,# 2,BLEU,34.3,MLM pretraining for encoder and decoder,-,Unsupervised Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
WMT2016 Romanian-English,,# 1,BLEU,31.8,MLM pretraining for encoder and decoder,-,Unsupervised Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
WMT2016 Romanian-English,,# 1,BLEU score,35.3,MLM pretraining,-,Machine Translation,Cross-lingual Language Model Pretraining,/paper/cross-lingual-language-model-pretraining,https://arxiv.org/pdf/1901.07291v1.pdf
CoNLL 2003 (English),,# 11,F1,91.57,S-LSTM,-,Named Entity Recognition (NER),Sentence-State LSTM for Text Representation,/paper/sentence-state-lstm-for-text-representation,https://arxiv.org/pdf/1805.02474v1.pdf
IMDb,,# 12,Accuracy,87.15,S-LSTM,-,Sentiment Analysis,Sentence-State LSTM for Text Representation,/paper/sentence-state-lstm-for-text-representation,https://arxiv.org/pdf/1805.02474v1.pdf
MR,,# 9,Accuracy,76.2,S-LSTM,-,Sentiment Analysis,Sentence-State LSTM for Text Representation,/paper/sentence-state-lstm-for-text-representation,https://arxiv.org/pdf/1805.02474v1.pdf
Penn Treebank,,# 5,Accuracy,97.55,S-LSTM,-,Part-Of-Speech Tagging,Sentence-State LSTM for Text Representation,/paper/sentence-state-lstm-for-text-representation,https://arxiv.org/pdf/1805.02474v1.pdf
SQuAD1.1,,# 120,EM,67.744,FABIR,-,Question Answering,A Fully Attention-Based Information Retriever,/paper/a-fully-attention-based-information-retriever,https://arxiv.org/pdf/1810.09580v1.pdf
SQuAD1.1,,# 117,F1,77.605,FABIR,-,Question Answering,A Fully Attention-Based Information Retriever,/paper/a-fully-attention-based-information-retriever,https://arxiv.org/pdf/1810.09580v1.pdf
TriviaQA,,# 1,EM,67.21,MemoReader,-,Question Answering,MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller,/paper/memoreader-large-scale-reading-comprehension,https://aclweb.org/anthology/D18-1237
TriviaQA,,# 1,F1,73.26,MemoReader,-,Question Answering,MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller,/paper/memoreader-large-scale-reading-comprehension,https://aclweb.org/anthology/D18-1237
ShapeNet-Part,,# 6,Class Average IoU,77.4,Kd-net,-,3D Part Segmentation,Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models,/paper/escape-from-cells-deep-kd-networks-for-the,https://arxiv.org/pdf/1704.01222v2.pdf
ShapeNet-Part,,# 8,Instance Average IoU,82.3,Kd-net,-,3D Part Segmentation,Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models,/paper/escape-from-cells-deep-kd-networks-for-the,https://arxiv.org/pdf/1704.01222v2.pdf
ACL-ARC,,# 1,F1,67.9,Structural-scaffolds,-,Citation Intent Classification,Structural Scaffolds for Citation Intent Classification in Scientific Publications,/paper/structural-scaffolds-for-citation-intent,https://arxiv.org/pdf/1904.01608v1.pdf
ACL-ARC,,# 1,F1,67.9,Structural-scaffolds,-,Sentence Classification,Structural Scaffolds for Citation Intent Classification in Scientific Publications,/paper/structural-scaffolds-for-citation-intent,https://arxiv.org/pdf/1904.01608v1.pdf
SciCite,,# 3,F1,84.0,Structural-scaffolds,-,Sentence Classification,Structural Scaffolds for Citation Intent Classification in Scientific Publications,/paper/structural-scaffolds-for-citation-intent,https://arxiv.org/pdf/1904.01608v1.pdf
SciCite,,# 2,F1,84.0,Structural-Scaffolds,-,Citation Intent Classification,Structural Scaffolds for Citation Intent Classification in Scientific Publications,/paper/structural-scaffolds-for-citation-intent,https://arxiv.org/pdf/1904.01608v1.pdf
CREMI,,# 1,VOI,0.606,U-NET MALA,-,Brain Image Segmentation,Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction,/paper/large-scale-image-segmentation-with,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8364622
CREMI,,# 1,CREMI Score,0.289,U-NET MALA,-,Brain Image Segmentation,Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction,/paper/large-scale-image-segmentation-with,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8364622
FIB-25 Synaptic Sites,,# 1,VOI,2.151,U-NET MALA,-,Brain Image Segmentation,Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction,/paper/large-scale-image-segmentation-with,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8364622
FIB-25 Whole Test,,# 1,VOI,1.071,U-NET MALA,-,Brain Image Segmentation,Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction,/paper/large-scale-image-segmentation-with,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8364622
SegEM,,# 1,IED,4.8389999999999995,U-NET MALA,-,Brain Image Segmentation,Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction,/paper/large-scale-image-segmentation-with,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8364622
CIFAR-10,,# 5,Percentage correct,97.12,ShakeShake-2x64d + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
CIFAR-10,,# 5,Percentage error,2.88,ShakeShake-2x64d + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
CIFAR-10,,# 6,Percentage correct,96.79,WRN-28-10 + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
CIFAR-10,,# 6,Percentage error,3.21,WRN-28-10 + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
CIFAR-100,,# 3,Percentage correct,84.16,PyramidNet-272 + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
CIFAR-100,,# 6,Percentage correct,82.15,WRN+SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
ImageNet,,# 14,Top 1 Accuracy,78.94,ResNet-152 + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
ImageNet,,# 16,Top 1 Accuracy,78.44,DenseNet-161 + SWA,-,Image Classification,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and,https://arxiv.org/pdf/1803.05407v3.pdf
CIFAR-10,,# 19,Percentage correct,94.4,ResNet+ELU,-,Image Classification,Deep Residual Networks with Exponential Linear Unit,/paper/deep-residual-networks-with-exponential,https://arxiv.org/pdf/1604.04112v4.pdf
CIFAR-100,,# 17,Percentage correct,73.5,ResNet+ELU,-,Image Classification,Deep Residual Networks with Exponential Linear Unit,/paper/deep-residual-networks-with-exponential,https://arxiv.org/pdf/1604.04112v4.pdf
CIFAR-10,,# 4,NLL Test,3.03,Gated PixelCNN,-,Image Generation,Conditional Image Generation with PixelCNN Decoders,/paper/conditional-image-generation-with-pixelcnn,https://arxiv.org/pdf/1606.05328v2.pdf
IC15,,# 4,F-Measure,84.3%,Corner localization + region segmentation,-,Scene Text Detection,Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation,/paper/multi-oriented-scene-text-detection-via,https://arxiv.org/pdf/1802.08948v2.pdf
IC17-MLT,,# 3,F-Measure,66.8%,Corner Localization + Region Segmentation,-,Scene Text Detection,Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation,/paper/multi-oriented-scene-text-detection-via,https://arxiv.org/pdf/1802.08948v2.pdf
IC15,,# 1,F-Measure,87.99%,FOTS,-,Scene Text Detection,FOTS: Fast Oriented Text Spotting with a Unified Network,/paper/fots-fast-oriented-text-spotting-with-a,https://arxiv.org/pdf/1801.01671v2.pdf
IC17-MLT,,# 2,F-Measure,67.25%,FOTS,-,Scene Text Detection,FOTS: Fast Oriented Text Spotting with a Unified Network,/paper/fots-fast-oriented-text-spotting-with-a,https://arxiv.org/pdf/1801.01671v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 8,Restaurant (Acc),81.25,MGAN,-,Aspect-Based Sentiment Analysis,Multi-grained Attention Network for Aspect-Level Sentiment Classification,/paper/multi-grained-attention-network-for-aspect,https://aclweb.org/anthology/D18-1380
SemEval 2014 Task 4 Sub Task 2,,# 20,Laptop (Acc),75.39,MGAN,-,Aspect-Based Sentiment Analysis,Multi-grained Attention Network for Aspect-Level Sentiment Classification,/paper/multi-grained-attention-network-for-aspect,https://aclweb.org/anthology/D18-1380
Story Cloze Test,,# 4,Accuracy,76.5,val-LS-skip,-,Question Answering,A Simple and Effective Approach to the Story Cloze Test,/paper/a-simple-and-effective-approach-to-the-story-1,https://aclweb.org/anthology/N18-2015
MovieLens 10M,,# 7,RMSE,0.799,Factorization with dictionary learning,-,Collaborative Filtering,Dictionary Learning for Massive Matrix Factorization,/paper/dictionary-learning-for-massive-matrix,https://arxiv.org/pdf/1605.00937v2.pdf
MovieLens 1M,,# 8,RMSE,0.866,Factorization with dictionary learning,-,Collaborative Filtering,Dictionary Learning for Massive Matrix Factorization,/paper/dictionary-learning-for-massive-matrix,https://arxiv.org/pdf/1605.00937v2.pdf
Netflix,,# 2,RMSE,0.934,Factorization with dictionary learning,-,Collaborative Filtering,Dictionary Learning for Massive Matrix Factorization,/paper/dictionary-learning-for-massive-matrix,https://arxiv.org/pdf/1605.00937v2.pdf
TCIA Pancreas-CT,,# 2,Dice Score,76.8,Multi-class 3D FCN,-,3D Medical Imaging Segmentation,An application of cascaded 3D fully convolutional networks for medical image segmentation,/paper/an-application-of-cascaded-3d-fully,https://arxiv.org/pdf/1803.05431v2.pdf
CIFAR-10 Image Classification,,# 2,Percentage error,2.13,AmoebaNet-B + c/o,-,Architecture Search,Regularized Evolution for Image Classifier Architecture Search,/paper/regularized-evolution-for-image-classifier,https://arxiv.org/pdf/1802.01548v7.pdf
CIFAR-10 Image Classification,,# 1,Params,34.9M,AmoebaNet-B + c/o,-,Architecture Search,Regularized Evolution for Image Classifier Architecture Search,/paper/regularized-evolution-for-image-classifier,https://arxiv.org/pdf/1802.01548v7.pdf
ImageNet,,# 2,Top 1 Accuracy,83.9%,AmoebaNet-A,-,Image Classification,Regularized Evolution for Image Classifier Architecture Search,/paper/regularized-evolution-for-image-classifier,https://arxiv.org/pdf/1802.01548v7.pdf
enwiki8,,# 7,Bit per Character (BPC),1.24,Large mLSTM,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
enwiki8,,# 1,Number of params,46M,Large mLSTM,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
Hutter Prize,,# 7,Bit per Character (BPC),1.24,Large mLSTM +emb +WN +VD,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
Hutter Prize,,# 1,Number of params,46M,Large mLSTM +emb +WN +VD,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
Text8,,# 10,Bit per Character (BPC),1.40,Unregularised mLSTM,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
Text8,,# 1,Number of params,45M,Unregularised mLSTM,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
Text8,,# 7,Bit per Character (BPC),1.27,Large mLSTM +emb +WN +VD,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
Text8,,# 1,Number of params,45M,Large mLSTM +emb +WN +VD,-,Language Modelling,Multiplicative LSTM for sequence modelling,/paper/multiplicative-lstm-for-sequence-modelling,https://arxiv.org/pdf/1609.07959v3.pdf
CelebA-HQ 1024x1024,,# 1,FID,5.06,StyleGAN,-,Image Generation,A Style-Based Generator Architecture for Generative Adversarial Networks,/paper/a-style-based-generator-architecture-for,https://arxiv.org/pdf/1812.04948v3.pdf
FFHQ,,# 2,FID,4.43,StyleGAN,-,Image Generation,A Style-Based Generator Architecture for Generative Adversarial Networks,/paper/a-style-based-generator-architecture-for,https://arxiv.org/pdf/1812.04948v3.pdf
SST-2 Binary classification,,# 9,Accuracy,90.0,CNN-RNF-LSTM,-,Sentiment Analysis,Convolutional Neural Networks with Recurrent Neural Filters,/paper/convolutional-neural-networks-with-recurrent,https://arxiv.org/pdf/1808.09315v1.pdf
SST-5 Fine-grained classification,,# 6,Accuracy,53.4,CNN-RNF-LSTM,-,Sentiment Analysis,Convolutional Neural Networks with Recurrent Neural Filters,/paper/convolutional-neural-networks-with-recurrent,https://arxiv.org/pdf/1808.09315v1.pdf
wireframe dataset,,# 1,F1 score,0.773,atrous Residual U-Net,-,Line segment detection,Learning Attraction Field Representation for Robust Line Segment Detection,/paper/learning-attraction-field-representation-for-1,https://arxiv.org/pdf/1812.02122.pdf
MPII Multi-Person,,# 2,AP,80.4%,Generative Partition Networks,-,Multi-Person Pose Estimation,Generative Partition Networks for Multi-Person Pose Estimation,/paper/generative-partition-networks-for-multi,https://arxiv.org/pdf/1705.07422v2.pdf
WAF,,# 1,AP,84.8,Generative Partition Networks,-,Multi-Person Pose Estimation,Generative Partition Networks for Multi-Person Pose Estimation,/paper/generative-partition-networks-for-multi,https://arxiv.org/pdf/1705.07422v2.pdf
ImageNet,,# 3,Top 1 Accuracy,82.9%,Oct-ResNet-152 + SE,-,Image Classification,Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution,/paper/drop-an-octave-reducing-spatial-redundancy-in,https://arxiv.org/pdf/1904.05049v2.pdf
ImageNet,,# 2,Top 5 Accuracy,96.3%,Oct-ResNet-152 + SE,-,Image Classification,Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution,/paper/drop-an-octave-reducing-spatial-redundancy-in,https://arxiv.org/pdf/1904.05049v2.pdf
MovieLens 10M,,# 1,RMSE,0.769,Sparse FC,-,Collaborative Filtering,Kernelized Synaptic Weight Matrices,/paper/kernelized-synaptic-weight-matrices,https://proceedings.mlr.press/v80/muller18a/muller18a.pdf
MovieLens 1M,,# 1,RMSE,0.8240000000000001,Sparse FC,-,Collaborative Filtering,Kernelized Synaptic Weight Matrices,/paper/kernelized-synaptic-weight-matrices,https://proceedings.mlr.press/v80/muller18a/muller18a.pdf
E2E NLG Challenge,,# 7,BLEU,56.57,TUDA,-,Data-to-Text Generation,E2E NLG Challenge: Neural Models vs. Templates,/paper/e2e-nlg-challenge-neural-models-vs-templates,https://aclweb.org/anthology/W18-6557
E2E NLG Challenge,,# 7,NIST,7.4544,TUDA,-,Data-to-Text Generation,E2E NLG Challenge: Neural Models vs. Templates,/paper/e2e-nlg-challenge-neural-models-vs-templates,https://aclweb.org/anthology/W18-6557
E2E NLG Challenge,,# 1,METEOR,45.29,TUDA,-,Data-to-Text Generation,E2E NLG Challenge: Neural Models vs. Templates,/paper/e2e-nlg-challenge-neural-models-vs-templates,https://aclweb.org/anthology/W18-6557
E2E NLG Challenge,,# 7,ROUGE-L,66.14,TUDA,-,Data-to-Text Generation,E2E NLG Challenge: Neural Models vs. Templates,/paper/e2e-nlg-challenge-neural-models-vs-templates,https://aclweb.org/anthology/W18-6557
E2E NLG Challenge,,# 6,CIDEr,1.8206,TUDA,-,Data-to-Text Generation,E2E NLG Challenge: Neural Models vs. Templates,/paper/e2e-nlg-challenge-neural-models-vs-templates,https://aclweb.org/anthology/W18-6557
CoNLL-2014 A1,,# 7,F0.5,16.4,Bi-LSTM (trained on FCE),-,Grammatical Error Detection,Compositional Sequence Labeling Models for Error Detection in Learner Writing,/paper/compositional-sequence-labeling-models-for,https://arxiv.org/pdf/1607.06153v1.pdf
CoNLL-2014 A1,,# 2,F0.5,34.3,Bi-LSTM (unrestricted data),-,Grammatical Error Detection,Compositional Sequence Labeling Models for Error Detection in Learner Writing,/paper/compositional-sequence-labeling-models-for,https://arxiv.org/pdf/1607.06153v1.pdf
CoNLL-2014 A2,,# 7,F0.5,23.9,Bi-LSTM (trained on FCE),-,Grammatical Error Detection,Compositional Sequence Labeling Models for Error Detection in Learner Writing,/paper/compositional-sequence-labeling-models-for,https://arxiv.org/pdf/1607.06153v1.pdf
CoNLL-2014 A2,,# 2,F0.5,44.0,Bi-LSTM (unrestricted data),-,Grammatical Error Detection,Compositional Sequence Labeling Models for Error Detection in Learner Writing,/paper/compositional-sequence-labeling-models-for,https://arxiv.org/pdf/1607.06153v1.pdf
FCE,,# 6,F0.5,41.1,Bi-LSTM,-,Grammatical Error Detection,Compositional Sequence Labeling Models for Error Detection in Learner Writing,/paper/compositional-sequence-labeling-models-for,https://arxiv.org/pdf/1607.06153v1.pdf
TuSimple,,# 1,Accuracy,96.53%,Spatial CNN,-,Lane Detection,Spatial As Deep: Spatial CNN for Traffic Scene Understanding,/paper/spatial-as-deep-spatial-cnn-for-traffic-scene,https://arxiv.org/pdf/1712.06080v1.pdf
CACDVS,,# 4,Accuracy,99.13%,DeepVisage,-,Age-Invariant Face Recognition,DeepVisage: Making face recognition simple yet with powerful generalization skills,/paper/deepvisage-making-face-recognition-simple-yet,https://arxiv.org/pdf/1703.08388v2.pdf
CNN / Daily Mail,,# 2,PPL,32.75,Bottom-Up Sum,-,Document Summarization,Bottom-Up Abstractive Summarization,/paper/bottom-up-abstractive-summarization,https://arxiv.org/pdf/1808.10792v2.pdf
CNN / Daily Mail,,# 2,ROUGE-1,41.22,Bottom-Up Sum,-,Document Summarization,Bottom-Up Abstractive Summarization,/paper/bottom-up-abstractive-summarization,https://arxiv.org/pdf/1808.10792v2.pdf
CNN / Daily Mail,,# 2,ROUGE-2,18.68,Bottom-Up Sum,-,Document Summarization,Bottom-Up Abstractive Summarization,/paper/bottom-up-abstractive-summarization,https://arxiv.org/pdf/1808.10792v2.pdf
CNN / Daily Mail,,# 2,ROUGE-L,38.34,Bottom-Up Sum,-,Document Summarization,Bottom-Up Abstractive Summarization,/paper/bottom-up-abstractive-summarization,https://arxiv.org/pdf/1808.10792v2.pdf
SNLI,,# 24,% Test Accuracy,86.6,600D BiLSTM with generalized pooling,-,Natural Language Inference,Enhancing Sentence Embedding with Generalized Pooling,/paper/enhancing-sentence-embedding-with-generalized,https://arxiv.org/pdf/1806.09828v1.pdf
SNLI,,# 10,% Train Accuracy,94.9,600D BiLSTM with generalized pooling,-,Natural Language Inference,Enhancing Sentence Embedding with Generalized Pooling,/paper/enhancing-sentence-embedding-with-generalized,https://arxiv.org/pdf/1806.09828v1.pdf
SNLI,,# 1,Parameters,65m,600D BiLSTM with generalized pooling,-,Natural Language Inference,Enhancing Sentence Embedding with Generalized Pooling,/paper/enhancing-sentence-embedding-with-generalized,https://arxiv.org/pdf/1806.09828v1.pdf
Yelp Fine-grained classification,,# 7,Error,33.45,BiLSTM generalized pooling,-,Sentiment Analysis,Enhancing Sentence Embedding with Generalized Pooling,/paper/enhancing-sentence-embedding-with-generalized,https://arxiv.org/pdf/1806.09828v1.pdf
SNLI,,# 31,% Test Accuracy,85.6,300D Gumbel TreeLSTM encoders,-,Natural Language Inference,Learning to Compose Task-Specific Tree Structures,/paper/learning-to-compose-task-specific-tree,https://arxiv.org/pdf/1707.02786v4.pdf
SNLI,,# 26,% Train Accuracy,91.2,300D Gumbel TreeLSTM encoders,-,Natural Language Inference,Learning to Compose Task-Specific Tree Structures,/paper/learning-to-compose-task-specific-tree,https://arxiv.org/pdf/1707.02786v4.pdf
SNLI,,# 1,Parameters,2.9m,300D Gumbel TreeLSTM encoders,-,Natural Language Inference,Learning to Compose Task-Specific Tree Structures,/paper/learning-to-compose-task-specific-tree,https://arxiv.org/pdf/1707.02786v4.pdf
SNLI,,# 28,% Test Accuracy,86.0,600D Gumbel TreeLSTM encoders,-,Natural Language Inference,Learning to Compose Task-Specific Tree Structures,/paper/learning-to-compose-task-specific-tree,https://arxiv.org/pdf/1707.02786v4.pdf
SNLI,,# 18,% Train Accuracy,93.1,600D Gumbel TreeLSTM encoders,-,Natural Language Inference,Learning to Compose Task-Specific Tree Structures,/paper/learning-to-compose-task-specific-tree,https://arxiv.org/pdf/1707.02786v4.pdf
SNLI,,# 1,Parameters,10m,600D Gumbel TreeLSTM encoders,-,Natural Language Inference,Learning to Compose Task-Specific Tree Structures,/paper/learning-to-compose-task-specific-tree,https://arxiv.org/pdf/1707.02786v4.pdf
IJB-A,,# 1,TAR @ FAR=0.01,97.60%,Dual-Agent GANs,-,Face Verification,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis,/paper/dual-agent-gans-for-photorealistic-and,https://papers.nips.cc/paper/6612-dual-agent-gans-for-photorealistic-and-identity-preserving-profile-face-synthesis.pdf
CelebA,,# 2,FID,7.3,PGGAN,-,Image Generation,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",/paper/progressive-growing-of-gans-for-improved,https://arxiv.org/pdf/1710.10196v3.pdf
CelebA-HQ 1024x1024,,# 3,FID,7.3,PGGAN,-,Image Generation,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",/paper/progressive-growing-of-gans-for-improved,https://arxiv.org/pdf/1710.10196v3.pdf
CIFAR-10,,# 1,Inception score,8.8,PGGAN,-,Image Generation,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",/paper/progressive-growing-of-gans-for-improved,https://arxiv.org/pdf/1710.10196v3.pdf
FFHQ,,# 3,FID,8.04,PGGAN,-,Image Generation,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",/paper/progressive-growing-of-gans-for-improved,https://arxiv.org/pdf/1710.10196v3.pdf
LSUN Bedroom 256 x 256,,# 3,FID,8.34,PGGAN,-,Image Generation,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",/paper/progressive-growing-of-gans-for-improved,https://arxiv.org/pdf/1710.10196v3.pdf
bAbi,,# 4,Accuracy (trained on 10k),93.4%,End-To-End Memory Networks,-,Question Answering,End-To-End Memory Networks,/paper/end-to-end-memory-networks,https://arxiv.org/pdf/1503.08895v5.pdf
bAbi,,# 3,Accuracy (trained on 1k),86.1%,End-To-End Memory Networks,-,Question Answering,End-To-End Memory Networks,/paper/end-to-end-memory-networks,https://arxiv.org/pdf/1503.08895v5.pdf
bAbi,,# 5,Mean Error Rate,7.5%,End-To-End Memory Networks,-,Question Answering,End-To-End Memory Networks,/paper/end-to-end-memory-networks,https://arxiv.org/pdf/1503.08895v5.pdf
HMDB51,,# 1,Accuracy,80.9,Two-Stream I3D,-,Action Recognition,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",/paper/quo-vadis-action-recognition-a-new-model-and,https://arxiv.org/pdf/1705.07750v3.pdf
UCF101,,# 1,Accuracy,93.4,Two-Stream I3D,-,Action Recognition,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",/paper/quo-vadis-action-recognition-a-new-model-and,https://arxiv.org/pdf/1705.07750v3.pdf
WikiHop,,# 2,Test,59.3,Coref-GRU,-,Question Answering,Neural Models for Reasoning over Multiple Mentions using Coreference,/paper/neural-models-for-reasoning-over-multiple,https://arxiv.org/pdf/1804.05922v1.pdf
WMT2016 English-German,,# 4,BLEU score,10.86,Unsupervised NMT + weight-sharing,-,Machine Translation,Unsupervised Neural Machine Translation with Weight Sharing,/paper/unsupervised-neural-machine-translation-with-1,https://arxiv.org/pdf/1804.09057v1.pdf
WMT2016 German-English,,# 4,BLEU score,14.62,Unsupervised NMT + weight-sharing,-,Machine Translation,Unsupervised Neural Machine Translation with Weight Sharing,/paper/unsupervised-neural-machine-translation-with-1,https://arxiv.org/pdf/1804.09057v1.pdf
Cityscapes,,# 6,Mean IoU,80.6%,ResNet-38,-,Semantic Segmentation,Wider or Deeper: Revisiting the ResNet Model for Visual Recognition,/paper/wider-or-deeper-revisiting-the-resnet-model,https://arxiv.org/pdf/1611.10080v1.pdf
PASCAL VOC 2012,,# 6,Mean IoU,84.9%,ResNet-38 MS COCO,-,Semantic Segmentation,Wider or Deeper: Revisiting the ResNet Model for Visual Recognition,/paper/wider-or-deeper-revisiting-the-resnet-model,https://arxiv.org/pdf/1611.10080v1.pdf
LineMOD,,# 3,Accuracy,90.37%,Single-shot deep CNN,-,6D Pose Estimation,Real-Time Seamless Single Shot 6D Object Pose Prediction,/paper/real-time-seamless-single-shot-6d-object-pose,https://arxiv.org/pdf/1711.08848v5.pdf
OCCLUSION,,# 1,MAP,0.48,Single-shot deep CNN,-,6D Pose Estimation,Real-Time Seamless Single Shot 6D Object Pose Prediction,/paper/real-time-seamless-single-shot-6d-object-pose,https://arxiv.org/pdf/1711.08848v5.pdf
DukeMTMC-reID,,# 1,Rank-1,89.0,Parameter-Free Spatial Attention,-,Person Re-Identification,Parameter-Free Spatial Attention Network for Person Re-Identification,/paper/parameter-free-spatial-attention-network-for,https://arxiv.org/pdf/1811.12150v1.pdf
DukeMTMC-reID,,# 1,MAP,85.9,Parameter-Free Spatial Attention,-,Person Re-Identification,Parameter-Free Spatial Attention Network for Person Re-Identification,/paper/parameter-free-spatial-attention-network-for,https://arxiv.org/pdf/1811.12150v1.pdf
Market-1501,,# 2,Rank-1,94.7,Parameter-Free Spatial Attention,-,Person Re-Identification,Parameter-Free Spatial Attention Network for Person Re-Identification,/paper/parameter-free-spatial-attention-network-for,https://arxiv.org/pdf/1811.12150v1.pdf
Market-1501,,# 1,MAP,91.7,Parameter-Free Spatial Attention,-,Person Re-Identification,Parameter-Free Spatial Attention Network for Person Re-Identification,/paper/parameter-free-spatial-attention-network-for,https://arxiv.org/pdf/1811.12150v1.pdf
CIFAR-10,,# 3,Inception score,8.22,SN-GANs,-,Image Generation,Spectral Normalization for Generative Adversarial Networks,/paper/spectral-normalization-for-generative,https://arxiv.org/pdf/1802.05957v1.pdf
CIFAR-10,,# 2,FID,21.7,SN-GANs,-,Image Generation,Spectral Normalization for Generative Adversarial Networks,/paper/spectral-normalization-for-generative,https://arxiv.org/pdf/1802.05957v1.pdf
TIMIT,,# 12,Percentage error,17.7,Bi-LSTM + skip connections w/ CTC,-,Speech Recognition,Speech Recognition with Deep Recurrent Neural Networks,/paper/speech-recognition-with-deep-recurrent-neural,https://arxiv.org/pdf/1303.5778v1.pdf
Caltech,,# 3,Reasonable Miss Rate,4.1,OR-CNN + CityPersons dataset,-,Pedestrian Detection,Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd,/paper/occlusion-aware-r-cnn-detecting-pedestrians,https://arxiv.org/pdf/1807.08407v1.pdf
CityPersons,,# 3,Reasonable MR^-2,12.8,OR-CNN,-,Pedestrian Detection,Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd,/paper/occlusion-aware-r-cnn-detecting-pedestrians,https://arxiv.org/pdf/1807.08407v1.pdf
CityPersons,,# 5,Heavy MR^-2,55.7,OR-CNN,-,Pedestrian Detection,Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd,/paper/occlusion-aware-r-cnn-detecting-pedestrians,https://arxiv.org/pdf/1807.08407v1.pdf
CityPersons,,# 3,Partial MR^-2,15.3,OR-CNN,-,Pedestrian Detection,Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd,/paper/occlusion-aware-r-cnn-detecting-pedestrians,https://arxiv.org/pdf/1807.08407v1.pdf
CityPersons,,# 1,Bare MR^-2,6.7,OR-CNN,-,Pedestrian Detection,Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd,/paper/occlusion-aware-r-cnn-detecting-pedestrians,https://arxiv.org/pdf/1807.08407v1.pdf
CIFAR-10,,# 4,Model Entropy,3.1,VAE with IAF,-,Image Generation,Improved Variational Inference with Inverse Autoregressive Flow,/paper/improved-variational-inference-with-inverse,https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf
CoNLL 2005,,# 3,F1,87.6,BiLSTM-Span,-,Semantic Role Labeling,A Span Selection Model for Semantic Role Labeling,/paper/a-span-selection-model-for-semantic-role,https://arxiv.org/pdf/1810.02245v1.pdf
CoNLL 2005,,# 1,F1,88.5,BiLSTM-Span (Ensemble),-,Semantic Role Labeling,A Span Selection Model for Semantic Role Labeling,/paper/a-span-selection-model-for-semantic-role,https://arxiv.org/pdf/1810.02245v1.pdf
OntoNotes,,# 2,F1,86.2,BiLSTM-Span,-,Semantic Role Labeling,A Span Selection Model for Semantic Role Labeling,/paper/a-span-selection-model-for-semantic-role,https://arxiv.org/pdf/1810.02245v1.pdf
OntoNotes,,# 1,F1,87.0,BiLSTM-Span (Ensemble),-,Semantic Role Labeling,A Span Selection Model for Semantic Role Labeling,/paper/a-span-selection-model-for-semantic-role,https://arxiv.org/pdf/1810.02245v1.pdf
WN18,,# 6,MRR,0.941,ComplEx,-,Link Prediction,Complex Embeddings for Simple Link Prediction,/paper/complex-embeddings-for-simple-link-prediction,https://arxiv.org/pdf/1606.06357v1.pdf
WN18,,# 4,[emailÂ protected],0.9470000000000001,ComplEx,-,Link Prediction,Complex Embeddings for Simple Link Prediction,/paper/complex-embeddings-for-simple-link-prediction,https://arxiv.org/pdf/1606.06357v1.pdf
WN18,,# 4,[emailÂ protected],0.9359999999999999,ComplEx,-,Link Prediction,Complex Embeddings for Simple Link Prediction,/paper/complex-embeddings-for-simple-link-prediction,https://arxiv.org/pdf/1606.06357v1.pdf
WN18,,# 5,[emailÂ protected],0.9359999999999999,ComplEx,-,Link Prediction,Complex Embeddings for Simple Link Prediction,/paper/complex-embeddings-for-simple-link-prediction,https://arxiv.org/pdf/1606.06357v1.pdf
CR,,# 1,Accuracy,92.2,Block-sparse LSTM,-,Sentiment Analysis,GPU Kernels for Block-Sparse Weights,/paper/gpu-kernels-for-block-sparse-weights,https://s3-us-west-2.amazonaws.com/openai-assets/blocksparse/blocksparsepaper.pdf
IMDb,,# 5,Accuracy,94.99,Block-sparse LSTM,-,Sentiment Analysis,GPU Kernels for Block-Sparse Weights,/paper/gpu-kernels-for-block-sparse-weights,https://s3-us-west-2.amazonaws.com/openai-assets/blocksparse/blocksparsepaper.pdf
SST-2 Binary classification,,# 3,Accuracy,93.2,Block-sparse LSTM,-,Sentiment Analysis,GPU Kernels for Block-Sparse Weights,/paper/gpu-kernels-for-block-sparse-weights,https://s3-us-west-2.amazonaws.com/openai-assets/blocksparse/blocksparsepaper.pdf
Yelp Binary classification,,# 7,Error,3.27,Block-sparse LSTM,-,Sentiment Analysis,GPU Kernels for Block-Sparse Weights,/paper/gpu-kernels-for-block-sparse-weights,https://s3-us-west-2.amazonaws.com/openai-assets/blocksparse/blocksparsepaper.pdf
DUC 2004 Task 1,,# 2,ROUGE-1,32.28,EndDec+WFE,-,Text Summarization,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,/paper/cutting-off-redundant-repeating-generations-1,https://aclweb.org/anthology/E17-2047
DUC 2004 Task 1,,# 3,ROUGE-2,10.54,EndDec+WFE,-,Text Summarization,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,/paper/cutting-off-redundant-repeating-generations-1,https://aclweb.org/anthology/E17-2047
DUC 2004 Task 1,,# 2,ROUGE-L,27.8,EndDec+WFE,-,Text Summarization,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,/paper/cutting-off-redundant-repeating-generations-1,https://aclweb.org/anthology/E17-2047
GigaWord,,# 4,ROUGE-1,36.3,EndDec+WFE,-,Text Summarization,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,/paper/cutting-off-redundant-repeating-generations-1,https://aclweb.org/anthology/E17-2047
GigaWord,,# 9,ROUGE-2,17.31,EndDec+WFE,-,Text Summarization,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,/paper/cutting-off-redundant-repeating-generations-1,https://aclweb.org/anthology/E17-2047
GigaWord,,# 4,ROUGE-L,33.88,EndDec+WFE,-,Text Summarization,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,/paper/cutting-off-redundant-repeating-generations-1,https://aclweb.org/anthology/E17-2047
300W,,# 2,NME,6.3,Pose-Invariant,-,Facial Landmark Detection,Pose-Invariant Face Alignment with a Single CNN,/paper/pose-invariant-face-alignment-with-a-single,https://arxiv.org/pdf/1707.06286v1.pdf
SST-2 Binary classification,,# 16,Accuracy,86.99,DC-MCNN,-,Sentiment Analysis,A Helping Hand: Transfer Learning for Deep Sentiment Analysis,/paper/a-helping-hand-transfer-learning-for-deep,https://aclweb.org/anthology/P18-1235
FDDB,,# 2,AP,0.99,Face R-FCN,-,Face Detection,Detecting Faces Using Region-based Fully Convolutional Networks,/paper/detecting-faces-using-region-based-fully,https://arxiv.org/pdf/1709.05256v2.pdf
WIDER Face (Easy),,# 5,AP,0.943,Face R-FCN,-,Face Detection,Detecting Faces Using Region-based Fully Convolutional Networks,/paper/detecting-faces-using-region-based-fully,https://arxiv.org/pdf/1709.05256v2.pdf
WIDER Face (Hard),,# 6,AP,0.8759999999999999,Face R-FCN,-,Face Detection,Detecting Faces Using Region-based Fully Convolutional Networks,/paper/detecting-faces-using-region-based-fully,https://arxiv.org/pdf/1709.05256v2.pdf
WIDER Face (Medium),,# 5,AP,0.9309999999999999,Face R-FCN,-,Face Detection,Detecting Faces Using Region-based Fully Convolutional Networks,/paper/detecting-faces-using-region-based-fully,https://arxiv.org/pdf/1709.05256v2.pdf
CIFAR-10,,# 14,Percentage correct,95.6,Evolution ensemble,-,Image Classification,Large-Scale Evolution of Image Classifiers,/paper/large-scale-evolution-of-image-classifiers,https://arxiv.org/pdf/1703.01041v2.pdf
CIFAR-10,,# 18,Percentage correct,94.6,Evolution,-,Image Classification,Large-Scale Evolution of Image Classifiers,/paper/large-scale-evolution-of-image-classifiers,https://arxiv.org/pdf/1703.01041v2.pdf
CIFAR-100,,# 13,Percentage correct,77.0,Evolution,-,Image Classification,Large-Scale Evolution of Image Classifiers,/paper/large-scale-evolution-of-image-classifiers,https://arxiv.org/pdf/1703.01041v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 19,Restaurant (Acc),79.11,PRET+MULT,-,Aspect-Based Sentiment Analysis,Exploiting Document Knowledge for Aspect-level Sentiment Classification,/paper/exploiting-document-knowledge-for-aspect,https://arxiv.org/pdf/1806.04346v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 5,Laptop (Acc),71.15,PRET+MULT,-,Aspect-Based Sentiment Analysis,Exploiting Document Knowledge for Aspect-level Sentiment Classification,/paper/exploiting-document-knowledge-for-aspect,https://arxiv.org/pdf/1806.04346v1.pdf
GigaWord,,# 2,ROUGE-1,37.04,Seq2seq + E2T_cnn,-,Text Summarization,Entity Commonsense Representation for Neural Abstractive Summarization,/paper/entity-commonsense-representation-for-neural-1,https://aclweb.org/anthology/N18-1064
GigaWord,,# 11,ROUGE-2,16.66,Seq2seq + E2T_cnn,-,Text Summarization,Entity Commonsense Representation for Neural Abstractive Summarization,/paper/entity-commonsense-representation-for-neural-1,https://aclweb.org/anthology/N18-1064
GigaWord,,# 1,ROUGE-L,34.93,Seq2seq + E2T_cnn,-,Text Summarization,Entity Commonsense Representation for Neural Abstractive Summarization,/paper/entity-commonsense-representation-for-neural-1,https://aclweb.org/anthology/N18-1064
WN18RR,,# 5,MRR,0.43700000000000006,M-Walk,-,Link Prediction,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,/paper/m-walk-learning-to-walk-over-graphs-using,https://arxiv.org/pdf/1802.04394v5.pdf
WN18RR,,# 3,[emailÂ protected],0.445,M-Walk,-,Link Prediction,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,/paper/m-walk-learning-to-walk-over-graphs-using,https://arxiv.org/pdf/1802.04394v5.pdf
WN18RR,,# 4,[emailÂ protected],0.414,M-Walk,-,Link Prediction,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,/paper/m-walk-learning-to-walk-over-graphs-using,https://arxiv.org/pdf/1802.04394v5.pdf
COCO,,# 1,Inception score,25.89,AttnGAN,-,Text-to-Image Generation,AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks,/paper/attngan-fine-grained-text-to-image-generation,https://arxiv.org/pdf/1711.10485v1.pdf
CUB,,# 1,Inception score,4.36,AttnGAN,-,Text-to-Image Generation,AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks,/paper/attngan-fine-grained-text-to-image-generation,https://arxiv.org/pdf/1711.10485v1.pdf
LDC2014T12:,,# 1,F1 Newswire,0.73,Transition-based+improved aligner+ensemble,-,Amr Parsing,An AMR Aligner Tuned by Transition-based Parser,/paper/an-amr-aligner-tuned-by-transition-based,https://arxiv.org/pdf/1810.03541v1.pdf
LDC2014T12:,,# 1,F1 Full,0.68,Transition-based+improved aligner+ensemble,-,Amr Parsing,An AMR Aligner Tuned by Transition-based Parser,/paper/an-amr-aligner-tuned-by-transition-based,https://arxiv.org/pdf/1810.03541v1.pdf
Cityscapes,,# 2,Mean IoU,82.0%,Mapillary,-,Semantic Segmentation,In-Place Activated BatchNorm for Memory-Optimized Training of DNNs,/paper/in-place-activated-batchnorm-for-memory,https://arxiv.org/pdf/1712.02616v3.pdf
UD,,# 3,Avg accuracy,95.55,Joint Bi-LSTM,-,Part-Of-Speech Tagging,A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing,/paper/a-novel-neural-network-model-for-joint-pos,https://arxiv.org/pdf/1705.05952v2.pdf
VggFace2,,# 1,PSNR,27.81,SymmFCNet (Full),-,Facial Inpainting,Learning Symmetry Consistent Deep CNNs for Face Completion,/paper/learning-symmetry-consistent-deep-cnns-for,https://arxiv.org/pdf/1812.07741v1.pdf
WebFace,,# 1,PSNR,27.22,SymmFCNet (Full),-,Facial Inpainting,Learning Symmetry Consistent Deep CNNs for Face Completion,/paper/learning-symmetry-consistent-deep-cnns-for,https://arxiv.org/pdf/1812.07741v1.pdf
VOT2017/18,,# 8,Expected Average Overlap (EAO),0.263,CSRDCF,-,Visual Object Tracking,Discriminative Correlation Filter with Channel and Spatial Reliability,/paper/discriminative-correlation-filter-with,https://arxiv.org/pdf/1611.08461v3.pdf
CIHP,,# 1,Mean IoU,61.1,Parsing R-CNN + ResNext101,-,Human Part Segmentation,Parsing R-CNN for Instance-Level Human Analysis,/paper/parsing-r-cnn-for-instance-level-human,https://arxiv.org/pdf/1811.12596v1.pdf
DensePose-COCO,,# 1,AP,61.6,Parsing R-CNN + ResNext101,-,Pose Estimation,Parsing R-CNN for Instance-Level Human Analysis,/paper/parsing-r-cnn-for-instance-level-human,https://arxiv.org/pdf/1811.12596v1.pdf
MHP v2.0,,# 1,Mean IoU,41.8,Parsing R-CNN + ResNext101,-,Human Part Segmentation,Parsing R-CNN for Instance-Level Human Analysis,/paper/parsing-r-cnn-for-instance-level-human,https://arxiv.org/pdf/1811.12596v1.pdf
SARC (all-bal),,# 1,Accuracy,77,CASCADE,-,Sarcasm Detection,CASCADE: Contextual Sarcasm Detection in Online Discussion Forums,/paper/cascade-contextual-sarcasm-detection-in,https://arxiv.org/pdf/1805.06413v1.pdf
SARC (pol-bal),,# 2,Accuracy,74,CASCADE,-,Sarcasm Detection,CASCADE: Contextual Sarcasm Detection in Online Discussion Forums,/paper/cascade-contextual-sarcasm-detection-in,https://arxiv.org/pdf/1805.06413v1.pdf
MNIST,,# 5,Accuracy,95,InfoGAN,-,Unsupervised MNIST,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,/paper/infogan-interpretable-representation-learning,https://arxiv.org/pdf/1606.03657v1.pdf
MNIST,,# 5,Accuracy,95,InfoGAN,-,Unsupervised image classification,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,/paper/infogan-interpretable-representation-learning,https://arxiv.org/pdf/1606.03657v1.pdf
Sentihood,,# 1,Aspect,87.9,BERT-pair-QA-B,-,Aspect-Based Sentiment Analysis,Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence,/paper/utilizing-bert-for-aspect-based-sentiment,https://arxiv.org/pdf/1903.09588v1.pdf
Sentihood,,# 2,Sentiment,93.3,BERT-pair-QA-B,-,Aspect-Based Sentiment Analysis,Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence,/paper/utilizing-bert-for-aspect-based-sentiment,https://arxiv.org/pdf/1903.09588v1.pdf
Sentihood,,# 2,Aspect,86.4,BERT-pair-QA-M,-,Aspect-Based Sentiment Analysis,Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence,/paper/utilizing-bert-for-aspect-based-sentiment,https://arxiv.org/pdf/1903.09588v1.pdf
Sentihood,,# 1,Sentiment,93.6,BERT-pair-QA-M,-,Aspect-Based Sentiment Analysis,Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence,/paper/utilizing-bert-for-aspect-based-sentiment,https://arxiv.org/pdf/1903.09588v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 21,Restaurant (Acc),78.6,IAN,-,Aspect-Based Sentiment Analysis,Interactive Attention Networks for Aspect-Level Sentiment Classification,/paper/interactive-attention-networks-for-aspect,https://arxiv.org/pdf/1709.00893v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 8,Laptop (Acc),72.1,IAN,-,Aspect-Based Sentiment Analysis,Interactive Attention Networks for Aspect-Level Sentiment Classification,/paper/interactive-attention-networks-for-aspect,https://arxiv.org/pdf/1709.00893v1.pdf
SQuAD1.1,,# 56,EM,76.99600000000001,Conductor-net (ensemble),-,Question Answering,Phase Conductor on Multi-layered Attentions for Machine Comprehension,/paper/phase-conductor-on-multi-layered-attentions,https://arxiv.org/pdf/1710.10504v2.pdf
SQuAD1.1,,# 55,F1,84.63,Conductor-net (ensemble),-,Question Answering,Phase Conductor on Multi-layered Attentions for Machine Comprehension,/paper/phase-conductor-on-multi-layered-attentions,https://arxiv.org/pdf/1710.10504v2.pdf
SQuAD1.1,,# 77,EM,74.405,Conductor-net (single model),-,Question Answering,Phase Conductor on Multi-layered Attentions for Machine Comprehension,/paper/phase-conductor-on-multi-layered-attentions,https://arxiv.org/pdf/1710.10504v2.pdf
SQuAD1.1,,# 74,F1,82.742,Conductor-net (single model),-,Question Answering,Phase Conductor on Multi-layered Attentions for Machine Comprehension,/paper/phase-conductor-on-multi-layered-attentions,https://arxiv.org/pdf/1710.10504v2.pdf
SQuAD1.1,,# 87,EM,73.24,Conductor-net (single),-,Question Answering,Phase Conductor on Multi-layered Attentions for Machine Comprehension,/paper/phase-conductor-on-multi-layered-attentions,https://arxiv.org/pdf/1710.10504v2.pdf
SQuAD1.1,,# 81,F1,81.93299999999999,Conductor-net (single),-,Question Answering,Phase Conductor on Multi-layered Attentions for Machine Comprehension,/paper/phase-conductor-on-multi-layered-attentions,https://arxiv.org/pdf/1710.10504v2.pdf
CHiME-4 real 6ch,,# 1,Word Error Rate (WER),2.74,HMM-TDNN(LFMMI) + LSTMLM + NN-GEV,-,Distant Speech Recognition,Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline,/paper/building-state-of-the-art-distant-speech,https://arxiv.org/pdf/1803.10109v1.pdf
CHiME real,,# 1,Percentage error,11.4,HMM-TDNN(LFMMI) + LSTMLM,-,Noisy Speech Recognition,Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline,/paper/building-state-of-the-art-distant-speech,https://arxiv.org/pdf/1803.10109v1.pdf
Cora,,# 3,Accuracy,83.0%,GAT,-,Document Classification,Graph Attention Networks,/paper/graph-attention-networks,https://arxiv.org/pdf/1710.10903v3.pdf
Cora,,# 4,Accuracy,81.7%,MoNet,-,Document Classification,Geometric deep learning on graphs and manifolds using mixture model CNNs,/paper/geometric-deep-learning-on-graphs-and,https://arxiv.org/pdf/1611.08402v3.pdf
CIFAR-10,,# 50,Percentage correct,84.9,Stochastic Pooling,-,Image Classification,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,/paper/stochastic-pooling-for-regularization-of-deep,https://arxiv.org/pdf/1301.3557v1.pdf
CIFAR-100,,# 45,Percentage correct,57.5,Stochastic Pooling,-,Image Classification,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,/paper/stochastic-pooling-for-regularization-of-deep,https://arxiv.org/pdf/1301.3557v1.pdf
SVHN,,# 20,Percentage error,2.8,Stochastic Pooling,-,Image Classification,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,/paper/stochastic-pooling-for-regularization-of-deep,https://arxiv.org/pdf/1301.3557v1.pdf
Stanford Cars,,# 1,[emailÂ protected],96.1,ABE-8-512,-,Image Retrieval,Attention-based Ensemble for Deep Metric Learning,/paper/attention-based-ensemble-for-deep-metric,https://arxiv.org/pdf/1804.00382v2.pdf
NewsQA,,# 3,F1,63.2,MINIMAL(Dyn),-,Question Answering,Efficient and Robust Question Answering from Minimal Context over Documents,/paper/efficient-and-robust-question-answering-from,https://arxiv.org/pdf/1805.08092v1.pdf
NewsQA,,# 2,EM,50.1,MINIMAL(Dyn),-,Question Answering,Efficient and Robust Question Answering from Minimal Context over Documents,/paper/efficient-and-robust-question-answering-from,https://arxiv.org/pdf/1805.08092v1.pdf
COCO,,# 2,AP,0.6970000000000001,Pose Residual Network,-,Multi-Person Pose Estimation,MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network,/paper/multiposenet-fast-multi-person-pose,https://arxiv.org/pdf/1807.04067v1.pdf
COCO,,# 4,Validation AP,69.6,Pose Residual Network,-,Keypoint Detection,MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network,/paper/multiposenet-fast-multi-person-pose,https://arxiv.org/pdf/1807.04067v1.pdf
Barrettâs Esophagus,,# 2,Mean Accuracy,74%,Sliding Window,-,Medical Object Detection,Detecting Cancer Metastases on Gigapixel Pathology Images,/paper/detecting-cancer-metastases-on-gigapixel,https://arxiv.org/pdf/1703.02442v2.pdf
MNIST,,# 4,Percentage error,0.4,C-SVDDNet,-,Image Classification,Unsupervised Feature Learning with C-SVDDNet,/paper/unsupervised-feature-learning-with-c-svddnet,https://arxiv.org/pdf/1412.7259v3.pdf
STL-10,,# 10,Percentage correct,68.23,C-SVDDNet,-,Image Classification,Unsupervised Feature Learning with C-SVDDNet,/paper/unsupervised-feature-learning-with-c-svddnet,https://arxiv.org/pdf/1412.7259v3.pdf
CCGBank,,# 2,Accuracy,94.7,Lewis et al.,-,CCG Supertagging,LSTM CCG Parsing,/paper/lstm-ccg-parsing,https://aclweb.org/anthology/N16-1026
AG News,,# 15,Error,9.51,Char-level CNN,-,Text Classification,Character-level Convolutional Networks for Text Classification,/paper/character-level-convolutional-networks-for,https://arxiv.org/pdf/1509.01626v3.pdf
DBpedia,,# 16,Error,1.55,Char-level CNN,-,Text Classification,Character-level Convolutional Networks for Text Classification,/paper/character-level-convolutional-networks-for,https://arxiv.org/pdf/1509.01626v3.pdf
Yelp Binary classification,,# 15,Error,4.88,Char-level CNN,-,Sentiment Analysis,Character-level Convolutional Networks for Text Classification,/paper/character-level-convolutional-networks-for,https://arxiv.org/pdf/1509.01626v3.pdf
Yelp Fine-grained classification,,# 12,Error,37.95,Char-level CNN,-,Sentiment Analysis,Character-level Convolutional Networks for Text Classification,/paper/character-level-convolutional-networks-for,https://arxiv.org/pdf/1509.01626v3.pdf
Atari-57,,# 3,Medium Human-Normalized Score,187.0%,Reactor,-,Atari Games,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,/paper/the-reactor-a-fast-and-sample-efficient-actor,https://arxiv.org/pdf/1704.04651v2.pdf
CIFAR-10,,# 7,NLL Test,3.47,RIDE,-,Image Generation,Generative Image Modeling Using Spatial LSTMs,/paper/generative-image-modeling-using-spatial-lstms,https://arxiv.org/pdf/1506.03478v2.pdf
AFLW2000,,# 2,Error rate,5.38,MCL,-,Face Alignment,Deep Multi-Center Learning for Face Alignment,/paper/deep-multi-center-learning-for-face-alignment,https://arxiv.org/pdf/1808.01558v2.pdf
SST-2 Binary classification,,# 12,Accuracy,87.8,C-LSTM,-,Sentiment Analysis,A C-LSTM Neural Network for Text Classification,/paper/a-c-lstm-neural-network-for-text,https://arxiv.org/pdf/1511.08630v2.pdf
SST-5 Fine-grained classification,,# 11,Accuracy,49.2,C-LSTM,-,Sentiment Analysis,A C-LSTM Neural Network for Text Classification,/paper/a-c-lstm-neural-network-for-text,https://arxiv.org/pdf/1511.08630v2.pdf
TREC-6,,# 6,Error,5.4,C-LSTM,-,Text Classification,A C-LSTM Neural Network for Text Classification,/paper/a-c-lstm-neural-network-for-text,https://arxiv.org/pdf/1511.08630v2.pdf
ShanghaiTech A,,# 1,MAE,101.3,Cascaded-MTL,-,Crowd Counting,CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting,/paper/cnn-based-cascaded-multi-task-learning-of,https://arxiv.org/pdf/1707.09605v2.pdf
ShanghaiTech B,,# 1,MAE,20.0,Cascaded-MTL,-,Crowd Counting,CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting,/paper/cnn-based-cascaded-multi-task-learning-of,https://arxiv.org/pdf/1707.09605v2.pdf
UCF CC 50,,# 1,MAE,322.8,Cascaded-MTL,-,Crowd Counting,CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting,/paper/cnn-based-cascaded-multi-task-learning-of,https://arxiv.org/pdf/1707.09605v2.pdf
HPatches,,# 2,Viewpoint I AEPE,4.43,PWC-Net,-,Dense Pixel Correspondence Estimation,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",/paper/pwc-net-cnns-for-optical-flow-using-pyramid,https://arxiv.org/pdf/1709.02371v3.pdf
HPatches,,# 3,Viewpoint II AEPE,11.44,PWC-Net,-,Dense Pixel Correspondence Estimation,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",/paper/pwc-net-cnns-for-optical-flow-using-pyramid,https://arxiv.org/pdf/1709.02371v3.pdf
HPatches,,# 3,Viewpoint III AEPE,15.47,PWC-Net,-,Dense Pixel Correspondence Estimation,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",/paper/pwc-net-cnns-for-optical-flow-using-pyramid,https://arxiv.org/pdf/1709.02371v3.pdf
HPatches,,# 3,Viewpoint IV AEPE,20.17,PWC-Net,-,Dense Pixel Correspondence Estimation,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",/paper/pwc-net-cnns-for-optical-flow-using-pyramid,https://arxiv.org/pdf/1709.02371v3.pdf
HPatches,,# 3,Viewpoint V AEPE,28.3,PWC-Net,-,Dense Pixel Correspondence Estimation,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",/paper/pwc-net-cnns-for-optical-flow-using-pyramid,https://arxiv.org/pdf/1709.02371v3.pdf
MNIST,,# 13,Percentage error,1.4,Convolutional Clustering,-,Image Classification,Convolutional Clustering for Unsupervised Learning,/paper/convolutional-clustering-for-unsupervised,https://arxiv.org/pdf/1511.06241v2.pdf
STL-10,,# 5,Percentage correct,74.1,Convolutional Clustering,-,Image Classification,Convolutional Clustering for Unsupervised Learning,/paper/convolutional-clustering-for-unsupervised,https://arxiv.org/pdf/1511.06241v2.pdf
COCO,,# 7,Bounding Box AP,45.1,CenterNet-HG (multi-scale),-,Object Detection,Objects as Points,/paper/objects-as-points,https://arxiv.org/pdf/1904.07850v2.pdf
COCO,,# 20,Bounding Box AP,41.6,CenterNet-DLA,-,Object Detection,Objects as Points,/paper/objects-as-points,https://arxiv.org/pdf/1904.07850v2.pdf
COCO,,# 5,Test AP,63.0,HG-jd,-,Keypoint Detection,Objects as Points,/paper/objects-as-points,https://arxiv.org/pdf/1904.07850v2.pdf
Story Cloze Test,,# 3,Accuracy,77.6,Hidden Coherence Model,-,Question Answering,Story Comprehension for Predicting What Happens Next,/paper/story-comprehension-for-predicting-what,https://aclweb.org/anthology/D17-1168
Company*,,# 2,AUC,0.8683,FNN,-,Click-Through Rate Prediction,Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction,/paper/deep-learning-over-multi-field-categorical,https://arxiv.org/pdf/1601.02376v1.pdf
Company*,,# 2,Log Loss,0.026289999999999997,FNN,-,Click-Through Rate Prediction,Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction,/paper/deep-learning-over-multi-field-categorical,https://arxiv.org/pdf/1601.02376v1.pdf
Criteo,,# 7,AUC,0.7963,FNN,-,Click-Through Rate Prediction,Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction,/paper/deep-learning-over-multi-field-categorical,https://arxiv.org/pdf/1601.02376v1.pdf
Criteo,,# 7,Log Loss,0.45738,FNN,-,Click-Through Rate Prediction,Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction,/paper/deep-learning-over-multi-field-categorical,https://arxiv.org/pdf/1601.02376v1.pdf
iPinYou,,# 4,AUC,0.7619,FNN,-,Click-Through Rate Prediction,Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction,/paper/deep-learning-over-multi-field-categorical,https://arxiv.org/pdf/1601.02376v1.pdf
General,,# 8,MAP,1.36,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
General,,# 8,MRR,3.18,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
General,,# 8,[emailÂ protected],1.3,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
Medical domain,,# 8,MAP,0.91,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
Medical domain,,# 8,MRR,2.1,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
Medical domain,,# 8,[emailÂ protected],1.08,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
Music domain,,# 7,MAP,1.95,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
Music domain,,# 7,MRR,5.01,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
Music domain,,# 7,[emailÂ protected],2.15,balAPInc,-,Hypernym Discovery,Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection,/paper/hypernyms-under-siege-linguistically,https://arxiv.org/pdf/1612.04460v2.pdf
AG News,,# 8,Error,7.4,Capsule-B,-,Text Classification,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic,https://arxiv.org/pdf/1804.00538v4.pdf
CR,,# 4,Accuracy,85.1,Capsule-B,-,Sentiment Analysis,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic,https://arxiv.org/pdf/1804.00538v4.pdf
MR,,# 3,Accuracy,82.3,Capsule-B,-,Sentiment Analysis,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic,https://arxiv.org/pdf/1804.00538v4.pdf
SST-2 Binary classification,,# 17,Accuracy,86.8,Capsule-B,-,Sentiment Analysis,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic,https://arxiv.org/pdf/1804.00538v4.pdf
SUBJ,,# 5,Accuracy,93.8,Capsule-B,-,Subjectivity Analysis,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic,https://arxiv.org/pdf/1804.00538v4.pdf
TREC-6,,# 8,Error,7.2,Capsule-B,-,Text Classification,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic,https://arxiv.org/pdf/1804.00538v4.pdf
CIFAR-10,,# 43,Percentage correct,90.5,GP EI,-,Image Classification,Practical Bayesian Optimization of Machine Learning Algorithms,/paper/practical-bayesian-optimization-of-machine-1,https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf
AG News,,# 4,Error,6.57,CNN,-,Text Classification,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,/paper/supervised-and-semi-supervised-text,https://arxiv.org/pdf/1602.02373v2.pdf
DBpedia,,# 5,Error,0.84,CNN,-,Text Classification,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,/paper/supervised-and-semi-supervised-text,https://arxiv.org/pdf/1602.02373v2.pdf
IMDb,,# 6,Accuracy,94.1,oh-LSTM,-,Sentiment Analysis,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,/paper/supervised-and-semi-supervised-text,https://arxiv.org/pdf/1602.02373v2.pdf
Yelp Binary classification,,# 6,Error,2.9,CNN,-,Sentiment Analysis,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,/paper/supervised-and-semi-supervised-text,https://arxiv.org/pdf/1602.02373v2.pdf
Yelp Fine-grained classification,,# 6,Error,32.39,CNN,-,Sentiment Analysis,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,/paper/supervised-and-semi-supervised-text,https://arxiv.org/pdf/1602.02373v2.pdf
Florence,,# 3,Average 3D Error,1.82,itwmm,-,3D Face Reconstruction,"3D Face Morphable Models ""In-the-Wild""",/paper/3d-face-morphable-models-in-the-wild,https://arxiv.org/pdf/1701.05360v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 3,Restaurant (Acc),82.23,HAPN,-,Aspect-Based Sentiment Analysis,Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis,/paper/hierarchical-attention-based-position-aware,https://aclweb.org/anthology/K18-1018
SemEval 2014 Task 4 Sub Task 2,,# 23,Laptop (Acc),77.27,HAPN,-,Aspect-Based Sentiment Analysis,Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis,/paper/hierarchical-attention-based-position-aware,https://aclweb.org/anthology/K18-1018
AFLW2000,,# 1,Error rate,4.7,Nonlinear 3D Face Morphable Model,-,Face Alignment,Nonlinear 3D Face Morphable Model,/paper/nonlinear-3d-face-morphable-model,https://arxiv.org/pdf/1804.03786v3.pdf
COCO,,# 11,Bounding Box AP,44.2,M2Det,-,Object Detection,M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network,/paper/m2det-a-single-shot-object-detector-based-on,https://arxiv.org/pdf/1811.04533v3.pdf
CIFAR-10,,# 42,Percentage correct,90.6,DNN+Probabilistic Maxout,-,Image Classification,Improving Deep Neural Networks with Probabilistic Maxout Units,/paper/improving-deep-neural-networks-with,https://arxiv.org/pdf/1312.6116v2.pdf
CIFAR-100,,# 41,Percentage correct,61.9,DNN+Probabilistic Maxout,-,Image Classification,Improving Deep Neural Networks with Probabilistic Maxout Units,/paper/improving-deep-neural-networks-with,https://arxiv.org/pdf/1312.6116v2.pdf
IWSLT2015 German-English,,# 12,BLEU score,29.56,DCCL,-,Machine Translation,Compressing Word Embeddings via Deep Compositional Code Learning,/paper/compressing-word-embeddings-via-deep,https://arxiv.org/pdf/1711.01068v2.pdf
Cityscapes,,# 8,mIoU,63.1%,ENet + LovÃ¡sz-Softmax,-,Real-Time Semantic Segmentation,The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks,/paper/the-lovasz-softmax-loss-a-tractable-surrogate-2,https://arxiv.org/abs/1705.08790
Cityscapes,,# 1,Time (ms),13,ENet + LovÃ¡sz-Softmax,-,Real-Time Semantic Segmentation,The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks,/paper/the-lovasz-softmax-loss-a-tractable-surrogate-2,https://arxiv.org/abs/1705.08790
Cityscapes,,# 1,Frame (fps),76.9,ENet + LovÃ¡sz-Softmax,-,Real-Time Semantic Segmentation,The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks,/paper/the-lovasz-softmax-loss-a-tractable-surrogate-2,https://arxiv.org/abs/1705.08790
Cityscapes,,# 16,Mean IoU,63.06%,ENet + LovÃ¡sz-Softmax,-,Semantic Segmentation,The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks,/paper/the-lovasz-softmax-loss-a-tractable-surrogate-2,https://arxiv.org/abs/1705.08790
PASCAL VOC 2012,,# 11,Mean IoU,79.0%,Deeplab-v2 + LovÃ¡sz-Softmax,-,Semantic Segmentation,The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks,/paper/the-lovasz-softmax-loss-a-tractable-surrogate-2,https://arxiv.org/abs/1705.08790
Restricted,,# 5,F0.5,54.79,CNN Seq2Seq,-,Grammatical Error Correction,A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction,/paper/a-multilayer-convolutional-encoder-decoder,https://arxiv.org/pdf/1801.08831v1.pdf
Restricted,,# 6,F0.5,"70.14 (measured by Ge et al., 2018)",CNN Seq2Seq,-,Grammatical Error Correction,A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction,/paper/a-multilayer-convolutional-encoder-decoder,https://arxiv.org/pdf/1801.08831v1.pdf
_Restricted_,,# 3,GLEU,57.47,CNN Seq2Seq,-,Grammatical Error Correction,A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction,/paper/a-multilayer-convolutional-encoder-decoder,https://arxiv.org/pdf/1801.08831v1.pdf
CIFAR-10,,# 59,Percentage correct,78.7,PCANet,-,Image Classification,PCANet: A Simple Deep Learning Baseline for Image Classification?,/paper/pcanet-a-simple-deep-learning-baseline-for,https://arxiv.org/pdf/1404.3606v2.pdf
MNIST,,# 6,Percentage error,0.6,PCANet,-,Image Classification,PCANet: A Simple Deep Learning Baseline for Image Classification?,/paper/pcanet-a-simple-deep-learning-baseline-for,https://arxiv.org/pdf/1404.3606v2.pdf
Set14 - 4x upscaling,,# 26,PSNR,27.83,MSSRNet,-,Image Super-Resolution,Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module,/paper/single-image-super-resolution-with-dilated,https://arxiv.org/pdf/1707.07128v1.pdf
Set14 - 4x upscaling,,# 26,SSIM,0.7631,MSSRNet,-,Image Super-Resolution,Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module,/paper/single-image-super-resolution-with-dilated,https://arxiv.org/pdf/1707.07128v1.pdf
Set5 - 4x upscaling,,# 21,PSNR,31.1,MSSRNet,-,Image Super-Resolution,Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module,/paper/single-image-super-resolution-with-dilated,https://arxiv.org/pdf/1707.07128v1.pdf
Set5 - 4x upscaling,,# 25,SSIM,0.8777,MSSRNet,-,Image Super-Resolution,Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module,/paper/single-image-super-resolution-with-dilated,https://arxiv.org/pdf/1707.07128v1.pdf
TrecQA,,# 2,MAP,0.7588,PWIN,-,Question Answering,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,/paper/pairwise-word-interaction-modeling-with-deep,https://aclweb.org/anthology/N16-1108
TrecQA,,# 2,MRR,0.8219,PWIN,-,Question Answering,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,/paper/pairwise-word-interaction-modeling-with-deep,https://aclweb.org/anthology/N16-1108
WikiQA,,# 2,MAP,0.709,PWIM,-,Question Answering,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,/paper/pairwise-word-interaction-modeling-with-deep,https://aclweb.org/anthology/N16-1108
WikiQA,,# 3,MRR,0.7234,PWIM,-,Question Answering,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,/paper/pairwise-word-interaction-modeling-with-deep,https://aclweb.org/anthology/N16-1108
Indian Pines,,# 1,Overall Accuracy,96.77%,BASSNet,-,Hyperspectral Image Classification,BASS Net: Band-Adaptive Spectral-Spatial Feature Learning Neural Network for Hyperspectral Image Classification,/paper/bass-net-band-adaptive-spectral-spatial,https://arxiv.org/pdf/1612.00144v2.pdf
Pavia University,,# 2,Overall Accuracy,97.48%,BASSNet,-,Hyperspectral Image Classification,BASS Net: Band-Adaptive Spectral-Spatial Feature Learning Neural Network for Hyperspectral Image Classification,/paper/bass-net-band-adaptive-spectral-spatial,https://arxiv.org/pdf/1612.00144v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 13,Restaurant (Acc),80.95,MemNet,-,Aspect-Based Sentiment Analysis,Aspect Level Sentiment Classification with Deep Memory Network,/paper/aspect-level-sentiment-classification-with-1,https://arxiv.org/pdf/1605.08900v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 9,Laptop (Acc),72.21,MemNet,-,Aspect-Based Sentiment Analysis,Aspect Level Sentiment Classification with Deep Memory Network,/paper/aspect-level-sentiment-classification-with-1,https://arxiv.org/pdf/1605.08900v2.pdf
CIFAR-10,,# 60,Percentage correct,75.9,FLSCNN,-,Image Classification,Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network,/paper/enhanced-image-classification-with-a-fast,https://arxiv.org/pdf/1503.04596v3.pdf
MNIST,,# 4,Percentage error,0.4,FLSCNN,-,Image Classification,Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network,/paper/enhanced-image-classification-with-a-fast,https://arxiv.org/pdf/1503.04596v3.pdf
SVHN,,# 22,Percentage error,3.96,FLSCNN,-,Image Classification,Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network,/paper/enhanced-image-classification-with-a-fast,https://arxiv.org/pdf/1503.04596v3.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 2,Percentage correct,64.2,N2NMN,-,Visual Question Answering,Learning to Reason: End-to-End Module Networks for Visual Question Answering,/paper/learning-to-reason-end-to-end-module-networks,https://arxiv.org/pdf/1704.05526v3.pdf
ModelNet40,,# 3,Accuracy,84.5%,Variational Shape Learner,-,3D Object Recognition,Learning a Hierarchical Latent-Variable Model of 3D Shapes,/paper/learning-a-hierarchical-latent-variable-model,https://arxiv.org/pdf/1705.05994v4.pdf
JIGSAWS,,# 1,Accuracy,0.98,CNN,-,Surgical Skills Evaluation,Evaluating surgical skills from kinematic data using convolutional neural networks,/paper/evaluating-surgical-skills-from-kinematic,https://arxiv.org/pdf/1806.02750v1.pdf
Douban,,# 2,RMSE,0.7049,U-CFN,-,Collaborative Filtering,Hybrid Recommender System based on Autoencoders,/paper/hybrid-recommender-system-based-on,https://arxiv.org/pdf/1606.07659v3.pdf
Douban,,# 1,RMSE,0.6911,I-CFN,-,Collaborative Filtering,Hybrid Recommender System based on Autoencoders,/paper/hybrid-recommender-system-based-on,https://arxiv.org/pdf/1606.07659v3.pdf
MovieLens 10M,,# 6,RMSE,0.7954,U-CFN,-,Collaborative Filtering,Hybrid Recommender System based on Autoencoders,/paper/hybrid-recommender-system-based-on,https://arxiv.org/pdf/1606.07659v3.pdf
MovieLens 10M,,# 3,RMSE,0.7767,I-CFN,-,Collaborative Filtering,Hybrid Recommender System based on Autoencoders,/paper/hybrid-recommender-system-based-on,https://arxiv.org/pdf/1606.07659v3.pdf
MovieLens 1M,,# 7,RMSE,0.8574,U-CFN,-,Collaborative Filtering,Hybrid Recommender System based on Autoencoders,/paper/hybrid-recommender-system-based-on,https://arxiv.org/pdf/1606.07659v3.pdf
MovieLens 1M,,# 5,RMSE,0.8321,I-CFN,-,Collaborative Filtering,Hybrid Recommender System based on Autoencoders,/paper/hybrid-recommender-system-based-on,https://arxiv.org/pdf/1606.07659v3.pdf
CIFAR-10,,# 24,Percentage correct,93.57,ResNet,-,Image Classification,Deep Residual Learning for Image Recognition,/paper/deep-residual-learning-for-image-recognition,https://arxiv.org/pdf/1512.03385v1.pdf
CIFAR-10,,# 13,Percentage error,6.43,ResNet,-,Image Classification,Deep Residual Learning for Image Recognition,/paper/deep-residual-learning-for-image-recognition,https://arxiv.org/pdf/1512.03385v1.pdf
COCO,,# 30,Bounding Box AP,34.9,Faster R-CNN + box refinement + context + multi-scale testing,-,Object Detection,Deep Residual Learning for Image Recognition,/paper/deep-residual-learning-for-image-recognition,https://arxiv.org/pdf/1512.03385v1.pdf
PASCAL VOC 2007,,# 10,MAP,76.4%,ResNet-101,-,Object Detection,Deep Residual Learning for Image Recognition,/paper/deep-residual-learning-for-image-recognition,https://arxiv.org/pdf/1512.03385v1.pdf
LDC2014T12:,,# 2,F1 Newswire,0.71,Incremental joint model,-,Amr Parsing,AMR Parsing with an Incremental Joint Model,/paper/amr-parsing-with-an-incremental-joint-model,https://aclweb.org/anthology/D16-1065
LDC2014T12:,,# 2,F1 Full,0.66,Incremental joint model,-,Amr Parsing,AMR Parsing with an Incremental Joint Model,/paper/amr-parsing-with-an-incremental-joint-model,https://aclweb.org/anthology/D16-1065
ImageNet,,# 21,Top 1 Accuracy,70.6%,MobileNet-224,-,Image Classification,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,/paper/mobilenets-efficient-convolutional-neural,https://arxiv.org/pdf/1704.04861v1.pdf
ImageNet,,# 17,Top 5 Accuracy,89.5%,MobileNet-224,-,Image Classification,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,/paper/mobilenets-efficient-convolutional-neural,https://arxiv.org/pdf/1704.04861v1.pdf
enwiki8,,# 11,Bit per Character (BPC),1.34,Hypernetworks,-,Language Modelling,HyperNetworks,/paper/hypernetworks,https://arxiv.org/pdf/1609.09106v4.pdf
enwiki8,,# 1,Number of params,27M,Hypernetworks,-,Language Modelling,HyperNetworks,/paper/hypernetworks,https://arxiv.org/pdf/1609.09106v4.pdf
Penn Treebank (Character Level),,# 8,Bit per Character (BPC),1.219,2-layer Norm HyperLSTM,-,Language Modelling,HyperNetworks,/paper/hypernetworks,https://arxiv.org/pdf/1609.09106v4.pdf
Penn Treebank (Character Level),,# 1,Number of params,14.4M,2-layer Norm HyperLSTM,-,Language Modelling,HyperNetworks,/paper/hypernetworks,https://arxiv.org/pdf/1609.09106v4.pdf
FFHQ,,# 1,FID,4.16,StyleGAN (no instance norm),-,Image Generation,Improved Precision and Recall Metric for Assessing Generative Models,/paper/improved-precision-and-recall-metric-for,https://arxiv.org/pdf/1904.06991v1.pdf
COCO,,# 13,Bounding Box AP,43.6,RefineDet512 + VoVNet-57 (multi-scale),-,Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
COCO,,# 27,Bounding Box AP,39.2,RefineDet512 + VoVNet-57 (sinlge-scale),-,Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
COCO,,# 33,Bounding Box AP,33.9,RefineDet320 + VoVNet-57 backbone,-,Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
COCO,,# 4,MAP,33.9%,VoVNet-57 + RefineDet320 backbone,-,Real-Time Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
COCO,,# 5,FPS,21.2,VoVNet-57 + RefineDet320 backbone,-,Real-Time Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
PASCAL VOC 2007,,# 12,MAP,74.8%,DSOD300 + VoVNet-27-slim,-,Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
PASCAL VOC 2007,,# 4,MAP,74.8%,DSOD300 + VoVNet-27-slim,-,Real-Time Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
PASCAL VOC 2007,,# 1,FPS,71,DSOD300 + VoVNet-27-slim,-,Real-Time Object Detection,An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,/paper/an-energy-and-gpu-computation-efficient,https://arxiv.org/pdf/1904.09730v1.pdf
Market-1501,,# 18,Rank-1,81.0,DF,-,Person Re-Identification,Deeply-Learned Part-Aligned Representations for Person Re-Identification,/paper/deeply-learned-part-aligned-representations,https://arxiv.org/pdf/1707.07256v1.pdf
Market-1501,,# 18,MAP,63.4,DF,-,Person Re-Identification,Deeply-Learned Part-Aligned Representations for Person Re-Identification,/paper/deeply-learned-part-aligned-representations,https://arxiv.org/pdf/1707.07256v1.pdf
CIFAR-10,,# 29,Percentage correct,92.5,NiN+APL,-,Image Classification,Learning Activation Functions to Improve Deep Neural Networks,/paper/learning-activation-functions-to-improve-deep,https://arxiv.org/pdf/1412.6830v3.pdf
CIFAR-100,,# 24,Percentage correct,69.2,NiN+APL,-,Image Classification,Learning Activation Functions to Improve Deep Neural Networks,/paper/learning-activation-functions-to-improve-deep,https://arxiv.org/pdf/1412.6830v3.pdf
VOT2017/18,,# 7,Expected Average Overlap (EAO),0.28,ECO,-,Visual Object Tracking,ECO: Efficient Convolution Operators for Tracking,/paper/eco-efficient-convolution-operators-for,https://arxiv.org/pdf/1611.09224v2.pdf
SQuAD1.1,,# 131,EM,62.498999999999995,Dynamic Chunk Reader,-,Question Answering,End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,/paper/end-to-end-answer-chunk-extraction-and,https://arxiv.org/pdf/1610.09996v2.pdf
SQuAD1.1,,# 134,F1,70.956,Dynamic Chunk Reader,-,Question Answering,End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,/paper/end-to-end-answer-chunk-extraction-and,https://arxiv.org/pdf/1610.09996v2.pdf
DUC 2004 Task 1,,# 1,ROUGE-1,32.85,Transformer+LRPE+PE+Re-ranking+Ensemble,-,Text Summarization,Positional Encoding to Control Output Sequence Length,/paper/positional-encoding-to-control-output,https://arxiv.org/pdf/1904.07418v1.pdf
DUC 2004 Task 1,,# 1,ROUGE-2,11.78,Transformer+LRPE+PE+Re-ranking+Ensemble,-,Text Summarization,Positional Encoding to Control Output Sequence Length,/paper/positional-encoding-to-control-output,https://arxiv.org/pdf/1904.07418v1.pdf
DUC 2004 Task 1,,# 1,ROUGE-L,28.52,Transformer+LRPE+PE+Re-ranking+Ensemble,-,Text Summarization,Positional Encoding to Control Output Sequence Length,/paper/positional-encoding-to-control-output,https://arxiv.org/pdf/1904.07418v1.pdf
Charades,,# 5,MAP,24.1,MultiScale TRN,-,Action Recognition In Videos,Temporal Relational Reasoning in Videos,/paper/temporal-relational-reasoning-in-videos,https://arxiv.org/pdf/1711.08496v2.pdf
IWSLT2015 Thai-English,,# 1,BLEU score,14.2,Seq-KD + Seq-Inter + Word-KD,-,Machine Translation,Sequence-Level Knowledge Distillation,/paper/sequence-level-knowledge-distillation,https://arxiv.org/pdf/1606.07947v4.pdf
WMT2014 English-German,,# 19,BLEU score,18.5,Seq-KD + Seq-Inter + Word-KD,-,Machine Translation,Sequence-Level Knowledge Distillation,/paper/sequence-level-knowledge-distillation,https://arxiv.org/pdf/1606.07947v4.pdf
LibriSpeech test-other,,# 3,Word Error Rate (WER),8.79,deep 1d convs + ctc + external lm rescoring,-,Speech Recognition,Jasper: An End-to-End Convolutional Neural Acoustic Model,/paper/jasper-an-end-to-end-convolutional-neural,https://arxiv.org/pdf/1904.03288v1.pdf
Penn Treebank (Character Level),,# 1,Bit per Character (BPC),1.158,Trellis Network,-,Language Modelling,Trellis Networks for Sequence Modeling,/paper/trellis-networks-for-sequence-modeling,https://arxiv.org/pdf/1810.06682v2.pdf
Penn Treebank (Character Level),,# 1,Number of params,13.4M,Trellis Network,-,Language Modelling,Trellis Networks for Sequence Modeling,/paper/trellis-networks-for-sequence-modeling,https://arxiv.org/pdf/1810.06682v2.pdf
Penn Treebank (Word Level),,# 11,Test perplexity,54.19,Trellis Network,-,Language Modelling,Trellis Networks for Sequence Modeling,/paper/trellis-networks-for-sequence-modeling,https://arxiv.org/pdf/1810.06682v2.pdf
Sequential MNIST,,# 1,Accuracy,99.20,Trellis Network,-,Language Modelling,Trellis Networks for Sequence Modeling,/paper/trellis-networks-for-sequence-modeling,https://arxiv.org/pdf/1810.06682v2.pdf
WikiText-103,,# 5,Test perplexity,29.19,Trellis Network,-,Language Modelling,Trellis Networks for Sequence Modeling,/paper/trellis-networks-for-sequence-modeling,https://arxiv.org/pdf/1810.06682v2.pdf
WMT2016 English-German,,# 2,BLEU score,28.4,Linguistic Input Features,-,Machine Translation,Linguistic Input Features Improve Neural Machine Translation,/paper/linguistic-input-features-improve-neural,https://arxiv.org/pdf/1606.02892v2.pdf
WMT2016 German-English,,# 2,BLEU score,32.9,Linguistic Input Features,-,Machine Translation,Linguistic Input Features Improve Neural Machine Translation,/paper/linguistic-input-features-improve-neural,https://arxiv.org/pdf/1606.02892v2.pdf
CIFAR-10,,# 3,Model Entropy,3.5,Real NVP,-,Image Generation,Density estimation using Real NVP,/paper/density-estimation-using-real-nvp,https://arxiv.org/pdf/1605.08803v3.pdf
CIFAR-10,,# 5,Model Entropy,3.0,PixelRNN,-,Image Generation,Density estimation using Real NVP,/paper/density-estimation-using-real-nvp,https://arxiv.org/pdf/1605.08803v3.pdf
enwiki8,,# 5,Bit per Character (BPC),1.06,Transformer-XL - 12 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
enwiki8,,# 1,Number of params,41M,Transformer-XL - 12 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
enwiki8,,# 3,Bit per Character (BPC),0.99,Transformer-XL - 24 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
enwiki8,,# 1,Number of params,277M,Transformer-XL - 24 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
enwiki8,,# 4,Bit per Character (BPC),1.03,Transformer-XL - 18 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
enwiki8,,# 1,Number of params,88M,Transformer-XL - 18 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Hutter Prize,,# 2,Bit per Character (BPC),1.03,18-layer Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Hutter Prize,,# 1,Number of params,88M,18-layer Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Hutter Prize,,# 3,Bit per Character (BPC),1.06,12-layer Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Hutter Prize,,# 1,Number of params,41M,12-layer Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Hutter Prize,,# 1,Bit per Character (BPC),0.99,24-layer Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Hutter Prize,,# 1,Number of params,277M,24-layer Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
One Billion Word,,# 3,PPL,23.5,Transformer-XL Base,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
One Billion Word,,# 1,Number of params,0.46B,Transformer-XL Base,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
One Billion Word,,# 1,PPL,21.8,Transformer-XL Large,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
One Billion Word,,# 1,Number of params,0.8B,Transformer-XL Large,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Penn Treebank (Word Level),,# 11,Validation perplexity,56.72,Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Penn Treebank (Word Level),,# 13,Test perplexity,54.55,Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Penn Treebank (Word Level),,# 1,Params,24M,Transformer-XL,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Text8,,# 3,Bit per Character (BPC),1.08,Transformer-XL - 24 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
Text8,,# 1,Number of params,277M,Transformer-XL - 24 layers,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
WikiText-103,,# 2,Validation perplexity,18.2,Transformer-XL Large,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
WikiText-103,,# 2,Test perplexity,18.3,Transformer-XL Large,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
WikiText-103,,# 1,Number of params,257M,Transformer-XL Large,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
WikiText-103,,# 3,Validation perplexity,23.1,Transformer-XL Standard,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
WikiText-103,,# 4,Test perplexity,24.0,Transformer-XL Standard,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
WikiText-103,,# 1,Number of params,151M,Transformer-XL Standard,-,Language Modelling,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,/paper/transformer-xl-attentive-language-models,https://arxiv.org/pdf/1901.02860v2.pdf
GTAV-to-Cityscapes Labels,,# 3,mIoU,39.5,CyCADA pixel+feat,-,Synthetic-to-Real Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
GTAV-to-Cityscapes Labels,,# 1,fwIOU,72.4,CyCADA pixel+feat,-,Synthetic-to-Real Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
GTAV-to-Cityscapes Labels,,# 1,Per-pixel Accuracy,82.3%,CyCADA pixel+feat,-,Synthetic-to-Real Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
SVNH-to-MNIST,,# 1,Classification Accuracy,90.4%,CyCADA pixel+feat,-,Unsupervised Image-To-Image Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
SYNTHIA Fall-to-Winter,,# 1,mIoU,63.3,CyCADA,-,Image-to-Image Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
SYNTHIA Fall-to-Winter,,# 1,Per-pixel Accuracy,92.1%,CyCADA,-,Image-to-Image Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
SYNTHIA Fall-to-Winter,,# 1,fwIOU,85.7,CyCADA,-,Image-to-Image Translation,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,/paper/cycada-cycle-consistent-adversarial-domain,https://arxiv.org/pdf/1711.03213v3.pdf
CompCars,,# 2,Accuracy,91.2%,GoogLeNet,-,Fine-Grained Image Classification,A Large-Scale Car Dataset for Fine-Grained Categorization and Verification,/paper/a-large-scale-car-dataset-for-fine-grained,https://arxiv.org/pdf/1506.08959v2.pdf
CompCars,,# 3,Accuracy,81.9%,AlexNet,-,Fine-Grained Image Classification,A Large-Scale Car Dataset for Fine-Grained Categorization and Verification,/paper/a-large-scale-car-dataset-for-fine-grained,https://arxiv.org/pdf/1506.08959v2.pdf
BSD100 - 4x upscaling,,# 14,PSNR,27.44,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
BSD100 - 4x upscaling,,# 19,SSIM,0.7302,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
Set14 - 4x upscaling,,# 17,PSNR,28.29,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
Set14 - 4x upscaling,,# 20,SSIM,0.7741,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
Set5 - 4x upscaling,,# 14,PSNR,31.82,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
Set5 - 4x upscaling,,# 17,SSIM,0.8907,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
Urban100 - 4x upscaling,,# 15,PSNR,25.59,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
Urban100 - 4x upscaling,,# 16,SSIM,0.768,RL-CSC,-,Image Super-Resolution,Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding,/paper/image-super-resolution-via-rl-csc-when,https://arxiv.org/pdf/1812.11950v1.pdf
BSD100 - 4x upscaling,,# 15,PSNR,27.42,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
BSD100 - 4x upscaling,,# 11,SSIM,0.737,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
Set14 - 4x upscaling,,# 16,PSNR,28.32,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
Set14 - 4x upscaling,,# 12,SSIM,0.784,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
Set5 - 4x upscaling,,# 13,PSNR,31.84,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
Set5 - 4x upscaling,,# 16,SSIM,0.8909999999999999,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
Urban100 - 4x upscaling,,# 17,PSNR,25.42,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
Urban100 - 4x upscaling,,# 14,SSIM,0.7709999999999999,SESR,-,Image Super-Resolution,SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks,/paper/sesr-single-image-super-resolution-with,https://arxiv.org/pdf/1801.10319v1.pdf
TREC Robust04,,# 6,MAP,0.2837,FNRM-RankProb_Embed,-,Ad-Hoc Information Retrieval,Neural Ranking Models with Weak Supervision,/paper/neural-ranking-models-with-weak-supervision,https://arxiv.org/pdf/1704.08803v2.pdf
TREC Robust04,,# 7,MAP,0.2811,FNRM-Rank_Embed,-,Ad-Hoc Information Retrieval,Neural Ranking Models with Weak Supervision,/paper/neural-ranking-models-with-weak-supervision,https://arxiv.org/pdf/1704.08803v2.pdf
Caltech,,# 9,Reasonable Miss Rate,7.36,SDS-RCNN,-,Pedestrian Detection,Illuminating Pedestrians via Simultaneous Detection & Segmentation,/paper/illuminating-pedestrians-via-simultaneous,https://arxiv.org/pdf/1706.08564v1.pdf
QASent,,# 5,MAP,0.6436,LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
QASent,,# 5,MRR,0.7235,LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
QASent,,# 2,MAP,0.7228,LSTM (lexical overlap + dist output),-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
QASent,,# 2,MRR,0.7986,LSTM (lexical overlap + dist output),-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
QASent,,# 1,MAP,0.7339,Attentive LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
QASent,,# 1,MRR,0.8117,Attentive LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
WikiQA,,# 6,MAP,0.6886,Attentive LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
WikiQA,,# 6,MRR,0.7069,Attentive LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
WikiQA,,# 7,MAP,0.682,LSTM (lexical overlap + dist output),-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
WikiQA,,# 8,MRR,0.6988,LSTM (lexical overlap + dist output),-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
WikiQA,,# 9,MAP,0.6552,LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
WikiQA,,# 10,MRR,0.6747,LSTM,-,Question Answering,Neural Variational Inference for Text Processing,/paper/neural-variational-inference-for-text,https://arxiv.org/pdf/1511.06038v4.pdf
Oxf105k,,# 2,MAP,87.8%,DIR+QE*,-,Image Retrieval,Deep Image Retrieval: Learning global representations for image search,/paper/deep-image-retrieval-learning-global,https://arxiv.org/pdf/1604.01325v2.pdf
Oxf5k,,# 2,MAP,89%,DIR+QE*,-,Image Retrieval,Deep Image Retrieval: Learning global representations for image search,/paper/deep-image-retrieval-learning-global,https://arxiv.org/pdf/1604.01325v2.pdf
Par106k,,# 2,Accuracy,90.5%,DIR+QE*,-,Image Retrieval,Deep Image Retrieval: Learning global representations for image search,/paper/deep-image-retrieval-learning-global,https://arxiv.org/pdf/1604.01325v2.pdf
Par6k,,# 2,Accuracy,93.8%,DIR+QE*,-,Image Retrieval,Deep Image Retrieval: Learning global representations for image search,/paper/deep-image-retrieval-learning-global,https://arxiv.org/pdf/1604.01325v2.pdf
BSD100 - 4x upscaling,,# 19,PSNR,27.29,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD100 - 4x upscaling,,# 23,SSIM,0.7253,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD68 sigma10,,# 2,PSNR,36.31,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD68 sigma15,,# 3,PSNR,31.73,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD68 sigma25,,# 3,PSNR,29.23,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD68 sigma30,,# 2,PSNR,30.4,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD68 sigma50,,# 6,PSNR,26.23,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
BSD68 sigma70,,# 2,PSNR,26.56,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Set14 - 4x upscaling,,# 22,PSNR,28.04,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Set14 - 4x upscaling,,# 24,SSIM,0.7672,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Set5 - 4x upscaling,,# 19,PSNR,31.4,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Set5 - 4x upscaling,,# 21,SSIM,0.8845,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Urban100 - 4x upscaling,,# 20,PSNR,25.2,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Urban100 - 4x upscaling,,# 19,SSIM,0.7521,DnCNN-3,-,Image Super-Resolution,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Urban100 sigma50,,# 6,PSNR,26.28,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
Urban100 sigma70,,# 4,PSNR,24.36,DnCNN,-,Image Denoising,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,/paper/beyond-a-gaussian-denoiser-residual-learning,https://arxiv.org/pdf/1608.03981v1.pdf
COCO,,# 3,Test AP,66.5,PersonLab,-,Keypoint Detection,"PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model",/paper/personlab-person-pose-estimation-and-instance,https://arxiv.org/pdf/1803.08225v1.pdf
SVNH-to-MNIST,,# 2,Classification Accuracy,84.4%,DTN,-,Unsupervised Image-To-Image Translation,Unsupervised Cross-Domain Image Generation,/paper/unsupervised-cross-domain-image-generation,https://arxiv.org/pdf/1611.02200v1.pdf
MR,,# 2,Accuracy,83.8,RNN-Capsule,-,Sentiment Analysis,Sentiment Analysis by Capsules,/paper/sentiment-analysis-by-capsules,https://www.researchgate.net/publication/323257127_Sentiment_Analysis_by_Capsules/download
SST-5 Fine-grained classification,,# 10,Accuracy,49.3,RNN-Capsule,-,Sentiment Analysis,Sentiment Analysis by Capsules,/paper/sentiment-analysis-by-capsules,https://www.researchgate.net/publication/323257127_Sentiment_Analysis_by_Capsules/download
BRATS-2015,,# 2,Dice Score,84%,Multi-Scale 3D + FC-CRF,-,Brain Tumor Segmentation,Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation,/paper/efficient-multi-scale-3d-cnn-with-fully,https://arxiv.org/pdf/1603.05959v3.pdf
ACL-ARC,,# 4,F1,53.0,Random Forest,-,Sentence Classification,Measuring the Evolution of a Scientific Field through Citation Frames,/paper/measuring-the-evolution-of-a-scientific-field,https://aclweb.org/anthology/Q18-1028
ACL-ARC,,# 4,F1,53.0,Feature-rich Random Forest,-,Citation Intent Classification,Measuring the Evolution of a Scientific Field through Citation Frames,/paper/measuring-the-evolution-of-a-scientific-field,https://aclweb.org/anthology/Q18-1028
SciCite,,# 4,F1,79.6,Feature-Rich Random Forest,-,Citation Intent Classification,Measuring the Evolution of a Scientific Field through Citation Frames,/paper/measuring-the-evolution-of-a-scientific-field,https://aclweb.org/anthology/Q18-1028
SciCite,,# 5,F1,79.6,Feature-Rich Random Forest,-,Sentence Classification,Measuring the Evolution of a Scientific Field through Citation Frames,/paper/measuring-the-evolution-of-a-scientific-field,https://aclweb.org/anthology/Q18-1028
OCCLUSION,,# 2,MAP,0.38,SSD-6D,-,6D Pose Estimation,SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again,/paper/ssd-6d-making-rgb-based-3d-detection-and-6d,https://arxiv.org/pdf/1711.10006v1.pdf
Penn Treebank,,# 3,POS,97.3,Deep Biaffine,-,Dependency Parsing,Deep Biaffine Attention for Neural Dependency Parsing,/paper/deep-biaffine-attention-for-neural-dependency,https://arxiv.org/pdf/1611.01734v3.pdf
Penn Treebank,,# 2,UAS,95.44,Deep Biaffine,-,Dependency Parsing,Deep Biaffine Attention for Neural Dependency Parsing,/paper/deep-biaffine-attention-for-neural-dependency,https://arxiv.org/pdf/1611.01734v3.pdf
Penn Treebank,,# 2,LAS,93.76,Deep Biaffine,-,Dependency Parsing,Deep Biaffine Attention for Neural Dependency Parsing,/paper/deep-biaffine-attention-for-neural-dependency,https://arxiv.org/pdf/1611.01734v3.pdf
CIFAR-10,,# 22,Percentage correct,93.7,SSCNN,-,Image Classification,Spatially-sparse convolutional neural networks,/paper/spatially-sparse-convolutional-neural,https://arxiv.org/pdf/1409.6070v1.pdf
CIFAR-100,,# 14,Percentage correct,75.7,SSCNN,-,Image Classification,Spatially-sparse convolutional neural networks,/paper/spatially-sparse-convolutional-neural,https://arxiv.org/pdf/1409.6070v1.pdf
SVHN,,# 2,Percentage error,1.54,EraseReLU,-,Image Classification,EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks,/paper/eraserelu-a-simple-way-to-ease-the-training,https://arxiv.org/pdf/1709.07634v2.pdf
BSD100 - 4x upscaling,,# 32,PSNR,25.02,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 30,SSIM,0.6606,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 6,MOS,1.11,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 29,PSNR,25.94,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 27,SSIM,0.6935,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 5,MOS,1.47,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 8,PSNR,27.58,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 1,SSIM,0.762,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 2,MOS,2.29,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 31,PSNR,25.16,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 29,SSIM,0.6688,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
BSD100 - 4x upscaling,,# 1,MOS,3.56,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 37,PSNR,25.99,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 29,SSIM,0.7486,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 5,MOS,1.8,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 11,PSNR,28.49,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 1,SSIM,0.8184,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 2,MOS,2.98,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 36,PSNR,26.02,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 31,SSIM,0.7397,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 1,MOS,3.72,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 38,PSNR,24.64,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 32,SSIM,0.71,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set14 - 4x upscaling,,# 6,MOS,1.2,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 34,PSNR,26.26,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 33,SSIM,0.7552,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 6,MOS,1.28,nearest neighbors,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 32,PSNR,28.43,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 31,SSIM,0.8211,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 5,MOS,1.97,bicubic,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 10,PSNR,32.05,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 2,SSIM,0.9019,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 2,MOS,3.37,SRResNet,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 30,PSNR,29.4,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 29,SSIM,0.8472,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Set5 - 4x upscaling,,# 1,MOS,3.58,SRGAN,-,Image Super-Resolution,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,/paper/photo-realistic-single-image-super-resolution,https://arxiv.org/pdf/1609.04802v5.pdf
Unrestricted,,# 2,F0.5,61.34,CNN Seq2Seq + Fluency Boost,-,Grammatical Error Correction,Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study,/paper/reaching-human-level-performance-in-automatic,https://arxiv.org/pdf/1807.01270v5.pdf
Unrestricted,,# 1,F0.5,76.88,CNN Seq2Seq + Fluency Boost,-,Grammatical Error Correction,Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study,/paper/reaching-human-level-performance-in-automatic,https://arxiv.org/pdf/1807.01270v5.pdf
Unrestricted,,# 1,GLEU,62.37,CNN Seq2Seq + Fluency Boost and inference,-,Grammatical Error Correction,Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study,/paper/reaching-human-level-performance-in-automatic,https://arxiv.org/pdf/1807.01270v5.pdf
BUCC French-to-English,,# 1,F1 score,93.91,Massively Multilingual Sentence Embeddings,-,Cross-Lingual Bitext Mining,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
BUCC German-to-English,,# 1,F1 score,96.19,Massively Multilingual Sentence Embeddings,-,Cross-Lingual Bitext Mining,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
MLDoc Zero-Shot English-to-French,,# 1,Accuracy,77.95%,Massively Multilingual Sentence Embeddings,-,Cross-Lingual Document Classification,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
MLDoc Zero-Shot English-to-German,,# 1,Accuracy,84.78%,Massively Multilingual Sentence Embeddings,-,Cross-Lingual Document Classification,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
MLDoc Zero-Shot English-to-Spanish,,# 1,Accuracy,77.33%,Massively Multilingual Sentence Embeddings,-,Cross-Lingual Document Classification,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
XNLI Zero-Shot English-to-French,,# 1,Accuracy,71.9%,BiLSTM,-,Cross-Lingual Natural Language Inference,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
XNLI Zero-Shot English-to-German,,# 1,Accuracy,72.6%,BiLSTM,-,Cross-Lingual Natural Language Inference,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
XNLI Zero-Shot English-to-Spanish,,# 2,Accuracy,72.9%,BiLSTM,-,Cross-Lingual Natural Language Inference,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,/paper/massively-multilingual-sentence-embeddings,https://arxiv.org/pdf/1812.10464v1.pdf
WMT2014 English-German,,# 11,BLEU score,26.1,SliceNet,-,Machine Translation,Depthwise Separable Convolutions for Neural Machine Translation,/paper/depthwise-separable-convolutions-for-neural,https://arxiv.org/pdf/1706.03059v2.pdf
bAbi,,# 3,Accuracy (trained on 10k),97.2%,DMN+,-,Question Answering,Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes,/paper/dynamic-neural-turing-machine-with-soft-and,https://arxiv.org/pdf/1607.00036v2.pdf
bAbi,,# 5,Accuracy (trained on 1k),66.8%,DMN+,-,Question Answering,Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes,/paper/dynamic-neural-turing-machine-with-soft-and,https://arxiv.org/pdf/1607.00036v2.pdf
ADE20K,,# 3,Validation mIoU,44.34,EncNet + JPU,-,Semantic Segmentation,FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation,/paper/fastfcn-rethinking-dilated-convolution-in-the,https://arxiv.org/pdf/1903.11816v1.pdf
ADE20K,,# 1,Test Score,0.5584,EncNet + JPU,-,Semantic Segmentation,FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation,/paper/fastfcn-rethinking-dilated-convolution-in-the,https://arxiv.org/pdf/1903.11816v1.pdf
PASCAL Context,,# 1,mIoU,53.1,Joint Pyramid Upsampling + EncNet,-,Semantic Segmentation,FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation,/paper/fastfcn-rethinking-dilated-convolution-in-the,https://arxiv.org/pdf/1903.11816v1.pdf
DUC 2004 Task 1,,# 5,ROUGE-1,29.21,SEASS,-,Text Summarization,Selective Encoding for Abstractive Sentence Summarization,/paper/selective-encoding-for-abstractive-sentence-1,https://aclweb.org/anthology/P17-1101
DUC 2004 Task 1,,# 5,ROUGE-2,9.56,SEASS,-,Text Summarization,Selective Encoding for Abstractive Sentence Summarization,/paper/selective-encoding-for-abstractive-sentence-1,https://aclweb.org/anthology/P17-1101
DUC 2004 Task 1,,# 4,ROUGE-L,25.51,SEASS,-,Text Summarization,Selective Encoding for Abstractive Sentence Summarization,/paper/selective-encoding-for-abstractive-sentence-1,https://aclweb.org/anthology/P17-1101
GigaWord,,# 6,ROUGE-1,36.15,SEASS,-,Text Summarization,Selective Encoding for Abstractive Sentence Summarization,/paper/selective-encoding-for-abstractive-sentence-1,https://aclweb.org/anthology/P17-1101
GigaWord,,# 8,ROUGE-2,17.54,SEASS,-,Text Summarization,Selective Encoding for Abstractive Sentence Summarization,/paper/selective-encoding-for-abstractive-sentence-1,https://aclweb.org/anthology/P17-1101
GigaWord,,# 7,ROUGE-L,33.63,SEASS,-,Text Summarization,Selective Encoding for Abstractive Sentence Summarization,/paper/selective-encoding-for-abstractive-sentence-1,https://aclweb.org/anthology/P17-1101
ChaLearn 2015,,# 1,MAE,3.51,DLDL+VGG-Face,-,Age Estimation,Deep Label Distribution Learning with Label Ambiguity,/paper/deep-label-distribution-learning-with-label,https://arxiv.org/pdf/1611.01731v2.pdf
MORPH Album2,,# 2,MAE,2.42Â±0.01,DLDL+VGG-Face,-,Age Estimation,Deep Label Distribution Learning with Label Ambiguity,/paper/deep-label-distribution-learning-with-label,https://arxiv.org/pdf/1611.01731v2.pdf
DukeMTMC-reID,,# 11,Rank-1,72.44,TriNet,-,Person Re-Identification,In Defense of the Triplet Loss for Person Re-Identification,/paper/in-defense-of-the-triplet-loss-for-person-re,https://arxiv.org/pdf/1703.07737v4.pdf
DukeMTMC-reID,,# 10,MAP,53.5,TriNet,-,Person Re-Identification,In Defense of the Triplet Loss for Person Re-Identification,/paper/in-defense-of-the-triplet-loss-for-person-re,https://arxiv.org/pdf/1703.07737v4.pdf
Market-1501,,# 13,Rank-1,84.92,TriNet,-,Person Re-Identification,In Defense of the Triplet Loss for Person Re-Identification,/paper/in-defense-of-the-triplet-loss-for-person-re,https://arxiv.org/pdf/1703.07737v4.pdf
Market-1501,,# 9,MAP,69.14,TriNet,-,Person Re-Identification,In Defense of the Triplet Loss for Person Re-Identification,/paper/in-defense-of-the-triplet-loss-for-person-re,https://arxiv.org/pdf/1703.07737v4.pdf
COCO,,# 3,FID,74.05,StackGAN-v1,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
COCO,,# 4,Inception score,8.45,StackGAN-v1,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
COCO,,# 4,FID,81.59,StackGAN-v2,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
COCO,,# 5,Inception score,8.3,StackGAN-v2,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
CUB,,# 1,FID,15.3,StackGAN-v2,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
CUB,,# 2,Inception score,3.82,StackGAN-v2,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
CUB,,# 2,FID,51.89,StackGAN-v1,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
CUB,,# 3,Inception score,3.7,StackGAN-v1,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
LSUN Bedroom 256 x 256,,# 6,FID,35.61,StackGAN-v2,-,Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
Oxford 102 Flowers,,# 1,FID,48.68,StackGAN-v2,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
Oxford 102 Flowers,,# 1,Inception score,3.26,StackGAN-v2,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
Oxford 102 Flowers,,# 2,FID,55.28,StackGAN-v1,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
Oxford 102 Flowers,,# 2,Inception score,3.2,StackGAN-v1,-,Text-to-Image Generation,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,/paper/stackgan-realistic-image-synthesis-with,https://arxiv.org/pdf/1710.10916v3.pdf
Data3DâR2N2,,# 3,Avg F1,48.58,PSG,-,3D Object Reconstruction,A Point Set Generation Network for 3D Object Reconstruction from a Single Image,/paper/a-point-set-generation-network-for-3d-object,https://arxiv.org/pdf/1612.00603v2.pdf
CoNLL-2014 A1,,# 1,F0.5,36.1,Bi-LSTM + POS (unrestricted data),-,Grammatical Error Detection,Auxiliary Objectives for Neural Error Detection Models,/paper/auxiliary-objectives-for-neural-error,https://arxiv.org/pdf/1707.05227v1.pdf
CoNLL-2014 A1,,# 6,F0.5,17.5,Bi-LSTM + POS (trained on FCE),-,Grammatical Error Detection,Auxiliary Objectives for Neural Error Detection Models,/paper/auxiliary-objectives-for-neural-error,https://arxiv.org/pdf/1707.05227v1.pdf
CoNLL-2014 A2,,# 1,F0.5,45.1,Bi-LSTM + POS (unrestricted data),-,Grammatical Error Detection,Auxiliary Objectives for Neural Error Detection Models,/paper/auxiliary-objectives-for-neural-error,https://arxiv.org/pdf/1707.05227v1.pdf
CoNLL-2014 A2,,# 5,F0.5,26.2,Bi-LSTM + POS (trained on FCE),-,Grammatical Error Detection,Auxiliary Objectives for Neural Error Detection Models,/paper/auxiliary-objectives-for-neural-error,https://arxiv.org/pdf/1707.05227v1.pdf
FCE,,# 4,F0.5,47.7,Bi-LSTM + err POS GR,-,Grammatical Error Detection,Auxiliary Objectives for Neural Error Detection Models,/paper/auxiliary-objectives-for-neural-error,https://arxiv.org/pdf/1707.05227v1.pdf
CIFAR-10,,# 9,Inception score,7.07,CEGAN-Ent-VI,-,Image Generation,Calibrating Energy-based Generative Adversarial Networks,/paper/calibrating-energy-based-generative,https://arxiv.org/pdf/1702.01691v2.pdf
Sequential MNIST,,# 1,Unpermuted Accuracy,99.2%,Dilated GRU,-,Sequential Image Classification,Dilated Recurrent Neural Networks,/paper/dilated-recurrent-neural-networks,https://arxiv.org/pdf/1710.02224v3.pdf
Sequential MNIST,,# 2,Permuted Accuracy,94.6%,Dilated GRU,-,Sequential Image Classification,Dilated Recurrent Neural Networks,/paper/dilated-recurrent-neural-networks,https://arxiv.org/pdf/1710.02224v3.pdf
CIFAR-10,,# 27,Percentage correct,93.1,CMsC,-,Image Classification,Competitive Multi-scale Convolution,/paper/competitive-multi-scale-convolution,https://arxiv.org/pdf/1511.05635v1.pdf
CIFAR-100,,# 20,Percentage correct,72.4,CMsC,-,Image Classification,Competitive Multi-scale Convolution,/paper/competitive-multi-scale-convolution,https://arxiv.org/pdf/1511.05635v1.pdf
MNIST,,# 3,Percentage error,0.3,CMsC,-,Image Classification,Competitive Multi-scale Convolution,/paper/competitive-multi-scale-convolution,https://arxiv.org/pdf/1511.05635v1.pdf
SVHN,,# 8,Percentage error,1.76,CMsC,-,Image Classification,Competitive Multi-scale Convolution,/paper/competitive-multi-scale-convolution,https://arxiv.org/pdf/1511.05635v1.pdf
Disguised Faces in the Wild,,# 1,GAR @0.1% FAR,23.25,DisguiseNet,-,Disguised Face Verification,DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild,/paper/disguisenet-a-contrastive-approach-for,https://arxiv.org/pdf/1804.09669v2.pdf
Disguised Faces in the Wild,,# 1,GAR @1% FAR,60.89,DisguiseNet,-,Disguised Face Verification,DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild,/paper/disguisenet-a-contrastive-approach-for,https://arxiv.org/pdf/1804.09669v2.pdf
Disguised Faces in the Wild,,# 1,GAR @10% FAR,98.99,DisguiseNet,-,Disguised Face Verification,DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild,/paper/disguisenet-a-contrastive-approach-for,https://arxiv.org/pdf/1804.09669v2.pdf
"CIFAR-10, 4000 Labels",,# 4,Accuracy,88.64,VAT,-,Semi-Supervised Image Classification,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,/paper/virtual-adversarial-training-a-regularization,https://arxiv.org/pdf/1704.03976v2.pdf
"CIFAR-10, 4000 Labels",,# 3,Accuracy,89.45,VAT+EntMin,-,Semi-Supervised Image Classification,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,/paper/virtual-adversarial-training-a-regularization,https://arxiv.org/pdf/1704.03976v2.pdf
"SVHN, 1000 labels",,# 1,Accuracy,96.14,VAT+EntMin,-,Semi-Supervised Image Classification,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,/paper/virtual-adversarial-training-a-regularization,https://arxiv.org/pdf/1704.03976v2.pdf
"SVHN, 1000 labels",,# 4,Accuracy,94.58,VAT,-,Semi-Supervised Image Classification,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,/paper/virtual-adversarial-training-a-regularization,https://arxiv.org/pdf/1704.03976v2.pdf
BSD100 - 4x upscaling,,# 17,PSNR,27.4,RED30,-,Image Super-Resolution,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,/paper/image-restoration-using-very-deep,https://arxiv.org/pdf/1603.09056v2.pdf
BSD100 - 4x upscaling,,# 21,SSIM,0.729,RED30,-,Image Super-Resolution,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,/paper/image-restoration-using-very-deep,https://arxiv.org/pdf/1603.09056v2.pdf
Set5 - 4x upscaling,,# 17,PSNR,31.51,RED30,-,Image Super-Resolution,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,/paper/image-restoration-using-very-deep,https://arxiv.org/pdf/1603.09056v2.pdf
Set5 - 4x upscaling,,# 20,SSIM,0.8869,RED30,-,Image Super-Resolution,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,/paper/image-restoration-using-very-deep,https://arxiv.org/pdf/1603.09056v2.pdf
Citeseer,,# 2,Accuracy,72.5%,GAT,-,Node Classification,Graphite: Iterative Generative Modeling of Graphs,/paper/graphite-iterative-generative-modeling-of,https://arxiv.org/pdf/1803.10459v3.pdf
Cora,,# 2,Accuracy,83.0%,GAT,-,Node Classification,Graphite: Iterative Generative Modeling of Graphs,/paper/graphite-iterative-generative-modeling-of,https://arxiv.org/pdf/1803.10459v3.pdf
Pubmed,,# 4,Accuracy,79.00%,GAT,-,Node Classification,Graphite: Iterative Generative Modeling of Graphs,/paper/graphite-iterative-generative-modeling-of,https://arxiv.org/pdf/1803.10459v3.pdf
SNLI,,# 26,% Test Accuracy,86.3,300D Reinforced Self-Attention Network,-,Natural Language Inference,Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling,/paper/reinforced-self-attention-network-a-hybrid-of,https://arxiv.org/pdf/1801.10296v2.pdf
SNLI,,# 19,% Train Accuracy,92.6,300D Reinforced Self-Attention Network,-,Natural Language Inference,Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling,/paper/reinforced-self-attention-network-a-hybrid-of,https://arxiv.org/pdf/1801.10296v2.pdf
SNLI,,# 1,Parameters,3.1m,300D Reinforced Self-Attention Network,-,Natural Language Inference,Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling,/paper/reinforced-self-attention-network-a-hybrid-of,https://arxiv.org/pdf/1801.10296v2.pdf
North American English,,# 4,Mean Opinion Score,4.001,Tacotron,-,Speech Synthesis,Tacotron: Towards End-to-End Speech Synthesis,/paper/tacotron-towards-end-to-end-speech-synthesis,https://arxiv.org/pdf/1703.10135v2.pdf
AG News,,# 9,Error,7.5,fastText,-,Text Classification,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
Amazon Review Full,,# 8,Accuracy,60.2,FastText,-,Sentiment Analysis,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
Amazon Review Polarity,,# 8,Accuracy,94.6,FastText,-,Sentiment Analysis,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
DBpedia,,# 14,Error,1.4,FastText,-,Text Classification,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
Sogou News,,# 1,Accuracy,96.8,fastText,-,Sentiment Analysis,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
Yahoo! Answers,,# 5,Accuracy,72.3,FastText,-,Text Classification,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
Yelp Binary classification,,# 12,Error,4.3,FastText,-,Sentiment Analysis,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
Yelp Fine-grained classification,,# 10,Error,36.1,FastText,-,Sentiment Analysis,Bag of Tricks for Efficient Text Classification,/paper/bag-of-tricks-for-efficient-text,https://arxiv.org/pdf/1607.01759v3.pdf
CIFAR-10,,# 3,Inception score,8.59,SGAN,-,Conditional Image Generation,Stacked Generative Adversarial Networks,/paper/stacked-generative-adversarial-networks,https://arxiv.org/pdf/1612.04357v4.pdf
IWSLT2015 English-German,,# 2,BLEU score,28.16,NAT +FT + NPD,-,Machine Translation,Non-Autoregressive Neural Machine Translation,/paper/non-autoregressive-neural-machine-translation-1,https://arxiv.org/pdf/1711.02281v2.pdf
WMT2014 English-German,,# 18,BLEU score,19.17,NAT +FT + NPD,-,Machine Translation,Non-Autoregressive Neural Machine Translation,/paper/non-autoregressive-neural-machine-translation-1,https://arxiv.org/pdf/1711.02281v2.pdf
WMT2014 German-English,,# 2,BLEU score,23.2,NAT +FT + NPD,-,Machine Translation,Non-Autoregressive Neural Machine Translation,/paper/non-autoregressive-neural-machine-translation-1,https://arxiv.org/pdf/1711.02281v2.pdf
WMT2016 English-Romanian,,# 2,BLEU score,29.79,NAT +FT + NPD,-,Machine Translation,Non-Autoregressive Neural Machine Translation,/paper/non-autoregressive-neural-machine-translation-1,https://arxiv.org/pdf/1711.02281v2.pdf
WMT2016 Romanian-English,,# 3,BLEU score,31.44,NAT +FT + NPD,-,Machine Translation,Non-Autoregressive Neural Machine Translation,/paper/non-autoregressive-neural-machine-translation-1,https://arxiv.org/pdf/1711.02281v2.pdf
DukeMTMC-reID,,# 7,Rank-1,78.32,IDE* + CamStyle + Random Erasing,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
DukeMTMC-reID,,# 7,MAP,57.61,IDE* + CamStyle + Random Erasing,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
DukeMTMC-reID,,# 9,Rank-1,75.27,IDE* + CamStyle,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
DukeMTMC-reID,,# 11,MAP,53.48,IDE* + CamStyle,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
DukeMTMC-reID,,# 12,Rank-1,72.31,IDE*,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
DukeMTMC-reID,,# 13,MAP,51.83,IDE*,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
Market-1501,,# 6,Rank-1,89.49,IDE* + CamStyle + Random Erasing,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
Market-1501,,# 7,MAP,71.55,IDE* + CamStyle + Random Erasing,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
Market-1501,,# 9,Rank-1,88.12,IDE* + CamStyle,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
Market-1501,,# 12,MAP,68.72,IDE* + CamStyle,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
Market-1501,,# 11,Rank-1,85.66,IDE*,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
Market-1501,,# 14,MAP,65.87,IDE*,-,Person Re-Identification,Camera Style Adaptation for Person Re-identification,/paper/camera-style-adaptation-for-person-re,https://arxiv.org/pdf/1711.10295v2.pdf
IWSLT2015 English-German,,# 7,BLEU score,25.04,RNNsearch,-,Machine Translation,An Actor-Critic Algorithm for Sequence Prediction,/paper/an-actor-critic-algorithm-for-sequence,https://arxiv.org/pdf/1607.07086v3.pdf
IWSLT2015 German-English,,# 11,BLEU score,29.98,RNNsearch,-,Machine Translation,An Actor-Critic Algorithm for Sequence Prediction,/paper/an-actor-critic-algorithm-for-sequence,https://arxiv.org/pdf/1607.07086v3.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 6,Percentage correct,59.5,CNN-RNN,-,Visual Question Answering,Image Captioning and Visual Question Answering Based on Attributes and External Knowledge,/paper/image-captioning-and-visual-question,https://arxiv.org/pdf/1603.02814v2.pdf
CIFAR-10,,# 1,Percentage correct,99,GPIPE + transfer learning,-,Image Classification,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,/paper/gpipe-efficient-training-of-giant-neural,https://arxiv.org/pdf/1811.06965v4.pdf
CIFAR-10,,# 1,Percentage error,1,GPIPE + transfer learning,-,Image Classification,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,/paper/gpipe-efficient-training-of-giant-neural,https://arxiv.org/pdf/1811.06965v4.pdf
CIFAR-100,,# 1,Percentage correct,91.3,GPIPE,-,Image Classification,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,/paper/gpipe-efficient-training-of-giant-neural,https://arxiv.org/pdf/1811.06965v4.pdf
ImageNet,,# 1,Top 1 Accuracy,84.3%,GPIPE,-,Image Classification,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,/paper/gpipe-efficient-training-of-giant-neural,https://arxiv.org/pdf/1811.06965v4.pdf
ImageNet,,# 1,Top 5 Accuracy,97%,GPIPE,-,Image Classification,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,/paper/gpipe-efficient-training-of-giant-neural,https://arxiv.org/pdf/1811.06965v4.pdf
CIFAR-10,,# 15,Percentage correct,95.51,SimpleNetv1,-,Image Classification,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",/paper/lets-keep-it-simple-using-simple,https://arxiv.org/pdf/1608.06037v7.pdf
CIFAR-100,,# 11,Percentage correct,78.37,SimpleNetv1,-,Image Classification,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",/paper/lets-keep-it-simple-using-simple,https://arxiv.org/pdf/1608.06037v7.pdf
MNIST,,# 2,Percentage error,0.2,SimpleNetv1,-,Image Classification,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",/paper/lets-keep-it-simple-using-simple,https://arxiv.org/pdf/1608.06037v7.pdf
WN18,,# 5,MRR,0.9420000000000001,ANALOGY,-,Link Prediction,Analogical Inference for Multi-Relational Embeddings,/paper/analogical-inference-for-multi-relational,https://arxiv.org/pdf/1705.02426v2.pdf
WN18,,# 4,[emailÂ protected],0.9470000000000001,ANALOGY,-,Link Prediction,Analogical Inference for Multi-Relational Embeddings,/paper/analogical-inference-for-multi-relational,https://arxiv.org/pdf/1705.02426v2.pdf
WN18,,# 3,[emailÂ protected],0.9440000000000001,ANALOGY,-,Link Prediction,Analogical Inference for Multi-Relational Embeddings,/paper/analogical-inference-for-multi-relational,https://arxiv.org/pdf/1705.02426v2.pdf
WN18,,# 4,[emailÂ protected],0.9390000000000001,ANALOGY,-,Link Prediction,Analogical Inference for Multi-Relational Embeddings,/paper/analogical-inference-for-multi-relational,https://arxiv.org/pdf/1705.02426v2.pdf
E2E NLG Challenge,,# 3,BLEU,65.93,TGen,-,Data-to-Text Generation,Findings of the E2E NLG Challenge,/paper/findings-of-the-e2e-nlg-challenge,https://arxiv.org/pdf/1810.01170v1.pdf
E2E NLG Challenge,,# 3,NIST,8.6094,TGen,-,Data-to-Text Generation,Findings of the E2E NLG Challenge,/paper/findings-of-the-e2e-nlg-challenge,https://arxiv.org/pdf/1810.01170v1.pdf
E2E NLG Challenge,,# 4,METEOR,44.83,TGen,-,Data-to-Text Generation,Findings of the E2E NLG Challenge,/paper/findings-of-the-e2e-nlg-challenge,https://arxiv.org/pdf/1810.01170v1.pdf
E2E NLG Challenge,,# 3,ROUGE-L,68.5,TGen,-,Data-to-Text Generation,Findings of the E2E NLG Challenge,/paper/findings-of-the-e2e-nlg-challenge,https://arxiv.org/pdf/1810.01170v1.pdf
E2E NLG Challenge,,# 3,CIDEr,2.2338,TGen,-,Data-to-Text Generation,Findings of the E2E NLG Challenge,/paper/findings-of-the-e2e-nlg-challenge,https://arxiv.org/pdf/1810.01170v1.pdf
CIFAR-10,,# 54,Percentage correct,82.2,CKN,-,Image Classification,Convolutional Kernel Networks,/paper/convolutional-kernel-networks,https://arxiv.org/pdf/1406.3332v2.pdf
MNIST,,# 4,Percentage error,0.4,CKN,-,Image Classification,Convolutional Kernel Networks,/paper/convolutional-kernel-networks,https://arxiv.org/pdf/1406.3332v2.pdf
STL-10,,# 14,Percentage correct,62.32,CKN,-,Image Classification,Convolutional Kernel Networks,/paper/convolutional-kernel-networks,https://arxiv.org/pdf/1406.3332v2.pdf
Human3.6M,,# 4,Average 3D Error,64.9,Weakly Supervised Transfer Learning,-,3D Human Pose Estimation,Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach,/paper/towards-3d-human-pose-estimation-in-the-wild,https://arxiv.org/pdf/1704.02447v2.pdf
Florence,,# 2,Mean NME,5.2667%,VRN-Guided,-,3D Face Reconstruction,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,/paper/large-pose-3d-face-reconstruction-from-a,https://arxiv.org/pdf/1703.07834v2.pdf
Atari 2600 Montezuma's Revenge,,# 6,Score,1778.8,DQN+SR,-,Atari Games,Count-Based Exploration with the Successor Representation,/paper/count-based-exploration-with-the-successor,https://arxiv.org/pdf/1807.11622v3.pdf
WMT2014 English-French,,# 30,BLEU score,14.36,Unsupervised attentional encoder-decoder + BPE,-,Machine Translation,Unsupervised Neural Machine Translation,/paper/unsupervised-neural-machine-translation,https://arxiv.org/pdf/1710.11041v2.pdf
WMT2015 English-German,,# 6,BLEU score,6.89,Unsupervised attentional encoder-decoder + BPE,-,Machine Translation,Unsupervised Neural Machine Translation,/paper/unsupervised-neural-machine-translation,https://arxiv.org/pdf/1710.11041v2.pdf
Children's Book Test,,# 2,Accuracy-CN,71.9%,NSE,-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Children's Book Test,,# 3,Accuracy-NE,73.2%,NSE,-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Children's Book Test,,# 3,Accuracy-CN,70.7%,GA + feature + fix L(w),-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Children's Book Test,,# 2,Accuracy-NE,74.9%,GA + feature + fix L(w),-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Children's Book Test,,# 4,Accuracy-CN,69.4%,GA reader,-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Children's Book Test,,# 5,Accuracy-NE,71.9%,GA reader,-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
CNN / Daily Mail,,# 2,CNN,77.9,GA Reader,-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
CNN / Daily Mail,,# 1,Daily Mail,80.9,GA Reader,-,Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Quasar,,# 4,EM (Quasar-T),26.4,GA,-,Open-Domain Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
Quasar,,# 5,F1 (Quasar-T),26.4,GA,-,Open-Domain Question Answering,Gated-Attention Readers for Text Comprehension,/paper/gated-attention-readers-for-text,https://arxiv.org/pdf/1606.01549v3.pdf
CIHP,,# 2,Mean IoU,55.8,PGN + ResNet101,-,Human Part Segmentation,Instance-level Human Parsing via Part Grouping Network,/paper/instance-level-human-parsing-via-part,https://arxiv.org/pdf/1808.00157v1.pdf
Photo-Art-50,,# 1,Overall Accuracy,93.02%,SwiDeN,-,Depiction Invariant Object Recognition,SwiDeN : Convolutional Neural Networks For Depiction Invariant Object Recognition,/paper/swiden-convolutional-neural-networks-for,https://arxiv.org/pdf/1607.08764v1.pdf
RaFD,,# 4,Classification Error,8.07%,IcGAN,-,Image-to-Image Translation,Invertible Conditional GANs for image editing,/paper/invertible-conditional-gans-for-image-editing,https://arxiv.org/pdf/1611.06355v1.pdf
MOSI,,# 1,Accuracy,82.31%,MMMU-BA,-,Multimodal Sentiment Analysis,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,/paper/contextual-inter-modal-attention-for-multi,https://aclweb.org/anthology/D18-1382
Quora Question Pairs,,# 3,Accuracy,88.6,Bi-CAS-LSTM,-,Paraphrase Identification,Cell-aware Stacked LSTMs for Modeling Sentences,/paper/cell-aware-stacked-lstms-for-modeling,https://arxiv.org/pdf/1809.02279v1.pdf
SNLI,,# 20,% Test Accuracy,87.0,300D 2-layer Bi-CAS-LSTM,-,Natural Language Inference,Cell-aware Stacked LSTMs for Modeling Sentences,/paper/cell-aware-stacked-lstms-for-modeling,https://arxiv.org/pdf/1809.02279v1.pdf
SST-2 Binary classification,,# 5,Accuracy,91.3,Bi-CAS-LSTM,-,Sentiment Analysis,Cell-aware Stacked LSTMs for Modeling Sentences,/paper/cell-aware-stacked-lstms-for-modeling,https://arxiv.org/pdf/1809.02279v1.pdf
SST-5 Fine-grained classification,,# 5,Accuracy,53.6,Bi-CAS-LSTM,-,Sentiment Analysis,Cell-aware Stacked LSTMs for Modeling Sentences,/paper/cell-aware-stacked-lstms-for-modeling,https://arxiv.org/pdf/1809.02279v1.pdf
AG News,,# 14,Error,9.45,SVDCNN,-,Text Classification,Squeezed Very Deep Convolutional Neural Networks for Text Classification,/paper/squeezed-very-deep-convolutional-neural,https://arxiv.org/pdf/1901.09821v1.pdf
Yelp Binary classification,,# 14,Error,4.74,SVDCNN,-,Sentiment Analysis,Squeezed Very Deep Convolutional Neural Networks for Text Classification,/paper/squeezed-very-deep-convolutional-neural,https://arxiv.org/pdf/1901.09821v1.pdf
Yelp Fine-grained classification,,# 13,Error,46.8,SVDCNN,-,Sentiment Analysis,Squeezed Very Deep Convolutional Neural Networks for Text Classification,/paper/squeezed-very-deep-convolutional-neural,https://arxiv.org/pdf/1901.09821v1.pdf
RumourEval,,# 2,Accuracy,0.78,Bahuleyan and Vechtomova 2017,-,Stance Detection,UWaterloo at SemEval-2017 Task 8: Detecting Stance towards Rumours with Topic Independent Features,/paper/uwaterloo-at-semeval-2017-task-8-detecting,https://aclweb.org/anthology/S17-2080
SNLI,,# 26,% Test Accuracy,86.3,200D decomposable attention model,-,Natural Language Inference,A Decomposable Attention Model for Natural Language Inference,/paper/a-decomposable-attention-model-for-natural,https://arxiv.org/pdf/1606.01933v2.pdf
SNLI,,# 35,% Train Accuracy,89.5,200D decomposable attention model,-,Natural Language Inference,A Decomposable Attention Model for Natural Language Inference,/paper/a-decomposable-attention-model-for-natural,https://arxiv.org/pdf/1606.01933v2.pdf
SNLI,,# 1,Parameters,380k,200D decomposable attention model,-,Natural Language Inference,A Decomposable Attention Model for Natural Language Inference,/paper/a-decomposable-attention-model-for-natural,https://arxiv.org/pdf/1606.01933v2.pdf
SNLI,,# 21,% Test Accuracy,86.8,200D decomposable attention model with intra-sentence attention,-,Natural Language Inference,A Decomposable Attention Model for Natural Language Inference,/paper/a-decomposable-attention-model-for-natural,https://arxiv.org/pdf/1606.01933v2.pdf
SNLI,,# 31,% Train Accuracy,90.5,200D decomposable attention model with intra-sentence attention,-,Natural Language Inference,A Decomposable Attention Model for Natural Language Inference,/paper/a-decomposable-attention-model-for-natural,https://arxiv.org/pdf/1606.01933v2.pdf
SNLI,,# 1,Parameters,580k,200D decomposable attention model with intra-sentence attention,-,Natural Language Inference,A Decomposable Attention Model for Natural Language Inference,/paper/a-decomposable-attention-model-for-natural,https://arxiv.org/pdf/1606.01933v2.pdf
HPatches,,# 4,Viewpoint I AEPE,5.99,FlowNet2,-,Dense Pixel Correspondence Estimation,FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,/paper/flownet-20-evolution-of-optical-flow,https://arxiv.org/pdf/1612.01925v1.pdf
HPatches,,# 4,Viewpoint II AEPE,15.55,FlowNet2,-,Dense Pixel Correspondence Estimation,FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,/paper/flownet-20-evolution-of-optical-flow,https://arxiv.org/pdf/1612.01925v1.pdf
HPatches,,# 4,Viewpoint III AEPE,17.09,FlowNet2,-,Dense Pixel Correspondence Estimation,FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,/paper/flownet-20-evolution-of-optical-flow,https://arxiv.org/pdf/1612.01925v1.pdf
HPatches,,# 4,Viewpoint IV AEPE,22.13,FlowNet2,-,Dense Pixel Correspondence Estimation,FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,/paper/flownet-20-evolution-of-optical-flow,https://arxiv.org/pdf/1612.01925v1.pdf
HPatches,,# 4,Viewpoint V AEPE,30.68,FlowNet2,-,Dense Pixel Correspondence Estimation,FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,/paper/flownet-20-evolution-of-optical-flow,https://arxiv.org/pdf/1612.01925v1.pdf
Atari 2600 Alien,,# 3,Score,4203.8,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Alien,,# 12,Score,1334.7,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Amidar,,# 20,Score,129.1,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Amidar,,# 4,Score,1838.9,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Assault,,# 9,Score,6548.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Assault,,# 7,Score,7672.1,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Asterix,,# 5,Score,31527.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Asterix,,# 7,Score,22484.5,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Asteroids,,# 7,Score,2654.3,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Asteroids,,# 9,Score,1745.1,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Atlantis,,# 13,Score,357324.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Atlantis,,# 15,Score,330647.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Bank Heist,,# 15,Score,876.6,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Bank Heist,,# 7,Score,1054.6,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Battle Zone,,# 6,Score,31530.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Battle Zone,,# 12,Score,25520.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Beam Rider,,# 7,Score,23384.2,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Beam Rider,,# 3,Score,31181.3,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Berzerk,,# 6,Score,1305.6,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Berzerk,,# 12,Score,865.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Bowling,,# 13,Score,47.9,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Bowling,,# 11,Score,52.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Boxing,,# 6,Score,95.6,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Boxing,,# 14,Score,72.3,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Breakout,,# 11,Score,373.9,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Breakout,,# 18,Score,343.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Centipede,,# 16,Score,4463.2,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Centipede,,# 20,Score,3489.1,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Chopper Command,,# 6,Score,8600.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Chopper Command,,# 14,Score,4635.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Crazy Climber,,# 5,Score,141161.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Crazy Climber,,# 9,Score,127512.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Demon Attack,,# 12,Score,61277.5,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Demon Attack,,# 9,Score,71846.4,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Double Dunk,,# 1,Score,18.5,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Double Dunk,,# 2,Score,16.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Enduro,,# 9,Score,1831.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Enduro,,# 6,Score,2093.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Fishing Derby,,# 4,Score,39.5,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Fishing Derby,,# 12,Score,9.8,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Freeway,,# 13,Score,28.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Freeway,,# 3,Score,33.7,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Frostbite,,# 4,Score,4380.1,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Frostbite,,# 8,Score,3510.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Gopher,,# 7,Score,32487.2,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Gopher,,# 5,Score,34858.8,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Gravitar,,# 5,Score,548.5,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Gravitar,,# 20,Score,269.5,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 HERO,,# 9,Score,20889.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 HERO,,# 6,Score,23037.7,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Ice Hockey,,# 4,Score,-0.2,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Ice Hockey,,# 1,Score,1.3,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 James Bond,,# 3,Score,3961.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 James Bond,,# 2,Score,5148.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Kangaroo,,# 8,Score,12185.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Kangaroo,,# 1,Score,16200.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Krull,,# 14,Score,6872.8,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Krull,,# 6,Score,9728.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Kung-Fu Master,,# 5,Score,39581.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Kung-Fu Master,,# 10,Score,31676.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Montezuma's Revenge,,# 16,Score,51.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Ms. Pacman,,# 1,Score,6518.7,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Ms. Pacman,,# 12,Score,1865.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Name This Game,,# 6,Score,12270.5,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Name This Game,,# 12,Score,10497.6,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Pong,,# 3,Score,20.6,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Pong,,# 6,Score,18.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Private Eye,,# 9,Score,670.7,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Private Eye,,# 16,Score,200.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Q*Bert,,# 6,Score,16256.5,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Q*Bert,,# 17,Score,9944.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 River Raid,,# 12,Score,11807.2,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 River Raid,,# 8,Score,14522.3,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Road Runner,,# 9,Score,52264.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Road Runner,,# 6,Score,57608.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Robotank,,# 6,Score,62.6,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Robotank,,# 12,Score,56.2,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Seaquest,,# 5,Score,26357.8,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Seaquest,,# 6,Score,25463.7,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Space Invaders,,# 11,Score,2865.8,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Space Invaders,,# 9,Score,3912.1,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Star Gunner,,# 9,Score,63302.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Star Gunner,,# 10,Score,61582.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Tennis,,# 13,Score,-5.3,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Tennis,,# 8,Score,0.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Time Pilot,,# 5,Score,9197.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Time Pilot,,# 13,Score,5963.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Tutankham,,# 7,Score,204.6,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Tutankham,,# 19,Score,56.9,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Up and Down,,# 14,Score,16154.1,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Up and Down,,# 16,Score,12157.4,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Venture,,# 16,Score,94.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Venture,,# 19,Score,54.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Video Pinball,,# 10,Score,295972.8,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Video Pinball,,# 11,Score,282007.3,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Wizard of Wor,,# 15,Score,4802.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Wizard of Wor,,# 13,Score,5727.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Zaxxon,,# 10,Score,10469.0,Prior noop,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
Atari 2600 Zaxxon,,# 13,Score,9474.0,Prior hs,-,Atari Games,Prioritized Experience Replay,/paper/prioritized-experience-replay,https://arxiv.org/pdf/1511.05952v4.pdf
WMT2015 English-German,,# 4,BLEU score,22.8,BPE word segmentation,-,Machine Translation,Neural Machine Translation of Rare Words with Subword Units,/paper/neural-machine-translation-of-rare-words-with,https://arxiv.org/pdf/1508.07909v5.pdf
WMT2015 English-Russian,,# 1,BLEU score,20.9,C2-50k Segmentation,-,Machine Translation,Neural Machine Translation of Rare Words with Subword Units,/paper/neural-machine-translation-of-rare-words-with,https://arxiv.org/pdf/1508.07909v5.pdf
CelebA,,# 1,Error,8.25,MGDA-UB,-,Multi-Task Learning,Multi-Task Learning as Multi-Objective Optimization,/paper/multi-task-learning-as-multi-objective,https://arxiv.org/pdf/1810.04650v2.pdf
Cityscapes,,# 1,mIoU,66.63,MultiObjectiveOptimization,-,Multi-Task Learning,Multi-Task Learning as Multi-Objective Optimization,/paper/multi-task-learning-as-multi-objective,https://arxiv.org/pdf/1810.04650v2.pdf
SQuAD1.1,,# 33,EM,79.608,SAN (ensemble model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD1.1,,# 35,F1,86.49600000000001,SAN (ensemble model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD1.1,,# 58,EM,76.828,SAN (single model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD1.1,,# 60,F1,84.396,SAN (single model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD2.0,,# 67,EM,71.316,SAN (ensemble model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD2.0,,# 71,F1,73.704,SAN (ensemble model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD2.0,,# 74,EM,68.653,SAN (single model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
SQuAD2.0,,# 78,F1,71.439,SAN (single model),-,Question Answering,Stochastic Answer Networks for Machine Reading Comprehension,/paper/stochastic-answer-networks-for-machine,https://arxiv.org/pdf/1712.03556v2.pdf
ImageNet,,# 4,MAP,6.0,Online Instance Classifier Refinement,-,Weakly Supervised Object Detection,Multiple Instance Detection Network with Online Instance Classifier Refinement,/paper/multiple-instance-detection-network-with,https://arxiv.org/pdf/1704.00138v1.pdf
PASCAL VOC 2007,,# 7,MAP,47.0,OICR-Ens + FRCNN,-,Weakly Supervised Object Detection,Multiple Instance Detection Network with Online Instance Classifier Refinement,/paper/multiple-instance-detection-network-with,https://arxiv.org/pdf/1704.00138v1.pdf
PASCAL VOC 2012,,# 5,MAP,42.5,OICR-Ens + FRCNN,-,Weakly Supervised Object Detection,Multiple Instance Detection Network with Online Instance Classifier Refinement,/paper/multiple-instance-detection-network-with,https://arxiv.org/pdf/1704.00138v1.pdf
ImageNet 64x64,,# 4,Bits per byte,3.7,Parallel Multiscale,-,Image Generation,Parallel Multiscale Autoregressive Density Estimation,/paper/parallel-multiscale-autoregressive-density,https://arxiv.org/pdf/1703.03664v1.pdf
RaFD,,# 1,Classification Error,2.12%,StarGAN,-,Image-to-Image Translation,StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation,/paper/stargan-unified-generative-adversarial,https://arxiv.org/pdf/1711.09020v3.pdf
bAbi,,# 3,Mean Error Rate,1.2%,ReMO,-,Question Answering,Finding ReMO (Related Memory Object): A Simple Neural Architecture for Text based Reasoning,/paper/finding-remo-related-memory-object-a-simple,https://arxiv.org/pdf/1801.08459v2.pdf
Knowledge-based:,,# 1,All,66.9,WSD-TM,-,Word Sense Disambiguation,Knowledge-based Word Sense Disambiguation using Topic Models,/paper/knowledge-based-word-sense-disambiguation,https://arxiv.org/pdf/1801.01900v1.pdf
Knowledge-based:,,# 5,Senseval 2,**69.0**,WSD-TM,-,Word Sense Disambiguation,Knowledge-based Word Sense Disambiguation using Topic Models,/paper/knowledge-based-word-sense-disambiguation,https://arxiv.org/pdf/1801.01900v1.pdf
Knowledge-based:,,# 5,Senseval 3,**66.9**,WSD-TM,-,Word Sense Disambiguation,Knowledge-based Word Sense Disambiguation using Topic Models,/paper/knowledge-based-word-sense-disambiguation,https://arxiv.org/pdf/1801.01900v1.pdf
Knowledge-based:,,# 5,SemEval 2007,**55.6**,WSD-TM,-,Word Sense Disambiguation,Knowledge-based Word Sense Disambiguation using Topic Models,/paper/knowledge-based-word-sense-disambiguation,https://arxiv.org/pdf/1801.01900v1.pdf
Knowledge-based:,,# 2,SemEval 2013,65.3,WSD-TM,-,Word Sense Disambiguation,Knowledge-based Word Sense Disambiguation using Topic Models,/paper/knowledge-based-word-sense-disambiguation,https://arxiv.org/pdf/1801.01900v1.pdf
Knowledge-based:,,# 1,SemEval 2015,69.6,WSD-TM,-,Word Sense Disambiguation,Knowledge-based Word Sense Disambiguation using Topic Models,/paper/knowledge-based-word-sense-disambiguation,https://arxiv.org/pdf/1801.01900v1.pdf
Polyvore,,# 1,Accuracy,0.7813,NGNN,-,Recommendation Systems,Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks,/paper/dressing-as-a-whole-outfit-compatibility,https://arxiv.org/pdf/1902.08009v1.pdf
PASCAL Context,,# 3,mIoU,52.5,DUpsampling,-,Semantic Segmentation,Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation,/paper/decoders-matter-for-semantic-segmentation,https://arxiv.org/pdf/1903.02120v3.pdf
RACE,,# 2,RACE-m,60.2,BiAttention MRU,-,Question Answering,Multi-range Reasoning for Machine Comprehension,/paper/multi-range-reasoning-for-machine,https://arxiv.org/pdf/1803.09074v1.pdf
RACE,,# 2,RACE-h,50.3,BiAttention MRU,-,Question Answering,Multi-range Reasoning for Machine Comprehension,/paper/multi-range-reasoning-for-machine,https://arxiv.org/pdf/1803.09074v1.pdf
RACE,,# 2,RACE,53.3,BiAttention MRU,-,Question Answering,Multi-range Reasoning for Machine Comprehension,/paper/multi-range-reasoning-for-machine,https://arxiv.org/pdf/1803.09074v1.pdf
CoNLL-2014 A1,,# 4,F0.5,21.87,Ann+PAT+MT,-,Grammatical Error Detection,Artificial Error Generation with Machine Translation and Syntactic Patterns,/paper/artificial-error-generation-with-machine,https://arxiv.org/pdf/1707.05236v1.pdf
CoNLL-2014 A2,,# 3,F0.5,30.13,Ann+PAT+MT,-,Grammatical Error Detection,Artificial Error Generation with Machine Translation and Syntactic Patterns,/paper/artificial-error-generation-with-machine,https://arxiv.org/pdf/1707.05236v1.pdf
FCE,,# 2,F0.5,49.11,Ann+PAT+MT,-,Grammatical Error Detection,Artificial Error Generation with Machine Translation and Syntactic Patterns,/paper/artificial-error-generation-with-machine,https://arxiv.org/pdf/1707.05236v1.pdf
VOT2017/18,,# 2,Expected Average Overlap (EAO),0.38,SiamMask,-,Visual Object Tracking,Fast Online Object Tracking and Segmentation: A Unifying Approach,/paper/fast-online-object-tracking-and-segmentation,https://arxiv.org/pdf/1812.05050v1.pdf
ImageNet,,# 8,Top 1 Accuracy,80.1%,Inception ResNet V2,-,Image Classification,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",/paper/inception-v4-inception-resnet-and-the-impact,https://arxiv.org/pdf/1602.07261v2.pdf
ImageNet,,# 6,Top 5 Accuracy,95.1%,Inception ResNet V2,-,Image Classification,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",/paper/inception-v4-inception-resnet-and-the-impact,https://arxiv.org/pdf/1602.07261v2.pdf
SNLI,,# 44,% Test Accuracy,81.4,1024D GRU encoders w/ unsupervised 'skip-thoughts' pre-training,-,Natural Language Inference,Order-Embeddings of Images and Language,/paper/order-embeddings-of-images-and-language,https://arxiv.org/pdf/1511.06361v6.pdf
SNLI,,# 2,% Train Accuracy,98.8,1024D GRU encoders w/ unsupervised 'skip-thoughts' pre-training,-,Natural Language Inference,Order-Embeddings of Images and Language,/paper/order-embeddings-of-images-and-language,https://arxiv.org/pdf/1511.06361v6.pdf
SNLI,,# 1,Parameters,15m,1024D GRU encoders w/ unsupervised 'skip-thoughts' pre-training,-,Natural Language Inference,Order-Embeddings of Images and Language,/paper/order-embeddings-of-images-and-language,https://arxiv.org/pdf/1511.06361v6.pdf
Caltech,,# 17,Reasonable Miss Rate,20.9,TA-CNN,-,Pedestrian Detection,Pedestrian Detection aided by Deep Learning Semantic Tasks,/paper/pedestrian-detection-aided-by-deep-learning,https://arxiv.org/pdf/1412.0069v1.pdf
NarrativeQA,,# 2,BLEU-1,43.63,MHPGM + NOIC,-,Question Answering,Commonsense for Generative Multi-Hop Question Answering Tasks,/paper/commonsense-for-generative-multi-hop-question,https://arxiv.org/pdf/1809.06309v2.pdf
NarrativeQA,,# 3,BLEU-4,21.07,MHPGM + NOIC,-,Question Answering,Commonsense for Generative Multi-Hop Question Answering Tasks,/paper/commonsense-for-generative-multi-hop-question,https://arxiv.org/pdf/1809.06309v2.pdf
NarrativeQA,,# 3,METEOR,19.03,MHPGM + NOIC,-,Question Answering,Commonsense for Generative Multi-Hop Question Answering Tasks,/paper/commonsense-for-generative-multi-hop-question,https://arxiv.org/pdf/1809.06309v2.pdf
NarrativeQA,,# 3,Rouge-L,44.16,MHPGM + NOIC,-,Question Answering,Commonsense for Generative Multi-Hop Question Answering Tasks,/paper/commonsense-for-generative-multi-hop-question,https://arxiv.org/pdf/1809.06309v2.pdf
WikiHop,,# 3,Test,57.9,MHPGM + NOIC,-,Question Answering,Commonsense for Generative Multi-Hop Question Answering Tasks,/paper/commonsense-for-generative-multi-hop-question,https://arxiv.org/pdf/1809.06309v2.pdf
CIFAR-10,,# 1,NLL Test,2.80,Sparse Transformer 59M (strided),-,Image Generation,Generating Long Sequences with Sparse Transformers,/paper/generating-long-sequences-with-sparse,https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf
"Classical music, 5 seconds at 12 kHz",,# 1,Bits per byte,1.97,Sparse Transformer 152M (strided),-,Audio Generation,Generating Long Sequences with Sparse Transformers,/paper/generating-long-sequences-with-sparse,https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf
enwiki8,,# 3,Bit per Character (BPC),0.99,Sparse Transformer (fixed),-,Language Modelling,Generating Long Sequences with Sparse Transformers,/paper/generating-long-sequences-with-sparse,https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf
enwiki8,,# 1,Number of params,95M,Sparse Transformer (fixed),-,Language Modelling,Generating Long Sequences with Sparse Transformers,/paper/generating-long-sequences-with-sparse,https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf
ImageNet 64x64,,# 1,Bits per byte,3.44,Sparse Transformer 152M (strided),-,Image Generation,Generating Long Sequences with Sparse Transformers,/paper/generating-long-sequences-with-sparse,https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf
Yahoo Questions,,# 1,NLL,326.7,Aggressive VAE,-,Text Generation,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,/paper/lagging-inference-networks-and-posterior,https://arxiv.org/pdf/1901.05534v2.pdf
Yahoo Questions,,# 3,KL,5.7,Aggressive VAE,-,Text Generation,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,/paper/lagging-inference-networks-and-posterior,https://arxiv.org/pdf/1901.05534v2.pdf
Yahoo Questions,,# 1,Perplexity,59.7,Aggressive VAE,-,Text Generation,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,/paper/lagging-inference-networks-and-posterior,https://arxiv.org/pdf/1901.05534v2.pdf
COCO,,# 8,Bounding Box AP,45.0,ResNeXt-152 + 1 NL block,-,Object Detection,Non-local Neural Networks,/paper/non-local-neural-networks,https://arxiv.org/pdf/1711.07971v3.pdf
COCO,,# 2,Average Precision,40.3%,ResNeXt-152 + 1 NL block,-,Instance Segmentation,Non-local Neural Networks,/paper/non-local-neural-networks,https://arxiv.org/pdf/1711.07971v3.pdf
COCO,,# 6,Validation AP,66.5,"Mask R-CNN + NL blocks (4 in head, 1 in backbone)",-,Keypoint Detection,Non-local Neural Networks,/paper/non-local-neural-networks,https://arxiv.org/pdf/1711.07971v3.pdf
SST-2 Binary classification,,# 4,Accuracy,91.8,bmLSTM,-,Sentiment Analysis,Learning to Generate Reviews and Discovering Sentiment,/paper/learning-to-generate-reviews-and-discovering,https://arxiv.org/pdf/1704.01444v2.pdf
SUBJ,,# 3,Accuracy,94.6,Byte mLSTM,-,Subjectivity Analysis,Learning to Generate Reviews and Discovering Sentiment,/paper/learning-to-generate-reviews-and-discovering,https://arxiv.org/pdf/1704.01444v2.pdf
Medical domain,,# 7,MAP,8.13,ADAPT,-,Hypernym Discovery,ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,/paper/adapt-at-semeval-2018-task-9-skip-gram-word,https://aclweb.org/anthology/S18-1151
Medical domain,,# 7,MRR,20.56,ADAPT,-,Hypernym Discovery,ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,/paper/adapt-at-semeval-2018-task-9-skip-gram-word,https://aclweb.org/anthology/S18-1151
Medical domain,,# 7,[emailÂ protected],8.32,ADAPT,-,Hypernym Discovery,ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,/paper/adapt-at-semeval-2018-task-9-skip-gram-word,https://aclweb.org/anthology/S18-1151
Music domain,,# 6,MAP,2.63,ADAPT,-,Hypernym Discovery,ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,/paper/adapt-at-semeval-2018-task-9-skip-gram-word,https://aclweb.org/anthology/S18-1151
Music domain,,# 6,MRR,7.46,ADAPT,-,Hypernym Discovery,ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,/paper/adapt-at-semeval-2018-task-9-skip-gram-word,https://aclweb.org/anthology/S18-1151
Music domain,,# 6,[emailÂ protected],2.64,ADAPT,-,Hypernym Discovery,ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,/paper/adapt-at-semeval-2018-task-9-skip-gram-word,https://aclweb.org/anthology/S18-1151
ADE20K-Outdoor Labels-to-Photos,,# 4,mIoU,13.1,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 2,Accuracy,74.7%,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 2,FID,67.7,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
Cityscapes Labels-to-Photo,,# 6,Class IOU,,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
Cityscapes Labels-to-Photo,,# 5,Per-class Accuracy,,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
Cityscapes Labels-to-Photo,,# 4,Per-pixel Accuracy,75.5%,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
Cityscapes Labels-to-Photo,,# 4,mIoU,47.2,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
Cityscapes Labels-to-Photo,,# 1,FID,49.7,SIMS,-,Image-to-Image Translation,Semi-parametric Image Synthesis,/paper/semi-parametric-image-synthesis,https://arxiv.org/pdf/1804.10992v1.pdf
UNBC-McMaster ShoulderPain dataset,,# 1,MAE,0.389,Regularized Deep Regressor,-,Pain Intensity Regression,Regularizing Face Verification Nets For Pain Intensity Regression,/paper/regularizing-face-verification-nets-for-pain,https://arxiv.org/pdf/1702.06925v3.pdf
WMT2014 English-French,,# 2,BLEU score,43.2,Transformer Big,-,Machine Translation,Scaling Neural Machine Translation,/paper/scaling-neural-machine-translation,https://arxiv.org/pdf/1806.00187v3.pdf
WMT2014 English-German,,# 3,BLEU score,29.3,Transformer Big,-,Machine Translation,Scaling Neural Machine Translation,/paper/scaling-neural-machine-translation,https://arxiv.org/pdf/1806.00187v3.pdf
Market-1501,,# 12,Rank-1,85.1,DJL,-,Person Re-Identification,Person Re-Identification by Deep Joint Learning of Multi-Loss Classification,/paper/person-re-identification-by-deep-joint,https://arxiv.org/pdf/1705.04724v2.pdf
Market-1501,,# 15,MAP,65.5,DJL,-,Person Re-Identification,Person Re-Identification by Deep Joint Learning of Multi-Loss Classification,/paper/person-re-identification-by-deep-joint,https://arxiv.org/pdf/1705.04724v2.pdf
MS MARCO,,# 3,Rouge-L,51.63,VNET,-,Question Answering,Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,/paper/multi-passage-machine-reading-comprehension,https://arxiv.org/pdf/1805.02220v2.pdf
MS MARCO,,# 2,BLEU-1,54.37,VNET,-,Question Answering,Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,/paper/multi-passage-machine-reading-comprehension,https://arxiv.org/pdf/1805.02220v2.pdf
BSD100 - 4x upscaling,,# 28,PSNR,26.5707,4PP-EUSR,-,Image Super-Resolution,Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality,/paper/deep-learning-based-image-super-resolution,https://arxiv.org/pdf/1809.04789v2.pdf
BSD100 - 4x upscaling,,# 28,SSIM,0.69,4PP-EUSR,-,Image Super-Resolution,Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality,/paper/deep-learning-based-image-super-resolution,https://arxiv.org/pdf/1809.04789v2.pdf
Set14 - 4x upscaling,,# 29,PSNR,27.6222,4PP-EUSR,-,Image Super-Resolution,Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality,/paper/deep-learning-based-image-super-resolution,https://arxiv.org/pdf/1809.04789v2.pdf
Set14 - 4x upscaling,,# 30,SSIM,0.7419,4PP-EUSR,-,Image Super-Resolution,Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality,/paper/deep-learning-based-image-super-resolution,https://arxiv.org/pdf/1809.04789v2.pdf
Set5 - 4x upscaling,,# 22,PSNR,31.0846,4PP-EUSR,-,Image Super-Resolution,Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality,/paper/deep-learning-based-image-super-resolution,https://arxiv.org/pdf/1809.04789v2.pdf
Set5 - 4x upscaling,,# 27,SSIM,0.8652,4PP-EUSR,-,Image Super-Resolution,Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality,/paper/deep-learning-based-image-super-resolution,https://arxiv.org/pdf/1809.04789v2.pdf
E2E NLG Challenge,,# 4,BLEU,65.61,Sys1-Primary,-,Data-to-Text Generation,"TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation",/paper/tnt-nlg-system-1-using-a-statistical-nlg-to,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-TNT_NLG1.pdf
E2E NLG Challenge,,# 4,NIST,8.5105,Sys1-Primary,-,Data-to-Text Generation,"TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation",/paper/tnt-nlg-system-1-using-a-statistical-nlg-to,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-TNT_NLG1.pdf
E2E NLG Challenge,,# 3,METEOR,45.17,Sys1-Primary,-,Data-to-Text Generation,"TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation",/paper/tnt-nlg-system-1-using-a-statistical-nlg-to,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-TNT_NLG1.pdf
E2E NLG Challenge,,# 4,ROUGE-L,68.39,Sys1-Primary,-,Data-to-Text Generation,"TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation",/paper/tnt-nlg-system-1-using-a-statistical-nlg-to,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-TNT_NLG1.pdf
E2E NLG Challenge,,# 4,CIDEr,2.2183,Sys1-Primary,-,Data-to-Text Generation,"TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation",/paper/tnt-nlg-system-1-using-a-statistical-nlg-to,https://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-TNT_NLG1.pdf
DensePose-COCO,,# 2,AP,55.8,DensePose + keypoints,-,Pose Estimation,DensePose: Dense Human Pose Estimation In The Wild,/paper/densepose-dense-human-pose-estimation-in-the,https://arxiv.org/pdf/1802.00434v1.pdf
COCO,,# 26,Bounding Box AP,39.5,FPN (ResNet101 backbone),-,Object Detection,ChainerCV: a Library for Deep Learning in Computer Vision,/paper/chainercv-a-library-for-deep-learning-in,https://arxiv.org/pdf/1708.08169v1.pdf
Market-1501,,# 17,Rank-1,82.21,SSM,-,Person Re-Identification,Scalable Person Re-identification on Supervised Smoothed Manifold,/paper/scalable-person-re-identification-on,https://arxiv.org/pdf/1703.08359v1.pdf
Market-1501,,# 11,MAP,68.8,SSM,-,Person Re-Identification,Scalable Person Re-identification on Supervised Smoothed Manifold,/paper/scalable-person-re-identification-on,https://arxiv.org/pdf/1703.08359v1.pdf
HPatches,,# 3,Viewpoint I AEPE,5.84,DeepMatching*,-,Dense Pixel Correspondence Estimation,DeepMatching: Hierarchical Deformable Dense Matching,/paper/deepmatching-hierarchical-deformable-dense,https://arxiv.org/pdf/1506.07656v2.pdf
HPatches,,# 1,Viewpoint II AEPE,4.63,DeepMatching*,-,Dense Pixel Correspondence Estimation,DeepMatching: Hierarchical Deformable Dense Matching,/paper/deepmatching-hierarchical-deformable-dense,https://arxiv.org/pdf/1506.07656v2.pdf
HPatches,,# 2,Viewpoint III AEPE,12.43,DeepMatching*,-,Dense Pixel Correspondence Estimation,DeepMatching: Hierarchical Deformable Dense Matching,/paper/deepmatching-hierarchical-deformable-dense,https://arxiv.org/pdf/1506.07656v2.pdf
HPatches,,# 2,Viewpoint IV AEPE,12.17,DeepMatching*,-,Dense Pixel Correspondence Estimation,DeepMatching: Hierarchical Deformable Dense Matching,/paper/deepmatching-hierarchical-deformable-dense,https://arxiv.org/pdf/1506.07656v2.pdf
HPatches,,# 2,Viewpoint V AEPE,22.55,DeepMatching*,-,Dense Pixel Correspondence Estimation,DeepMatching: Hierarchical Deformable Dense Matching,/paper/deepmatching-hierarchical-deformable-dense,https://arxiv.org/pdf/1506.07656v2.pdf
DukeMTMC-reID,,# 14,Rank-1,70.69,APR,-,Person Re-Identification,Improving Person Re-identification by Attribute and Identity Learning,/paper/improving-person-re-identification-by,https://arxiv.org/pdf/1703.07220v2.pdf
DukeMTMC-reID,,# 12,MAP,51.88,APR,-,Person Re-Identification,Improving Person Re-identification by Attribute and Identity Learning,/paper/improving-person-re-identification-by,https://arxiv.org/pdf/1703.07220v2.pdf
"CIFAR-10, 4000 Labels",,# 5,Accuracy,87.84,Pi Model,-,Semi-Supervised Image Classification,Temporal Ensembling for Semi-Supervised Learning,/paper/temporal-ensembling-for-semi-supervised,https://arxiv.org/pdf/1610.02242v3.pdf
"SVHN, 1000 labels",,# 3,Accuracy,95.58,Pi Model,-,Semi-Supervised Image Classification,Temporal Ensembling for Semi-Supervised Learning,/paper/temporal-ensembling-for-semi-supervised,https://arxiv.org/pdf/1610.02242v3.pdf
Data3DâR2N2,,# 1,Avg F1,66.39,MVD,-,3D Object Reconstruction,Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation,/paper/multi-view-silhouette-and-depth-decomposition,https://arxiv.org/pdf/1802.09987v3.pdf
KITTI Cars Easy,,# 3,AP,83.71%,RoarNet,-,3D Object Detection,RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement,/paper/roarnet-a-robust-3d-object-detection-based-on,https://arxiv.org/pdf/1811.03818v1.pdf
KITTI Cars Hard,,# 6,AP,59.16%,RoarNet,-,3D Object Detection,RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement,/paper/roarnet-a-robust-3d-object-detection-based-on,https://arxiv.org/pdf/1811.03818v1.pdf
KITTI Cars Moderate,,# 4,AP,73.04%,RoarNet,-,3D Object Detection,RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement,/paper/roarnet-a-robust-3d-object-detection-based-on,https://arxiv.org/pdf/1811.03818v1.pdf
ImageNet,,# 10,Top 1 Accuracy,79.4%,MultiGrain R50-AA-500,-,Image Classification,MultiGrain: a unified image embedding for classes and instances,/paper/multigrain-a-unified-image-embedding-for,https://arxiv.org/pdf/1902.05509v2.pdf
ImageNet,,# 7,Top 5 Accuracy,94.8%,MultiGrain R50-AA-500,-,Image Classification,MultiGrain: a unified image embedding for classes and instances,/paper/multigrain-a-unified-image-embedding-for,https://arxiv.org/pdf/1902.05509v2.pdf
ImageNet,,# 17,Top 1 Accuracy,78.2%,MultiGrain R50-AA-224,-,Image Classification,MultiGrain: a unified image embedding for classes and instances,/paper/multigrain-a-unified-image-embedding-for,https://arxiv.org/pdf/1902.05509v2.pdf
ImageNet,,# 12,Top 5 Accuracy,93.9%,MultiGrain R50-AA-224,-,Image Classification,MultiGrain: a unified image embedding for classes and instances,/paper/multigrain-a-unified-image-embedding-for,https://arxiv.org/pdf/1902.05509v2.pdf
INRIA Holidays,,# 2,Mean mAP,91.8%,MultiGrain R50 @ 500,-,Image Retrieval,MultiGrain: a unified image embedding for classes and instances,/paper/multigrain-a-unified-image-embedding-for,https://arxiv.org/pdf/1902.05509v2.pdf
INRIA Holidays,,# 1,Mean mAP,92.5%,MultiGrain R50 @ 800,-,Image Retrieval,MultiGrain: a unified image embedding for classes and instances,/paper/multigrain-a-unified-image-embedding-for,https://arxiv.org/pdf/1902.05509v2.pdf
COCO,,# 23,Bounding Box AP,40.6,FoveaBox + ResNet-101,-,Object Detection,FoveaBox: Beyond Anchor-based Object Detector,/paper/foveabox-beyond-anchor-based-object-detector,https://arxiv.org/pdf/1904.03797v1.pdf
COCO,,# 18,Bounding Box AP,42.1,FoveaBox + ResNeXt-101,-,Object Detection,FoveaBox: Beyond Anchor-based Object Detector,/paper/foveabox-beyond-anchor-based-object-detector,https://arxiv.org/pdf/1904.03797v1.pdf
Vid4 - 4x upscaling,,# 10,PSNR,24.43,BRCN,-,Video Super-Resolution,Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution,/paper/bidirectional-recurrent-convolutional,https://papers.nips.cc/paper/5778-bidirectional-recurrent-convolutional-networks-for-multi-frame-super-resolution.pdf
Vid4 - 4x upscaling,,# 2,SSIM,0.662,BRCN,-,Video Super-Resolution,Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution,/paper/bidirectional-recurrent-convolutional,https://papers.nips.cc/paper/5778-bidirectional-recurrent-convolutional-networks-for-multi-frame-super-resolution.pdf
KITTI Novel View Synthesis,,# 1,SSIM,0.626,Multi-view to Novel View,-,Novel View Synthesis,Multi-view to Novel view: Synthesizing Novel Views with Self-Learned Confidence,/paper/multi-view-to-novel-view-synthesizing-novel-1,https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf
ShapeNet Car,,# 1,SSIM,0.9229999999999999,Multi-view to Novel View,-,Novel View Synthesis,Multi-view to Novel view: Synthesizing Novel Views with Self-Learned Confidence,/paper/multi-view-to-novel-view-synthesizing-novel-1,https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf
ShapeNet Chair,,# 1,SSIM,0.895,Multi-view to Novel View,-,Novel View Synthesis,Multi-view to Novel view: Synthesizing Novel Views with Self-Learned Confidence,/paper/multi-view-to-novel-view-synthesizing-novel-1,https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf
Synthia Novel View Synthesis,,# 1,SSIM,0.6970000000000001,Multi-view to Novel View,-,Novel View Synthesis,Multi-view to Novel view: Synthesizing Novel Views with Self-Learned Confidence,/paper/multi-view-to-novel-view-synthesizing-novel-1,https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf
Human3.6M,,# 3,Average 3D Error,59.9,Neural Body Fitting (NBF),-,3D Human Pose Estimation,Neural Body Fitting: Unifying Deep Learning and Model-Based Human Pose and Shape Estimation,/paper/neural-body-fitting-unifying-deep-learning,https://arxiv.org/pdf/1808.05942v1.pdf
CUHK,,# 2,SSIM,61.56%,PS2-MAN,-,Face Sketch Synthesis,High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks,/paper/high-quality-facial-photo-sketch-synthesis,https://arxiv.org/pdf/1710.10182v2.pdf
CUHK,,# 2,FSIM,73.61%,PS2-MAN,-,Face Sketch Synthesis,High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks,/paper/high-quality-facial-photo-sketch-synthesis,https://arxiv.org/pdf/1710.10182v2.pdf
PeopleArt,,# 1,MAP,59,Fast R-CNN (VGG16),-,Object Detection,Detecting People in Artwork with CNNs,/paper/detecting-people-in-artwork-with-cnns,https://arxiv.org/pdf/1610.08871v1.pdf
ModelNet40,,# 1,Accuracy,93.8%,MVCNN-MultiRes,-,3D Object Recognition,Volumetric and Multi-View CNNs for Object Classification on 3D Data,/paper/volumetric-and-multi-view-cnns-for-object,https://arxiv.org/pdf/1604.03265v2.pdf
SQuAD1.1,,# 93,EM,72.139,BiDAF + Self Attention (single model),-,Question Answering,Simple and Effective Multi-Paragraph Reading Comprehension,/paper/simple-and-effective-multi-paragraph-reading,https://arxiv.org/pdf/1710.10723v2.pdf
SQuAD1.1,,# 91,F1,81.048,BiDAF + Self Attention (single model),-,Question Answering,Simple and Effective Multi-Paragraph Reading Comprehension,/paper/simple-and-effective-multi-paragraph-reading,https://arxiv.org/pdf/1710.10723v2.pdf
TriviaQA,,# 2,EM,66.37,S-Norm,-,Question Answering,Simple and Effective Multi-Paragraph Reading Comprehension,/paper/simple-and-effective-multi-paragraph-reading,https://arxiv.org/pdf/1710.10723v2.pdf
TriviaQA,,# 2,F1,71.32,S-Norm,-,Question Answering,Simple and Effective Multi-Paragraph Reading Comprehension,/paper/simple-and-effective-multi-paragraph-reading,https://arxiv.org/pdf/1710.10723v2.pdf
Tox21,,# 3,AUC,0.845,SNN,-,Drug Discovery,Self-Normalizing Neural Networks,/paper/self-normalizing-neural-networks,https://arxiv.org/pdf/1706.02515v5.pdf
SemEval 2018 Task 1E-c,,# 1,Macro-F1,56.1,Transformer (finetune),-,Emotion Classification,Practical Text Classification With Large Pre-Trained Language Models,/paper/practical-text-classification-with-large-pre,https://arxiv.org/pdf/1812.01207v1.pdf
SST-2 Binary classification,,# 7,Accuracy,90.9,Transformer (finetune),-,Sentiment Analysis,Practical Text Classification With Large Pre-Trained Language Models,/paper/practical-text-classification-with-large-pre,https://arxiv.org/pdf/1812.01207v1.pdf
CUFS,,# 1,FSIM,72.56%,Residual net + Pseudo Sketch Feature Loss + LSGAN,-,Face Sketch Synthesis,Semi-Supervised Learning for Face Sketch Synthesis in the Wild,/paper/semi-supervised-learning-for-face-sketch,https://arxiv.org/pdf/1812.04929v1.pdf
CUFS,,# 1,SSIM,54.63%,Residual net + Pseudo Sketch Feature Loss + LSGAN,-,Face Sketch Synthesis,Semi-Supervised Learning for Face Sketch Synthesis in the Wild,/paper/semi-supervised-learning-for-face-sketch,https://arxiv.org/pdf/1812.04929v1.pdf
CUFSF,,# 1,FSIM,71.59%,Residual net + Pseudo Sketch Feature Loss + LSGAN,-,Face Sketch Synthesis,Semi-Supervised Learning for Face Sketch Synthesis in the Wild,/paper/semi-supervised-learning-for-face-sketch,https://arxiv.org/pdf/1812.04929v1.pdf
CUFSF,,# 1,SSIM,40.85%,Residual net + Pseudo Sketch Feature Loss + LSGAN,-,Face Sketch Synthesis,Semi-Supervised Learning for Face Sketch Synthesis in the Wild,/paper/semi-supervised-learning-for-face-sketch,https://arxiv.org/pdf/1812.04929v1.pdf
CUHK,,# 1,SSIM,63.28%,Residual net + Pseudo Sketch Feature Loss + LSGAN,-,Face Sketch Synthesis,Semi-Supervised Learning for Face Sketch Synthesis in the Wild,/paper/semi-supervised-learning-for-face-sketch,https://arxiv.org/pdf/1812.04929v1.pdf
CUHK,,# 1,FSIM,74.23%,Residual net + Pseudo Sketch Feature Loss + LSGAN,-,Face Sketch Synthesis,Semi-Supervised Learning for Face Sketch Synthesis in the Wild,/paper/semi-supervised-learning-for-face-sketch,https://arxiv.org/pdf/1812.04929v1.pdf
IJB-A,,# 6,TAR @ FAR=0.01,93.90%,Template adaptation,-,Face Verification,Template Adaptation for Face Verification and Identification,/paper/template-adaptation-for-face-verification-and,https://arxiv.org/pdf/1603.03958v3.pdf
CIFAR-10,,# 20,Percentage correct,94.2,Fitnet4-LSUV,-,Image Classification,All you need is a good init,/paper/all-you-need-is-a-good-init,https://arxiv.org/pdf/1511.06422v7.pdf
CIFAR-100,,# 21,Percentage correct,72.3,Fitnet4-LSUV,-,Image Classification,All you need is a good init,/paper/all-you-need-is-a-good-init,https://arxiv.org/pdf/1511.06422v7.pdf
MNIST,,# 4,Percentage error,0.4,Fitnet-LSUV-SVM,-,Image Classification,All you need is a good init,/paper/all-you-need-is-a-good-init,https://arxiv.org/pdf/1511.06422v7.pdf
WikiQA,,# 3,MAP,0.7069,Key-Value Memory Network,-,Question Answering,Key-Value Memory Networks for Directly Reading Documents,/paper/key-value-memory-networks-for-directly,https://arxiv.org/pdf/1606.03126v2.pdf
WikiQA,,# 2,MRR,0.7265,Key-Value Memory Network,-,Question Answering,Key-Value Memory Networks for Directly Reading Documents,/paper/key-value-memory-networks-for-directly,https://arxiv.org/pdf/1606.03126v2.pdf
CNN / Daily Mail,,# 15,CNN,63.8,Impatient Reader,-,Question Answering,Teaching Machines to Read and Comprehend,/paper/teaching-machines-to-read-and-comprehend,https://arxiv.org/pdf/1506.03340v3.pdf
CNN / Daily Mail,,# 10,Daily Mail,68.0,Impatient Reader,-,Question Answering,Teaching Machines to Read and Comprehend,/paper/teaching-machines-to-read-and-comprehend,https://arxiv.org/pdf/1506.03340v3.pdf
CNN / Daily Mail,,# 16,CNN,63.0,Attentive Reader,-,Question Answering,Teaching Machines to Read and Comprehend,/paper/teaching-machines-to-read-and-comprehend,https://arxiv.org/pdf/1506.03340v3.pdf
CNN / Daily Mail,,# 8,Daily Mail,69.0,Attentive Reader,-,Question Answering,Teaching Machines to Read and Comprehend,/paper/teaching-machines-to-read-and-comprehend,https://arxiv.org/pdf/1506.03340v3.pdf
CNN / Daily Mail,,# 13,CNN,69.4,MemNNs (ensemble),-,Question Answering,Teaching Machines to Read and Comprehend,/paper/teaching-machines-to-read-and-comprehend,https://arxiv.org/pdf/1506.03340v3.pdf
IconArt,,# 1,MAP,13.2,MI-max-C,-,Weakly Supervised Object Detection,Weakly Supervised Object Detection in Artworks,/paper/weakly-supervised-object-detection-in,https://arxiv.org/pdf/1810.02569v1.pdf
PeopleArt,,# 1,MAP,55.4,MI-max,-,Weakly Supervised Object Detection,Weakly Supervised Object Detection in Artworks,/paper/weakly-supervised-object-detection-in,https://arxiv.org/pdf/1810.02569v1.pdf
Watercolor2k,,# 1,MAP,50.1,MI-max,-,Weakly Supervised Object Detection,Weakly Supervised Object Detection in Artworks,/paper/weakly-supervised-object-detection-in,https://arxiv.org/pdf/1810.02569v1.pdf
IMDb,,# 7,Accuracy,92.33,seq2-bown-CNN,-,Sentiment Analysis,Effective Use of Word Order for Text Categorization with Convolutional Neural Networks,/paper/effective-use-of-word-order-for-text-1,https://arxiv.org/pdf/1412.1058v2.pdf
CIFAR-100,,# 26,Percentage correct,69,NiN+Superclass+CDJ,-,Image Classification,Deep Convolutional Decision Jungle for Image Classification,/paper/deep-convolutional-decision-jungle-for-image,https://arxiv.org/pdf/1706.02003v2.pdf
AG News,,# 17,Error,14.0,ToWE-SG,-,Text Classification,Task-oriented Word Embedding for Text Classification,/paper/task-oriented-word-embedding-for-text,https://aclweb.org/anthology/C18-1172
IMDb,,# 9,Accuracy,90.8,ToWE-SG,-,Sentiment Analysis,Task-oriented Word Embedding for Text Classification,/paper/task-oriented-word-embedding-for-text,https://aclweb.org/anthology/C18-1172
SST-2 Binary classification,,# 22,Accuracy,78.8,ToWE-CBOW,-,Sentiment Analysis,Task-oriented Word Embedding for Text Classification,/paper/task-oriented-word-embedding-for-text,https://aclweb.org/anthology/C18-1172
COCO,,# 7,Validation AP,60.5,Part Affinity Fields,-,Keypoint Detection,Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields,/paper/realtime-multi-person-2d-pose-estimation,https://arxiv.org/pdf/1611.08050v2.pdf
MPII Multi-Person,,# 4,AP,75.6%,Part Affinity Fields,-,Multi-Person Pose Estimation,Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields,/paper/realtime-multi-person-2d-pose-estimation,https://arxiv.org/pdf/1611.08050v2.pdf
Penn Treebank,,# 3,Accuracy,97.78,Char Bi-LSTM,-,Part-Of-Speech Tagging,Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation,/paper/finding-function-in-form-compositional-1,https://arxiv.org/pdf/1508.02096v2.pdf
Penn Treebank,,# 10,Accuracy,97.36,Bi-LSTM,-,Part-Of-Speech Tagging,Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation,/paper/finding-function-in-form-compositional-1,https://arxiv.org/pdf/1508.02096v2.pdf
Labeled Faces in the Wild,,# 8,Accuracy,99.30%,Git Loss,-,Face Verification,Git Loss for Deep Face Recognition,/paper/git-loss-for-deep-face-recognition,https://arxiv.org/pdf/1807.08512v4.pdf
YouTube Faces DB,,# 6,Accuracy,95.30%,Git Loss,-,Face Verification,Git Loss for Deep Face Recognition,/paper/git-loss-for-deep-face-recognition,https://arxiv.org/pdf/1807.08512v4.pdf
ADE20K,,# 1,Validation mIoU,44.94,PSPNet,-,Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
ADE20K,,# 3,Test Score,0.5538,PSPNet,-,Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
CamVid,,# 1,Mean IoU,69.1%,PSPNet,-,Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
CamVid,,# 1,mIoU,69.1%,PSPNet,-,Real-Time Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
CamVid,,# 2,Time (ms),185,PSPNet,-,Real-Time Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
CamVid,,# 2,Frame (fps),5.4,PSPNet,-,Real-Time Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
Cityscapes,,# 5,Mean IoU,81.2%,PSPNet,-,Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
Cityscapes,,# 1,mIoU,81.2%,PSPNet,-,Real-Time Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
Cityscapes,,# 7,Time (ms),1288,PSPNet,-,Real-Time Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
Cityscapes,,# 9,Frame (fps),0.78,PSPNet,-,Real-Time Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
PASCAL VOC 2012,,# 5,Mean IoU,85.4%,PSPNet,-,Semantic Segmentation,Pyramid Scene Parsing Network,/paper/pyramid-scene-parsing-network,https://arxiv.org/pdf/1612.01105v2.pdf
BRATS-2015,,# 1,Dice Score,85%,CNN + 3D filters,-,Brain Tumor Segmentation,CNN-based Segmentation of Medical Imaging Data,/paper/cnn-based-segmentation-of-medical-imaging,https://arxiv.org/pdf/1701.03056v2.pdf
Atari 2600 Alien,,# 16,Score,939.2,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Amidar,,# 22,Score,103.4,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Assault,,# 21,Score,628.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Asterix,,# 22,Score,987.3,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Asteroids,,# 20,Score,907.3,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Atlantis,,# 21,Score,62687.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Bank Heist,,# 21,Score,190.8,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Battle Zone,,# 18,Score,15820.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Beam Rider,,# 22,Score,929.4,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Bowling,,# 15,Score,43.9,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Boxing,,# 19,Score,44.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Breakout,,# 23,Score,5.2,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Centipede,,# 4,Score,8803.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Chopper Command,,# 20,Score,1582.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Crazy Climber,,# 21,Score,23411.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Demon Attack,,# 21,Score,520.5,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Double Dunk,,# 18,Score,-13.1,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Enduro,,# 18,Score,129.1,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Fishing Derby,,# 22,Score,-89.5,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Freeway,,# 19,Score,19.1,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Frostbite,,# 22,Score,216.9,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Gopher,,# 21,Score,1288.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Gravitar,,# 13,Score,387.7,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 HERO,,# 21,Score,6459.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Ice Hockey,,# 17,Score,-9.5,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 James Bond,,# 21,Score,202.8,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Kangaroo,,# 16,Score,1622.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Krull,,# 21,Score,3372.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Kung-Fu Master,,# 20,Score,19544.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Montezuma's Revenge,,# 22,Score,10.7,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Ms. Pacman,,# 13,Score,1692.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Name This Game,,# 21,Score,2500.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Pong,,# 15,Score,-19.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Private Eye,,# 8,Score,684.3,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Q*Bert,,# 25,Score,613.5,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 River Raid,,# 22,Score,1904.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Road Runner,,# 22,Score,67.7,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Robotank,,# 16,Score,28.7,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Seaquest,,# 23,Score,664.8,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Space Invaders,,# 23,Score,250.1,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Star Gunner,,# 20,Score,1070.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Tennis,,# 9,Score,-0.1,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Time Pilot,,# 20,Score,3741.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Tutankham,,# 14,Score,114.3,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Up and Down,,# 21,Score,3533.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Venture,,# 18,Score,66.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Video Pinball,,# 22,Score,16871.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Wizard of Wor,,# 19,Score,1981.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Atari 2600 Zaxxon,,# 20,Score,3365.0,Best linear,-,Atari Games,The Arcade Learning Environment: An Evaluation Platform for General Agents,/paper/the-arcade-learning-environment-an-evaluation,https://arxiv.org/pdf/1207.4708v2.pdf
Switchboard corpus,,# 3,Accuracy,77.34,RNN with 3 utterances in context,-,Dialogue Act Classification,A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks,/paper/a-context-based-approach-for-dialogue-act,https://arxiv.org/pdf/1805.06280v1.pdf
Vid4 - 4x upscaling,,# 1,PSNR,27.31,VSR-DUF,-,Video Super-Resolution,Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation,/paper/deep-video-super-resolution-network-using,https://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf
Vid4 - 4x upscaling,,# 11,SSIM,0.8320000000000001,VSR-DUF,-,Video Super-Resolution,Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation,/paper/deep-video-super-resolution-network-using,https://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf
SKU-110K,,# 1,AP,0.49200000000000005,Soft-IoU + EM-Merger unit,-,Dense Object Detection,Precise Detection in Densely Packed Scenes,/paper/precise-detection-in-densely-packed-scenes,https://arxiv.org/pdf/1904.00853v3.pdf
TREC Robust04,,# 4,MAP,0.2856,SNRM,-,Ad-Hoc Information Retrieval,From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing,/paper/from-neural-re-ranking-to-neural-ranking,https://ciir-publications.cs.umass.edu/getpdf.php?id=1302
TREC Robust04,,# 2,MAP,0.2971,SNRM-PRF,-,Ad-Hoc Information Retrieval,From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing,/paper/from-neural-re-ranking-to-neural-ranking,https://ciir-publications.cs.umass.edu/getpdf.php?id=1302
TREC Robust04,,# 11,MAP,0.2499,QL,-,Ad-Hoc Information Retrieval,From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing,/paper/from-neural-re-ranking-to-neural-ranking,https://ciir-publications.cs.umass.edu/getpdf.php?id=1302
GENIA - LAS,,# 1,F1,91.92,BiLSTM-CRF,-,Dependency Parsing,From POS tagging to dependency parsing for biomedical event extraction,/paper/from-pos-tagging-to-dependency-parsing-for,https://arxiv.org/pdf/1808.03731v2.pdf
GENIA - UAS,,# 1,F1,92.84,BiLSTM-CRF,-,Dependency Parsing,From POS tagging to dependency parsing for biomedical event extraction,/paper/from-pos-tagging-to-dependency-parsing-for,https://arxiv.org/pdf/1808.03731v2.pdf
CIFAR-10,,# 19,Percentage correct,94.4,Deep Complex,-,Image Classification,Deep Complex Networks,/paper/deep-complex-networks,https://arxiv.org/pdf/1705.09792v4.pdf
CIFAR-100,,# 18,Percentage correct,72.9,Deep Complex,-,Image Classification,Deep Complex Networks,/paper/deep-complex-networks,https://arxiv.org/pdf/1705.09792v4.pdf
SVHN,,# 21,Percentage error,3.3,Deep Complex,-,Image Classification,Deep Complex Networks,/paper/deep-complex-networks,https://arxiv.org/pdf/1705.09792v4.pdf
CIFAR-10,,# 38,Percentage correct,91.2,NiN,-,Image Classification,Network In Network,/paper/network-in-network,https://arxiv.org/pdf/1312.4400v3.pdf
CIFAR-10,,# 17,Percentage error,8.81,NiN,-,Image Classification,Network In Network,/paper/network-in-network,https://arxiv.org/pdf/1312.4400v3.pdf
CIFAR-100,,# 39,Percentage correct,64.3,NiN,-,Image Classification,Network In Network,/paper/network-in-network,https://arxiv.org/pdf/1312.4400v3.pdf
CIFAR-100,,# 11,Percentage error,35.68,NiN,-,Image Classification,Network In Network,/paper/network-in-network,https://arxiv.org/pdf/1312.4400v3.pdf
MNIST,,# 5,Percentage error,0.5,NiN,-,Image Classification,Network In Network,/paper/network-in-network,https://arxiv.org/pdf/1312.4400v3.pdf
SVHN,,# 17,Percentage error,2.35,Network in Network,-,Image Classification,Network In Network,/paper/network-in-network,https://arxiv.org/pdf/1312.4400v3.pdf
IJB-A,,# 9,TAR @ FAR=0.01,90%,Triplet probabilistic embedding,-,Face Verification,Triplet Probabilistic Embedding for Face Verification and Clustering,/paper/triplet-probabilistic-embedding-for-face,https://arxiv.org/pdf/1604.05417v3.pdf
Restricted,,# 3,F0.5,56.25,SMT + BiGRU,-,Grammatical Error Correction,Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation,/paper/near-human-level-performance-in-grammatical-1,https://aclweb.org/anthology/N18-2046
Restricted,,# 1,F0.5,72.04,SMT + BiGRU,-,Grammatical Error Correction,Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation,/paper/near-human-level-performance-in-grammatical-1,https://aclweb.org/anthology/N18-2046
_Restricted_,,# 1,GLEU,61.5,SMT + BiGRU,-,Grammatical Error Correction,Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation,/paper/near-human-level-performance-in-grammatical-1,https://aclweb.org/anthology/N18-2046
STL-10,,# 11,Percentage correct,68.0,DFF Committees,-,Image Classification,Committees of deep feedforward networks trained with few data,/paper/committees-of-deep-feedforward-networks,https://arxiv.org/pdf/1406.5947v1.pdf
SCUT-FBP,,# 2,MAE,0.3931,Combined Features + Gaussian Reg,-,Facial Beauty Prediction,SCUT-FBP5500: A Diverse Benchmark Dataset for Multi-Paradigm Facial Beauty Prediction,/paper/scut-fbp5500-a-diverse-benchmark-dataset-for,https://arxiv.org/pdf/1801.06345v1.pdf
COCO,,# 24,Bounding Box AP,40.4,ResNet-50-FPN Mask R-CNN + KL Loss + var voting + soft-NMS,-,Object Detection,Bounding Box Regression with Uncertainty for Accurate Object Detection,/paper/softer-nms-rethinking-bounding-box-regression,https://arxiv.org/pdf/1809.08545v3.pdf
PASCAL VOC 2007,,# 15,MAP,71.6%,VGG-16 + KL Loss + var voting + soft-NMS,-,Object Detection,Bounding Box Regression with Uncertainty for Accurate Object Detection,/paper/softer-nms-rethinking-bounding-box-regression,https://arxiv.org/pdf/1809.08545v3.pdf
NarrativeQA,,# 3,BLEU-1,42.76,ConZNet,-,Question Answering,Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,/paper/cut-to-the-chase-a-context-zoom-in-network,https://aclweb.org/anthology/D18-1054
NarrativeQA,,# 2,BLEU-4,22.49,ConZNet,-,Question Answering,Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,/paper/cut-to-the-chase-a-context-zoom-in-network,https://aclweb.org/anthology/D18-1054
NarrativeQA,,# 2,METEOR,19.24,ConZNet,-,Question Answering,Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,/paper/cut-to-the-chase-a-context-zoom-in-network,https://aclweb.org/anthology/D18-1054
NarrativeQA,,# 1,Rouge-L,46.67,ConZNet,-,Question Answering,Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,/paper/cut-to-the-chase-a-context-zoom-in-network,https://aclweb.org/anthology/D18-1054
NYU Depth v2,,# 1,MAP,41.30,SGPN-CNN,-,3D Object Detection,SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation,/paper/sgpn-similarity-group-proposal-network-for-3d,https://arxiv.org/pdf/1711.08588v1.pdf
NYU Depth v2,,# 1,MAP,43.5,SGPN-CNN,-,Instance Segmentation,SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation,/paper/sgpn-similarity-group-proposal-network-for-3d,https://arxiv.org/pdf/1711.08588v1.pdf
ShapeNet,,# 1,Mean IoU,85.8%,SGPN,-,Semantic Segmentation,SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation,/paper/sgpn-similarity-group-proposal-network-for-3d,https://arxiv.org/pdf/1711.08588v1.pdf
CHASE_DB1,,# 2,F1 score,0.7928,R2U-Net,-,Retinal Vessel Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
CHASE_DB1,,# 2,AUC,0.9815,R2U-Net,-,Retinal Vessel Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
DRIVE,,# 2,F1 score,0.8171,R2U-Net,-,Retinal Vessel Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
DRIVE,,# 2,AUC,0.9784,R2U-Net,-,Retinal Vessel Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
Kaggle Skin Lesion Segmentation,,# 1,F1 score,0.892,R2U-Net,-,Skin Cancer Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
Kaggle Skin Lesion Segmentation,,# 1,AUC,0.9419,R2U-Net,-,Skin Cancer Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
LUNA,,# 1,F1 score,0.9823,R2U-Net,-,Lung Nodule Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
LUNA,,# 1,AUC,0.9889,R2U-Net,-,Lung Nodule Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
STARE,,# 1,F1 score,0.8475,R2U-Net,-,Retinal Vessel Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
STARE,,# 1,AUC,0.9914,R2U-Net,-,Retinal Vessel Segmentation,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,/paper/recurrent-residual-convolutional-neural,https://arxiv.org/pdf/1802.06955v5.pdf
WSJ eval92,,# 3,Percentage error,3.47,TC-DNN-BLSTM-DNN,-,Speech Recognition,Deep Recurrent Neural Networks for Acoustic Modelling,/paper/deep-recurrent-neural-networks-for-acoustic,https://arxiv.org/pdf/1504.01482v1.pdf
IWSLT2015 German-English,,# 14,BLEU score,24.0,"Word-level CNN w/attn, input feeding",-,Machine Translation,Sequence-to-Sequence Learning as Beam-Search Optimization,/paper/sequence-to-sequence-learning-as-beam-search,https://arxiv.org/pdf/1606.02960v2.pdf
LDC2016E25,,# 1,BLEU,22,Graph2Seq,-,Text Generation,A Graph-to-Sequence Model for AMR-to-Text Generation,/paper/a-graph-to-sequence-model-for-amr-to-text,https://arxiv.org/pdf/1805.02473v3.pdf
SQuAD1.1,,# 105,EM,70.84899999999999,RaSoR (single model),-,Question Answering,Learning Recurrent Span Representations for Extractive Question Answering,/paper/learning-recurrent-span-representations-for,https://arxiv.org/pdf/1611.01436v2.pdf
SQuAD1.1,,# 112,F1,78.741,RaSoR (single model),-,Question Answering,Learning Recurrent Span Representations for Extractive Question Answering,/paper/learning-recurrent-span-representations-for,https://arxiv.org/pdf/1611.01436v2.pdf
MovieLens 10M,,# 5,RMSE,0.782,I-AutoRec,-,Collaborative Filtering,AutoRec: Autoencoders Meet Collaborative Filtering,/paper/autorec-autoencoders-meet-collaborative,https://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf
MovieLens 1M,,# 3,RMSE,0.831,I-AutoRec,-,Collaborative Filtering,AutoRec: Autoencoders Meet Collaborative Filtering,/paper/autorec-autoencoders-meet-collaborative,https://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf
PA-100K,,# 1,Accuracy,72.19%,HP-net,-,Pedestrian Attribute Recognition,HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis,/paper/hydraplus-net-attentive-deep-features-for,https://arxiv.org/pdf/1709.09930v1.pdf
PETA,,# 1,Accuracy,76.13%,HP-net,-,Pedestrian Attribute Recognition,HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis,/paper/hydraplus-net-attentive-deep-features-for,https://arxiv.org/pdf/1709.09930v1.pdf
RAP,,# 1,Accuracy,65.39%,HP-net,-,Pedestrian Attribute Recognition,HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis,/paper/hydraplus-net-attentive-deep-features-for,https://arxiv.org/pdf/1709.09930v1.pdf
TIMIT,,# 13,Percentage error,19.64,QCNN-10L-256FM,-,Speech Recognition,Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition,/paper/quaternion-convolutional-neural-networks-for-1,https://arxiv.org/pdf/1806.07789v1.pdf
DUC 2004 Task 1,,# 3,ROUGE-1,31.79,DRGD,-,Text Summarization,Deep Recurrent Generative Decoder for Abstractive Text Summarization,/paper/deep-recurrent-generative-decoder-for-1,https://aclweb.org/anthology/D17-1222
DUC 2004 Task 1,,# 2,ROUGE-2,10.75,DRGD,-,Text Summarization,Deep Recurrent Generative Decoder for Abstractive Text Summarization,/paper/deep-recurrent-generative-decoder-for-1,https://aclweb.org/anthology/D17-1222
DUC 2004 Task 1,,# 3,ROUGE-L,27.48,DRGD,-,Text Summarization,Deep Recurrent Generative Decoder for Abstractive Text Summarization,/paper/deep-recurrent-generative-decoder-for-1,https://aclweb.org/anthology/D17-1222
GigaWord,,# 5,ROUGE-1,36.27,DRGD,-,Text Summarization,Deep Recurrent Generative Decoder for Abstractive Text Summarization,/paper/deep-recurrent-generative-decoder-for-1,https://aclweb.org/anthology/D17-1222
GigaWord,,# 7,ROUGE-2,17.57,DRGD,-,Text Summarization,Deep Recurrent Generative Decoder for Abstractive Text Summarization,/paper/deep-recurrent-generative-decoder-for-1,https://aclweb.org/anthology/D17-1222
GigaWord,,# 8,ROUGE-L,33.62,DRGD,-,Text Summarization,Deep Recurrent Generative Decoder for Abstractive Text Summarization,/paper/deep-recurrent-generative-decoder-for-1,https://aclweb.org/anthology/D17-1222
LDC2015E86:,,# 1,BLEU,23.95,GCNSEQ,-,Graph-to-Sequence,Structural Neural Encoders for AMR-to-text Generation,/paper/structural-neural-encoders-for-amr-to-text,https://arxiv.org/pdf/1903.11410v1.pdf
ModelNet40,,# 1,Classification Accuracy,89.3%,3D-PointCapsNet,-,3D Object Classification,3D Point-Capsule Networks,/paper/3d-point-capsule-networks,https://arxiv.org/pdf/1812.10775v1.pdf
ShapeNet-Part,,# 1,Accuracy,86%,3D-PointCapsNet,-,3D Part Segmentation,3D Point-Capsule Networks,/paper/3d-point-capsule-networks,https://arxiv.org/pdf/1812.10775v1.pdf
ImageNet 128x128,,# 5,FID,27.62,Projection Discriminator,-,Conditional Image Generation,cGANs with Projection Discriminator,/paper/cgans-with-projection-discriminator,https://arxiv.org/pdf/1802.05637v2.pdf
ImageNet 128x128,,# 5,Inception score,36.8,Projection Discriminator,-,Conditional Image Generation,cGANs with Projection Discriminator,/paper/cgans-with-projection-discriminator,https://arxiv.org/pdf/1802.05637v2.pdf
GTAV-to-Cityscapes Labels,,# 4,mIoU,39.4,ROAD,-,Synthetic-to-Real Translation,ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes,/paper/road-reality-oriented-adaptation-for-semantic,https://arxiv.org/pdf/1711.11556v2.pdf
NewsQA,,# 2,F1,63.7,AMANDA,-,Question Answering,A Question-Focused Multi-Factor Attention Network for Question Answering,/paper/a-question-focused-multi-factor-attention,https://arxiv.org/pdf/1801.08290v1.pdf
NewsQA,,# 3,EM,48.4,AMANDA,-,Question Answering,A Question-Focused Multi-Factor Attention Network for Question Answering,/paper/a-question-focused-multi-factor-attention,https://arxiv.org/pdf/1801.08290v1.pdf
SearchQA,,# 3,Unigram Acc,46.8,AMANDA,-,Open-Domain Question Answering,A Question-Focused Multi-Factor Attention Network for Question Answering,/paper/a-question-focused-multi-factor-attention,https://arxiv.org/pdf/1801.08290v1.pdf
SearchQA,,# 3,N-gram F1,56.6,AMANDA,-,Open-Domain Question Answering,A Question-Focused Multi-Factor Attention Network for Question Answering,/paper/a-question-focused-multi-factor-attention,https://arxiv.org/pdf/1801.08290v1.pdf
SearchQA,,# 4,EM,-,AMANDA,-,Open-Domain Question Answering,A Question-Focused Multi-Factor Attention Network for Question Answering,/paper/a-question-focused-multi-factor-attention,https://arxiv.org/pdf/1801.08290v1.pdf
SearchQA,,# 4,F1,-,AMANDA,-,Open-Domain Question Answering,A Question-Focused Multi-Factor Attention Network for Question Answering,/paper/a-question-focused-multi-factor-attention,https://arxiv.org/pdf/1801.08290v1.pdf
WikiQA,,# 4,MAP,0.7058,LDC,-,Question Answering,Sentence Similarity Learning by Lexical Decomposition and Composition,/paper/sentence-similarity-learning-by-lexical,https://arxiv.org/pdf/1602.07019v2.pdf
WikiQA,,# 4,MRR,0.7226,LDC,-,Question Answering,Sentence Similarity Learning by Lexical Decomposition and Composition,/paper/sentence-similarity-learning-by-lexical,https://arxiv.org/pdf/1602.07019v2.pdf
Penn Treebank,,# 11,F1 score,ï»¿93.3,RNN Grammar,-,Constituency Parsing,Recurrent Neural Network Grammars,/paper/recurrent-neural-network-grammars,https://arxiv.org/pdf/1602.07776v4.pdf
MHP v1.0,,# 4,AP 0.5,47.76%,DL,-,Multi-Human Parsing,Semantic Instance Segmentation with a Discriminative Loss Function,/paper/semantic-instance-segmentation-with-a,https://arxiv.org/pdf/1708.02551v1.pdf
TuSimple,,# 3,Accuracy,96.40%,Discriminative loss function,-,Lane Detection,Semantic Instance Segmentation with a Discriminative Loss Function,/paper/semantic-instance-segmentation-with-a,https://arxiv.org/pdf/1708.02551v1.pdf
WMT2016 English-Romanian,,# 4,BLEU score,28.9,GRU BPE90k,-,Machine Translation,The QT21/HimL Combined Machine Translation System,/paper/the-qt21himl-combined-machine-translation,https://aclweb.org/anthology/W16-2320
HIV dataset,,# 1,AUC,0.851,GraphConv + dummy super node + focal loss,-,Drug Discovery,Learning Graph-Level Representation for Drug Discovery,/paper/learning-graph-level-representation-for-drug,https://arxiv.org/pdf/1709.03741v2.pdf
MUV,,# 1,AUC,0.845,GraphConv + dummy super node,-,Drug Discovery,Learning Graph-Level Representation for Drug Discovery,/paper/learning-graph-level-representation-for-drug,https://arxiv.org/pdf/1709.03741v2.pdf
PCBA,,# 1,AUC,0.867,GraphConv + dummy super node,-,Drug Discovery,Learning Graph-Level Representation for Drug Discovery,/paper/learning-graph-level-representation-for-drug,https://arxiv.org/pdf/1709.03741v2.pdf
Tox21,,# 1,AUC,0.8540000000000001,GraphConv + dummy super node,-,Drug Discovery,Learning Graph-Level Representation for Drug Discovery,/paper/learning-graph-level-representation-for-drug,https://arxiv.org/pdf/1709.03741v2.pdf
ToxCast,,# 1,AUC,0.768,GraphConv + dummy super node,-,Drug Discovery,Learning Graph-Level Representation for Drug Discovery,/paper/learning-graph-level-representation-for-drug,https://arxiv.org/pdf/1709.03741v2.pdf
MPII Human Pose,,# 7,PCKh-0.5,89.4%,CU-Net,-,Pose Estimation,CU-Net: Coupled U-Nets,/paper/cu-net-coupled-u-nets,https://arxiv.org/pdf/1808.06521v1.pdf
CIFAR-10,,# 26,Percentage correct,93.3,Universum Prescription,-,Image Classification,Universum Prescription: Regularization using Unlabeled Data,/paper/universum-prescription-regularization-using,https://arxiv.org/pdf/1511.03719v7.pdf
CIFAR-100,,# 34,Percentage correct,67.2,Universum Prescription,-,Image Classification,Universum Prescription: Regularization using Unlabeled Data,/paper/universum-prescription-regularization-using,https://arxiv.org/pdf/1511.03719v7.pdf
Atari 2600 Alien,,# 9,Score,2436.6,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Amidar,,# 7,Score,1272.5,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Assault,,# 6,Score,8047.1,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Asterix,,# 9,Score,19713.2,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Asteroids,,# 17,Score,1032.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Atlantis,,# 2,Score,994500.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Bank Heist,,# 4,Score,1208.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Battle Zone,,# 2,Score,38666.7,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Beam Rider,,# 6,Score,23429.8,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Bowling,,# 8,Score,60.2,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Boxing,,# 7,Score,93.2,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Breakout,,# 1,Score,855.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Centipede,,# 15,Score,4553.5,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Chopper Command,,# 15,Score,4100.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Crazy Climber,,# 7,Score,137925.9,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Demon Attack,,# 6,Score,82610.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Double Dunk,,# 4,Score,3.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Enduro,,# 10,Score,1591.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Fishing Derby,,# 6,Score,26.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Freeway,,# 2,Score,33.9,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Frostbite,,# 12,Score,2181.4,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Gopher,,# 9,Score,17438.4,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Gravitar,,# 19,Score,286.1,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 HERO,,# 8,Score,21021.3,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Ice Hockey,,# 6,Score,-1.3,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 James Bond,,# 5,Score,1663.5,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Kangaroo,,# 3,Score,14862.5,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Krull,,# 8,Score,8627.9,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Kung-Fu Master,,# 7,Score,36733.3,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Montezuma's Revenge,,# 11,Score,100.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Ms. Pacman,,# 8,Score,2983.3,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Name This Game,,# 9,Score,11501.1,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Pong,,# 2,Score,20.9,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Private Eye,,# 5,Score,1812.5,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Q*Bert,,# 9,Score,15092.7,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 River Raid,,# 9,Score,12845.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Road Runner,,# 10,Score,51500.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Robotank,,# 1,Score,66.6,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Seaquest,,# 11,Score,9083.1,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Space Invaders,,# 10,Score,2893.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Star Gunner,,# 14,Score,55725.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Tennis,,# 8,Score,0.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Time Pilot,,# 6,Score,9079.4,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Tutankham,,# 5,Score,214.8,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Up and Down,,# 8,Score,26231.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Venture,,# 11,Score,212.5,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Video Pinball,,# 2,Score,811610.0,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Wizard of Wor,,# 11,Score,6804.7,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
Atari 2600 Zaxxon,,# 7,Score,11491.7,Bootstrapped DQN,-,Atari Games,Deep Exploration via Bootstrapped DQN,/paper/deep-exploration-via-bootstrapped-dqn,https://arxiv.org/pdf/1602.04621v3.pdf
AFLW2000,,# 1,MAE,6.155,Multi-Loss ResNet50,-,Head Pose Estimation,Fine-Grained Head Pose Estimation Without Keypoints,/paper/fine-grained-head-pose-estimation-without,https://arxiv.org/pdf/1710.00925v5.pdf
BIWI,,# 1,MAE,4.895,Multi-Loss ResNet50,-,Head Pose Estimation,Fine-Grained Head Pose Estimation Without Keypoints,/paper/fine-grained-head-pose-estimation-without,https://arxiv.org/pdf/1710.00925v5.pdf
Penn Treebank,,# 1,Accuracy,97.96,Meta BiLSTM,-,Part-Of-Speech Tagging,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,/paper/morphosyntactic-tagging-with-a-meta-bilstm,https://arxiv.org/pdf/1805.08237v1.pdf
ITOP front-view,,# 2,Mean mAP,77.4,Multi-task learning + viewpoint invariance,-,Pose Estimation,Towards Viewpoint Invariant 3D Human Pose Estimation,/paper/towards-viewpoint-invariant-3d-human-pose,https://arxiv.org/pdf/1603.07076v3.pdf
ITOP top-view,,# 2,Mean mAP,75.5,Multi-task learning + viewpoint invariance,-,Pose Estimation,Towards Viewpoint Invariant 3D Human Pose Estimation,/paper/towards-viewpoint-invariant-3d-human-pose,https://arxiv.org/pdf/1603.07076v3.pdf
Mini-ImageNet - 1-Shot Learning,,# 8,Accuracy,48.70%,MAML,-,Few-Shot Image Classification,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,/paper/model-agnostic-meta-learning-for-fast,https://arxiv.org/pdf/1703.03400v3.pdf
Mini-ImageNet - 5-Shot Learning,,# 8,Accuracy,63.10%,MAML,-,Few-Shot Image Classification,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,/paper/model-agnostic-meta-learning-for-fast,https://arxiv.org/pdf/1703.03400v3.pdf
OMNIGLOT - 1-Shot Learning,,# 2,Accuracy,98.7%,MAML,-,Few-Shot Image Classification,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,/paper/model-agnostic-meta-learning-for-fast,https://arxiv.org/pdf/1703.03400v3.pdf
OMNIGLOT - 5-Shot Learning,,# 1,Accuracy,99.9%,MAML,-,Few-Shot Image Classification,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,/paper/model-agnostic-meta-learning-for-fast,https://arxiv.org/pdf/1703.03400v3.pdf
Hutter Prize,,# 6,Bit per Character (BPC),1.232,3-layer AWD-LSTM,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
Hutter Prize,,# 1,Number of params,47M,3-layer AWD-LSTM,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
Penn Treebank (Character Level),,# 3,Bit per Character (BPC),1.175,3-layer AWD-LSTM,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
Penn Treebank (Character Level),,# 1,Number of params,13.8M,3-layer AWD-LSTM,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
Penn Treebank (Character Level),,# 4,Bit per Character (BPC),1.187,6-layer QRNN,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
Penn Treebank (Character Level),,# 1,Number of params,13.8M,6-layer QRNN,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
WikiText-103,,# 5,Validation perplexity,32.0,4-layer QRNN,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
WikiText-103,,# 7,Test perplexity,33.0,4-layer QRNN,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
WikiText-103,,# 1,Number of params,151M,4-layer QRNN,-,Language Modelling,An Analysis of Neural Language Modeling at Multiple Scales,/paper/an-analysis-of-neural-language-modeling-at,https://arxiv.org/pdf/1803.08240v1.pdf
Citeseer,,# 2,Accuracy,91.40%,Variational graph auto-encoders,-,Link Prediction,Variational Graph Auto-Encoders,/paper/variational-graph-auto-encoders,https://arxiv.org/pdf/1611.07308v1.pdf
Cora,,# 2,Accuracy,92.00%,Variational graph auto-encoders,-,Link Prediction,Variational Graph Auto-Encoders,/paper/variational-graph-auto-encoders,https://arxiv.org/pdf/1611.07308v1.pdf
Pubmed,,# 1,Accuracy,96.50%,Variational graph auto-encoders,-,Link Prediction,Variational Graph Auto-Encoders,/paper/variational-graph-auto-encoders,https://arxiv.org/pdf/1611.07308v1.pdf
Citeseer,,# 6,Accuracy,69.8%,ChebNet,-,Node Classification,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,/paper/convolutional-neural-networks-on-graphs-with,https://arxiv.org/pdf/1606.09375v3.pdf
Cora,,# 5,Accuracy,81.2%,ChebNet,-,Node Classification,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,/paper/convolutional-neural-networks-on-graphs-with,https://arxiv.org/pdf/1606.09375v3.pdf
Pubmed,,# 7,Accuracy,74.4%,ChebNet,-,Node Classification,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,/paper/convolutional-neural-networks-on-graphs-with,https://arxiv.org/pdf/1606.09375v3.pdf
COCO,,# 1,AP 0.5,16.3,Siamese Mask R-CNN,-,One-Shot Object Detection,One-Shot Instance Segmentation,/paper/one-shot-instance-segmentation,https://arxiv.org/pdf/1811.11507v1.pdf
COCO,,# 1,AP 0.5,14.5,Siamese Mask R-CNN,-,One-Shot Instance Segmentation,One-Shot Instance Segmentation,/paper/one-shot-instance-segmentation,https://arxiv.org/pdf/1811.11507v1.pdf
PASCAL VOC 2012,,# 8,Mean IoU,83.6%,Large Kernel Matters,-,Semantic Segmentation,Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network,/paper/large-kernel-matters-improve-semantic,https://arxiv.org/pdf/1703.02719v1.pdf
COCO,,# 5,Bounding Box AP,46.0,Deform-v2,-,Object Detection,"Deformable ConvNets v2: More Deformable, Better Results",/paper/deformable-convnets-v2-more-deformable-better,https://arxiv.org/pdf/1811.11168v2.pdf
GTAV-to-Cityscapes Labels,,# 1,mIoU,43.2,Domain adaptation + ResNet-101,-,Synthetic-to-Real Translation,Diverse Image-to-Image Translation via Disentangled Representations,/paper/diverse-image-to-image-translation-via,https://arxiv.org/pdf/1808.00948v1.pdf
IWSLT2015 German-English,,# 15,BLEU score,20.2,Word-level LSTM w/attn,-,Machine Translation,Sequence Level Training with Recurrent Neural Networks,/paper/sequence-level-training-with-recurrent-neural,https://arxiv.org/pdf/1511.06732v7.pdf
KITTI Cars Easy,,# 7,AP,77.47%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cars Easy,,# 1,AP,89.35%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cars Hard,,# 1,AP,77.39%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cars Hard,,# 7,AP,57.73%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cars Moderate,,# 8,AP,65.11%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cars Moderate,,# 2,AP,79.26%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cyclists Easy,,# 4,AP,61.22%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cyclists Easy,,# 2,AP,66.70%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cyclists Hard,,# 4,AP,44.37%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cyclists Hard,,# 2,AP,50.55%,VoxelNe,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cyclists Moderate,,# 5,AP,48.36%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Cyclists Moderate,,# 2,AP,54.76%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Pedestrians Easy,,# 2,AP,46.13%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Pedestrians Easy,,# 4,AP,39.48%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Pedestrians Hard,,# 2,AP,38.11%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Pedestrians Hard,,# 4,AP,31.51%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Pedestrians Moderate,,# 2,AP,40.74%,VoxelNet,-,Object Localization,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
KITTI Pedestrians Moderate,,# 5,AP,33.69%,VoxelNet,-,3D Object Detection,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,/paper/voxelnet-end-to-end-learning-for-point-cloud,https://arxiv.org/pdf/1711.06396v1.pdf
Penn Treebank (Word Level),,# 3,Validation perplexity,48.33,AWD-LSTM-MoS + dynamic eval,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
Penn Treebank (Word Level),,# 5,Test perplexity,47.69,AWD-LSTM-MoS + dynamic eval,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
Penn Treebank (Word Level),,# 1,Params,22M,AWD-LSTM-MoS + dynamic eval,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
Penn Treebank (Word Level),,# 10,Validation perplexity,56.54,AWD-LSTM-MoS,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
Penn Treebank (Word Level),,# 12,Test perplexity,54.44,AWD-LSTM-MoS,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
Penn Treebank (Word Level),,# 1,Params,22M,AWD-LSTM-MoS,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
WikiText-2,,# 3,Validation perplexity,42.41,AWD-LSTM-MoS + dynamic eval,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
WikiText-2,,# 4,Test perplexity,40.68,AWD-LSTM-MoS + dynamic eval,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
WikiText-2,,# 1,Number of params,35M,AWD-LSTM-MoS + dynamic eval,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
WikiText-2,,# 10,Validation perplexity,63.88,AWD-LSTM-MoS,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
WikiText-2,,# 11,Test perplexity,61.45,AWD-LSTM-MoS,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
WikiText-2,,# 1,Number of params,35M,AWD-LSTM-MoS,-,Language Modelling,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,/paper/breaking-the-softmax-bottleneck-a-high-rank,https://arxiv.org/pdf/1711.03953v4.pdf
Set14 - 4x upscaling,,# 27,PSNR,27.68,TNRD,-,Image Super-Resolution,Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,/paper/trainable-nonlinear-reaction-diffusion-a,https://arxiv.org/pdf/1508.02848v2.pdf
Set5 - 4x upscaling,,# 24,PSNR,30.85,TNRD,-,Image Super-Resolution,Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,/paper/trainable-nonlinear-reaction-diffusion-a,https://arxiv.org/pdf/1508.02848v2.pdf
Cityscapes,,# 9,Mean IoU,75.5%,SwiftNetRN-18,-,Semantic Segmentation,In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images,/paper/in-defense-of-pre-trained-imagenet,https://arxiv.org/pdf/1903.08469v2.pdf
Cityscapes,,# 2,mIoU,75.5%,SwiftNetRN-18,-,Real-Time Semantic Segmentation,In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images,/paper/in-defense-of-pre-trained-imagenet,https://arxiv.org/pdf/1903.08469v2.pdf
Cityscapes,,# 3,Frame (fps),39.9,SwiftNetRN-18,-,Real-Time Semantic Segmentation,In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images,/paper/in-defense-of-pre-trained-imagenet,https://arxiv.org/pdf/1903.08469v2.pdf
MR,,# 1,Accuracy,84.5,MEAN,-,Sentiment Analysis,A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification,/paper/a-multi-sentiment-resource-enhanced-attention,https://arxiv.org/pdf/1807.04990v1.pdf
SST-5 Fine-grained classification,,# 7,Accuracy,51.4,MEAN,-,Sentiment Analysis,A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification,/paper/a-multi-sentiment-resource-enhanced-attention,https://arxiv.org/pdf/1807.04990v1.pdf
ICVL Hands,,# 3,Average 3D Error,7.3,Dense Pixel-wise Estimation,-,Hand Pose Estimation,Dense 3D Regression for Hand Pose Estimation,/paper/dense-3d-regression-for-hand-pose-estimation,https://arxiv.org/pdf/1711.08996v1.pdf
MSRA Hands,,# 1,Average 3D Error,7.2,Dense Pixel-wise Estimation,-,Hand Pose Estimation,Dense 3D Regression for Hand Pose Estimation,/paper/dense-3d-regression-for-hand-pose-estimation,https://arxiv.org/pdf/1711.08996v1.pdf
NYU Hands,,# 2,Average 3D Error,10.2,Dense Pixel-wise Estimation,-,Hand Pose Estimation,Dense 3D Regression for Hand Pose Estimation,/paper/dense-3d-regression-for-hand-pose-estimation,https://arxiv.org/pdf/1711.08996v1.pdf
LineMOD,,# 4,Accuracy,83.9%,BB8,-,6D Pose Estimation,"BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth",/paper/bb8-a-scalable-accurate-robust-to-partial,https://arxiv.org/pdf/1703.10896v2.pdf
MHP v1.0,,# 3,AP 0.5,50.10%,MH-Parser,-,Multi-Human Parsing,Multiple-Human Parsing in the Wild,/paper/multiple-human-parsing-in-the-wild,https://arxiv.org/pdf/1705.07206v2.pdf
MHP v2.0,,# 2,AP 0.5,17.99%,MH-Parser,-,Multi-Human Parsing,Multiple-Human Parsing in the Wild,/paper/multiple-human-parsing-in-the-wild,https://arxiv.org/pdf/1705.07206v2.pdf
SCUT-CTW1500,,# 3,F-Measure,73.4%,CTD+TLOC,-,Curved Text Detection,Detecting Curve Text in the Wild: New Dataset and New Solution,/paper/detecting-curve-text-in-the-wild-new-dataset,https://arxiv.org/pdf/1712.02170v1.pdf
WikiText-103,,# 4,Validation perplexity,29.0,LSTM + Hebbian + Cache + MbPA,-,Language Modelling,Fast Parametric Learning with Activation Memorization,/paper/fast-parametric-learning-with-activation,https://arxiv.org/pdf/1803.10049v1.pdf
WikiText-103,,# 6,Test perplexity,29.2,LSTM + Hebbian + Cache + MbPA,-,Language Modelling,Fast Parametric Learning with Activation Memorization,/paper/fast-parametric-learning-with-activation,https://arxiv.org/pdf/1803.10049v1.pdf
WikiText-103,,# 6,Validation perplexity,34.1,LSTM + Hebbian,-,Language Modelling,Fast Parametric Learning with Activation Memorization,/paper/fast-parametric-learning-with-activation,https://arxiv.org/pdf/1803.10049v1.pdf
WikiText-103,,# 8,Test perplexity,34.3,LSTM + Hebbian,-,Language Modelling,Fast Parametric Learning with Activation Memorization,/paper/fast-parametric-learning-with-activation,https://arxiv.org/pdf/1803.10049v1.pdf
WikiText-103,,# 7,Validation perplexity,36.0,LSTM,-,Language Modelling,Fast Parametric Learning with Activation Memorization,/paper/fast-parametric-learning-with-activation,https://arxiv.org/pdf/1803.10049v1.pdf
WikiText-103,,# 9,Test perplexity,36.4,LSTM,-,Language Modelling,Fast Parametric Learning with Activation Memorization,/paper/fast-parametric-learning-with-activation,https://arxiv.org/pdf/1803.10049v1.pdf
Sentihood,,# 3,Aspect,78.5,Liu et al.,-,Aspect-Based Sentiment Analysis,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis,/paper/recurrent-entity-networks-with-delayed-memory-1,https://aclweb.org/anthology/N18-2045
Sentihood,,# 3,Sentiment,91.0,Liu et al.,-,Aspect-Based Sentiment Analysis,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis,/paper/recurrent-entity-networks-with-delayed-memory-1,https://aclweb.org/anthology/N18-2045
CIFAR-10,,# 7,FID,28.73,MSGAN,-,Image Generation,Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis,/paper/mode-seeking-generative-adversarial-networks,https://arxiv.org/pdf/1903.05628v5.pdf
ModelNet40,,# 2,Accuracy,88.4%,FPNN (4-FCs + NF),-,3D Object Recognition,FPNN: Field Probing Neural Networks for 3D Data,/paper/fpnn-field-probing-neural-networks-for-3d,https://arxiv.org/pdf/1605.06240v3.pdf
BSD100 - 4x upscaling,,# 12,PSNR,27.49,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
BSD100 - 4x upscaling,,# 16,SSIM,0.7340000000000001,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
Set14 - 4x upscaling,,# 15,PSNR,28.35,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
Set14 - 4x upscaling,,# 18,SSIM,0.777,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
Set5 - 4x upscaling,,# 11,PSNR,31.96,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
Set5 - 4x upscaling,,# 14,SSIM,0.893,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
Urban100 - 4x upscaling,,# 13,PSNR,25.68,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
Urban100 - 4x upscaling,,# 12,SSIM,0.773,SRMDNF,-,Image Super-Resolution,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,/paper/learning-a-single-convolutional-super,https://arxiv.org/pdf/1712.06116v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 16,Restaurant (Acc),80.23,RAM,-,Aspect-Based Sentiment Analysis,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,/paper/recurrent-attention-network-on-memory-for,https://aclweb.org/anthology/D17-1047
SemEval 2014 Task 4 Sub Task 2,,# 14,Laptop (Acc),74.49,RAM,-,Aspect-Based Sentiment Analysis,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,/paper/recurrent-attention-network-on-memory-for,https://aclweb.org/anthology/D17-1047
Set14 - 4x upscaling,,# 2,PSNR,28.98,PFF,-,Image Super-Resolution,Image Reconstruction with Predictive Filter Flow,/paper/image-reconstruction-with-predictive-filter,https://arxiv.org/pdf/1811.11482v1.pdf
Set14 - 4x upscaling,,# 5,SSIM,0.7904,PFF,-,Image Super-Resolution,Image Reconstruction with Predictive Filter Flow,/paper/image-reconstruction-with-predictive-filter,https://arxiv.org/pdf/1811.11482v1.pdf
Set5 - 4x upscaling,,# 1,PSNR,32.74,PFF,-,Image Super-Resolution,Image Reconstruction with Predictive Filter Flow,/paper/image-reconstruction-with-predictive-filter,https://arxiv.org/pdf/1811.11482v1.pdf
Set5 - 4x upscaling,,# 1,SSIM,0.9021,PFF,-,Image Super-Resolution,Image Reconstruction with Predictive Filter Flow,/paper/image-reconstruction-with-predictive-filter,https://arxiv.org/pdf/1811.11482v1.pdf
FB15k,,# 3,MRR,0.79,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k,,# 3,[emailÂ protected],0.885,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k,,# 2,[emailÂ protected],0.8290000000000001,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k,,# 2,[emailÂ protected],0.7340000000000001,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k-237,,# 3,MRR,0.341,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k-237,,# 3,[emailÂ protected],0.52,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k-237,,# 2,[emailÂ protected],0.376,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
FB15k-237,,# 2,[emailÂ protected],0.252,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18,,# 3,MRR,0.951,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18,,# 3,[emailÂ protected],0.958,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18,,# 2,[emailÂ protected],0.955,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18,,# 3,[emailÂ protected],0.9470000000000001,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18RR,,# 4,MRR,0.465,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18RR,,# 4,[emailÂ protected],0.522,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18RR,,# 2,[emailÂ protected],0.47700000000000004,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
WN18RR,,# 3,[emailÂ protected],0.436,HypER,-,Link Prediction,Hypernetwork Knowledge Graph Embeddings,/paper/hypernetwork-knowledge-graph-embeddings,https://arxiv.org/pdf/1808.07018v3.pdf
COCO,,# 17,Bounding Box AP,42.3,ResNet-101 + Group Normalization,-,Object Detection,Group Normalization,/paper/group-normalization,https://arxiv.org/pdf/1803.08494v3.pdf
BSD68 sigma15,,# 4,PSNR,31.63,Deep CNN Denoiser,-,Image Denoising,Learning Deep CNN Denoiser Prior for Image Restoration,/paper/learning-deep-cnn-denoiser-prior-for-image,https://arxiv.org/pdf/1704.03264v1.pdf
BSD68 sigma25,,# 5,PSNR,29.15,Deep CNN Denoiser,-,Image Denoising,Learning Deep CNN Denoiser Prior for Image Restoration,/paper/learning-deep-cnn-denoiser-prior-for-image,https://arxiv.org/pdf/1704.03264v1.pdf
BSD68 sigma50,,# 7,PSNR,26.19,Deep CNN Denoiser,-,Image Denoising,Learning Deep CNN Denoiser Prior for Image Restoration,/paper/learning-deep-cnn-denoiser-prior-for-image,https://arxiv.org/pdf/1704.03264v1.pdf
COCO,,# 22,Bounding Box AP,40.8,RetinaNet,-,Object Detection,Focal Loss for Dense Object Detection,/paper/focal-loss-for-dense-object-detection,https://arxiv.org/pdf/1708.02002v2.pdf
SUBJ,,# 2,Accuracy,94.8,CNN+MCFA,-,Subjectivity Analysis,Translations as Additional Contexts for Sentence Classification,/paper/translations-as-additional-contexts-for,https://arxiv.org/pdf/1806.05516v1.pdf
TREC-6,,# 4,Error,4.0,CNN+MCFA,-,Text Classification,Translations as Additional Contexts for Sentence Classification,/paper/translations-as-additional-contexts-for,https://arxiv.org/pdf/1806.05516v1.pdf
NarrativeQA,,# 6,BLEU-1,54.60/55.55,Oracle IR Models,-,Question Answering,The NarrativeQA Reading Comprehension Challenge,/paper/the-narrativeqa-reading-comprehension,https://arxiv.org/pdf/1712.07040v1.pdf
NarrativeQA,,# 6,BLEU-4,26.71/27.78,Oracle IR Models,-,Question Answering,The NarrativeQA Reading Comprehension Challenge,/paper/the-narrativeqa-reading-comprehension,https://arxiv.org/pdf/1712.07040v1.pdf
LibriSpeech test-clean,,# 1,Word Error Rate (WER),2.5,LAS + SpecAugment,-,Speech Recognition,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,/paper/specaugment-a-simple-data-augmentation-method,https://arxiv.org/pdf/1904.08779v1.pdf
LibriSpeech test-clean,,# 4,Word Error Rate (WER),3.2,LAS,-,Speech Recognition,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,/paper/specaugment-a-simple-data-augmentation-method,https://arxiv.org/pdf/1904.08779v1.pdf
LibriSpeech test-other,,# 1,Word Error Rate (WER),5.8,LAS + SpecAugment,-,Speech Recognition,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,/paper/specaugment-a-simple-data-augmentation-method,https://arxiv.org/pdf/1904.08779v1.pdf
Switchboard + Hub500,,# 6,Percentage error,6.8,LAS + SpecAugment (SM),-,Speech Recognition,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,/paper/specaugment-a-simple-data-augmentation-method,https://arxiv.org/pdf/1904.08779v1.pdf
ACL-ARC,,# 6,F1,41.0,SVM,-,Citation Intent Classification,Purpose and Polarity of Citation: Towards NLP-based Bibliometrics,/paper/purpose-and-polarity-of-citation-towards-nlp,https://aclweb.org/anthology/N13-1067
WMT2014 English-French,,# 1,BLEU,36.2,SMT + NMT (tuning and joint refinement),-,Unsupervised Machine Translation,An Effective Approach to Unsupervised Machine Translation,/paper/an-effective-approach-to-unsupervised-machine,https://arxiv.org/pdf/1902.01313v1.pdf
WMT2014 English-German,,# 1,BLEU,22.5,SMT + NMT (tuning and joint refinement),-,Unsupervised Machine Translation,An Effective Approach to Unsupervised Machine Translation,/paper/an-effective-approach-to-unsupervised-machine,https://arxiv.org/pdf/1902.01313v1.pdf
WMT2014 French-English,,# 1,BLEU,33.5,SMT + NMT (tuning and joint refinement),-,Unsupervised Machine Translation,An Effective Approach to Unsupervised Machine Translation,/paper/an-effective-approach-to-unsupervised-machine,https://arxiv.org/pdf/1902.01313v1.pdf
WMT2014 German-English,,# 1,BLEU,27.0,SMT + NMT (tuning and joint refinement),-,Unsupervised Machine Translation,An Effective Approach to Unsupervised Machine Translation,/paper/an-effective-approach-to-unsupervised-machine,https://arxiv.org/pdf/1902.01313v1.pdf
WMT2016 English-German,,# 1,BLEU,26.9,SMT + NMT (tuning and joint refinement),-,Unsupervised Machine Translation,An Effective Approach to Unsupervised Machine Translation,/paper/an-effective-approach-to-unsupervised-machine,https://arxiv.org/pdf/1902.01313v1.pdf
WMT2016 German-English,,# 1,BLEU,34.4,SMT + NMT (tuning and joint refinement),-,Unsupervised Machine Translation,An Effective Approach to Unsupervised Machine Translation,/paper/an-effective-approach-to-unsupervised-machine,https://arxiv.org/pdf/1902.01313v1.pdf
Cohn-Kanade,,# 1,Accuracy,88.7%,Sequential forward selection,-,Facial Expression Recognition,Greedy Search for Descriptive Spatial Face Features,/paper/greedy-search-for-descriptive-spatial-face,https://arxiv.org/pdf/1701.01879v2.pdf
Data3DâR2N2,,# 5,Avg F1,33.8,N3MR,-,3D Object Reconstruction,Neural 3D Mesh Renderer,/paper/neural-3d-mesh-renderer,https://arxiv.org/pdf/1711.07566v1.pdf
OMNIGLOT - 1-Shot Learning,,# 3,Accuracy,98.4%,Memory Mod,-,Few-Shot Image Classification,Learning to Remember Rare Events,/paper/learning-to-remember-rare-events,https://arxiv.org/pdf/1703.03129v1.pdf
OMNIGLOT - 5-Shot Learning,,# 3,Accuracy,99.6%,Memory Mod,-,Few-Shot Image Classification,Learning to Remember Rare Events,/paper/learning-to-remember-rare-events,https://arxiv.org/pdf/1703.03129v1.pdf
One Billion Word,,# 15,PPL,52.9,Sparse Non-Negative,-,Language Modelling,Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation,/paper/skip-gram-language-modeling-using-sparse-non,https://arxiv.org/pdf/1412.1454v2.pdf
One Billion Word,,# 1,Number of params,33B,Sparse Non-Negative,-,Language Modelling,Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation,/paper/skip-gram-language-modeling-using-sparse-non,https://arxiv.org/pdf/1412.1454v2.pdf
BSD100 - 4x upscaling,,# 10,PSNR,27.56,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
BSD100 - 4x upscaling,,# 14,SSIM,0.735,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
Set14 - 4x upscaling,,# 10,PSNR,28.54,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
Set14 - 4x upscaling,,# 16,SSIM,0.78,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
Set5 - 4x upscaling,,# 8,PSNR,32.13,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
Set5 - 4x upscaling,,# 13,SSIM,0.8932,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
Urban100 - 4x upscaling,,# 10,PSNR,26.05,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
Urban100 - 4x upscaling,,# 11,SSIM,0.7834,SRRAM,-,Image Super-Resolution,RAM: Residual Attention Module for Single Image Super-Resolution,/paper/ram-residual-attention-module-for-single,https://arxiv.org/pdf/1811.12043v1.pdf
PASCAL VOC 2007,,# 18,MAP,63.4%,YOLO,-,Object Detection,"You Only Look Once: Unified, Real-Time Object Detection",/paper/you-only-look-once-unified-real-time-object,https://arxiv.org/pdf/1506.02640v5.pdf
PASCAL VOC 2007,,# 6,MAP,63.4%,YOLO,-,Real-Time Object Detection,"You Only Look Once: Unified, Real-Time Object Detection",/paper/you-only-look-once-unified-real-time-object,https://arxiv.org/pdf/1506.02640v5.pdf
PASCAL VOC 2007,,# 2,FPS,46,YOLO,-,Real-Time Object Detection,"You Only Look Once: Unified, Real-Time Object Detection",/paper/you-only-look-once-unified-real-time-object,https://arxiv.org/pdf/1506.02640v5.pdf
MPII Human Pose,,# 9,PCKh-0.5,87.5%,Tucker T-Net,-,Pose Estimation,T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor,/paper/t-net-parametrizing-fully-convolutional-nets,https://arxiv.org/pdf/1904.02698v1.pdf
COCO,,# 21,Bounding Box AP,40.9,D-RFCN + ResNet-101 (6 scales),-,Object Detection,Deformable Convolutional Networks,/paper/deformable-convolutional-networks,https://arxiv.org/pdf/1703.06211v3.pdf
Human3.6M,,# 1,Average 3D Error,51.8,Fully supervised EpipolarPose,-,3D Human Pose Estimation,Self-Supervised Learning of 3D Human Pose using Multi-view Geometry,/paper/self-supervised-learning-of-3d-human-pose,https://arxiv.org/pdf/1903.02330v2.pdf
Human3.6M,,# 2,Average 3D Error,55.0,Self supervised EpipolarPose,-,3D Human Pose Estimation,Self-Supervised Learning of 3D Human Pose using Multi-view Geometry,/paper/self-supervised-learning-of-3d-human-pose,https://arxiv.org/pdf/1903.02330v2.pdf
Second dialogue state tracking challenge,,# 1,Joint,75.5,StateNet,-,Dialogue State Tracking,Towards Universal Dialogue State Tracking,/paper/towards-universal-dialogue-state-tracking,https://arxiv.org/pdf/1810.09587v1.pdf
Wizard-of-Oz,,# 1,Joint,88.9,StateNet,-,Dialogue State Tracking,Towards Universal Dialogue State Tracking,/paper/towards-universal-dialogue-state-tracking,https://arxiv.org/pdf/1810.09587v1.pdf
Sequential MNIST,,# 5,Unpermuted Accuracy,96.9%,Full-capacity uRNN,-,Sequential Image Classification,Full-Capacity Unitary Recurrent Neural Networks,/paper/full-capacity-unitary-recurrent-neural,https://arxiv.org/pdf/1611.00035v1.pdf
Sequential MNIST,,# 3,Permuted Accuracy,94.1%,Full-capacity uRNN,-,Sequential Image Classification,Full-Capacity Unitary Recurrent Neural Networks,/paper/full-capacity-unitary-recurrent-neural,https://arxiv.org/pdf/1611.00035v1.pdf
ShapeNet-Part,,# 1,Instance Average IoU,86.0,SSCN,-,3D Part Segmentation,Submanifold Sparse Convolutional Networks,/paper/submanifold-sparse-convolutional-networks,https://arxiv.org/pdf/1706.01307v1.pdf
SWAG,,# 3,Dev,59.1,ESIM + ELMo,-,Common Sense Reasoning,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,/paper/swag-a-large-scale-adversarial-dataset-for,https://arxiv.org/pdf/1808.05326v1.pdf
SWAG,,# 2,Test,59.2,ESIM + ELMo,-,Common Sense Reasoning,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,/paper/swag-a-large-scale-adversarial-dataset-for,https://arxiv.org/pdf/1808.05326v1.pdf
SWAG,,# 4,Dev,51.9,ESIM + GloVe,-,Common Sense Reasoning,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,/paper/swag-a-large-scale-adversarial-dataset-for,https://arxiv.org/pdf/1808.05326v1.pdf
SWAG,,# 3,Test,52.7,ESIM + GloVe,-,Common Sense Reasoning,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,/paper/swag-a-large-scale-adversarial-dataset-for,https://arxiv.org/pdf/1808.05326v1.pdf
PASCAL VOC 2007,,# 3,MAP,51.2,pipeline method,-,Weakly Supervised Object Detection,"Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning",/paper/multi-evidence-filtering-and-fusion-for-multi,https://arxiv.org/pdf/1802.09129v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 2,Accuracy,50.44%,Relation-Net,-,Few-Shot Learning,Learning to Compare: Relation Network for Few-Shot Learning,/paper/learning-to-compare-relation-network-for-few,https://arxiv.org/pdf/1711.06025v2.pdf
Mini-ImageNet - 5-Shot Learning,,# 1,Accuracy,65.32%,Relation-Net,-,Few-Shot Learning,Learning to Compare: Relation Network for Few-Shot Learning,/paper/learning-to-compare-relation-network-for-few,https://arxiv.org/pdf/1711.06025v2.pdf
Penn Treebank,,# 2,F1 score,95.77,JMT,-,Chunking,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,/paper/a-joint-many-task-model-growing-a-neural,https://arxiv.org/pdf/1611.01587v5.pdf
General,,# 6,MAP,5.77,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
General,,# 6,MRR,10.56,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
General,,# 6,[emailÂ protected],5.96,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Medical domain,,# 6,MAP,11.69,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Medical domain,,# 6,MRR,25.95,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Medical domain,,# 6,[emailÂ protected],11.69,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Music domain,,# 5,MAP,4.71,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Music domain,,# 5,MRR,9.15,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Music domain,,# 5,[emailÂ protected],4.91,SJTU BCMI,-,Hypernym Discovery,SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings,/paper/sjtu-nlp-at-semeval-2018-task-9-neural-1,https://aclweb.org/anthology/S18-1147
Caltech,,# 15,Reasonable Miss Rate,12.4,Part-level CNN + saliency and bounding box alignment,-,Pedestrian Detection,Part-Level Convolutional Neural Networks for Pedestrian Detection Using Saliency and Boundary Box Alignment,/paper/part-level-convolutional-neural-networks-for,https://arxiv.org/pdf/1810.00689v1.pdf
ImageNet,,# 22,Top 1 Accuracy,69.8%,Inception V1,-,Image Classification,Going Deeper with Convolutions,/paper/going-deeper-with-convolutions,https://arxiv.org/pdf/1409.4842v1.pdf
ImageNet,,# 15,Top 5 Accuracy,89.9%,Inception V1,-,Image Classification,Going Deeper with Convolutions,/paper/going-deeper-with-convolutions,https://arxiv.org/pdf/1409.4842v1.pdf
ImageNet Detection,,# 1,MAP,43.9%,Inception V1,-,Object Detection,Going Deeper with Convolutions,/paper/going-deeper-with-convolutions,https://arxiv.org/pdf/1409.4842v1.pdf
One Billion Word,,# 7,PPL,28.0,High-Budget MoE,-,Language Modelling,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/pdf/1701.06538v1.pdf
One Billion Word,,# 1,Number of params,5B,High-Budget MoE,-,Language Modelling,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/pdf/1701.06538v1.pdf
One Billion Word,,# 11,PPL,34.1,Low-Budget MoE,-,Language Modelling,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/pdf/1701.06538v1.pdf
One Billion Word,,# 1,Number of params,5B,Low-Budget MoE,-,Language Modelling,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/pdf/1701.06538v1.pdf
WMT2014 English-French,,# 10,BLEU score,40.56,MoE,-,Machine Translation,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/pdf/1701.06538v1.pdf
WMT2014 English-German,,# 12,BLEU score,26.03,MoE,-,Machine Translation,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/pdf/1701.06538v1.pdf
Atari 2600 Alien,,# 18,Score,813.5,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Amidar,,# 14,Score,189.2,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Assault,,# 20,Score,1195.8,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Asterix,,# 18,Score,3324.7,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Asteroids,,# 19,Score,933.6,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Atlantis,,# 8,Score,629166.5,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Bank Heist,,# 18,Score,399.4,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Battle Zone,,# 16,Score,19938.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Beam Rider,,# 20,Score,3822.1,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Bowling,,# 10,Score,54.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Boxing,,# 12,Score,74.2,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Breakout,,# 19,Score,313.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Centipede,,# 9,Score,6296.9,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Chopper Command,,# 19,Score,3191.8,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Crazy Climber,,# 19,Score,65451.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Demon Attack,,# 16,Score,14880.1,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Double Dunk,,# 15,Score,-11.3,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Enduro,,# 20,Score,71.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Fishing Derby,,# 14,Score,4.6,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Freeway,,# 20,Score,10.2,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Frostbite,,# 19,Score,426.6,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Gopher,,# 19,Score,4373.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Gravitar,,# 6,Score,538.4,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 HERO,,# 19,Score,8963.4,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Ice Hockey,,# 8,Score,-1.7,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 James Bond,,# 18,Score,444.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Kangaroo,,# 17,Score,1431.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Krull,,# 16,Score,6363.1,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Kung-Fu Master,,# 19,Score,20620.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Montezuma's Revenge,,# 12,Score,84.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Ms. Pacman,,# 14,Score,1263.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Name This Game,,# 14,Score,9238.5,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Pong,,# 10,Score,16.7,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Private Eye,,# 4,Score,2598.6,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Q*Bert,,# 19,Score,7089.8,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 River Raid,,# 18,Score,5310.3,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Road Runner,,# 14,Score,43079.8,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Robotank,,# 9,Score,61.8,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Seaquest,,# 10,Score,10145.9,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Space Invaders,,# 19,Score,1183.3,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Star Gunner,,# 18,Score,14919.2,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Tennis,,# 10,Score,-0.7,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Time Pilot,,# 9,Score,8267.8,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Tutankham,,# 13,Score,118.5,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Up and Down,,# 18,Score,8747.7,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Venture,,# 7,Score,523.4,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Video Pinball,,# 15,Score,112093.4,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Wizard of Wor,,# 6,Score,10431.0,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Atari 2600 Zaxxon,,# 16,Score,6159.4,Gorila,-,Atari Games,Massively Parallel Methods for Deep Reinforcement Learning,/paper/massively-parallel-methods-for-deep,https://arxiv.org/pdf/1507.04296v2.pdf
Noun Phrase Canonicalization,,# 1,Base Dataset,98.2,CESI,-,Open Knowledge Graph Canonicalization,CESI: Canonicalizing Open Knowledge Bases using Embeddings and Side Information,/paper/cesi-canonicalizing-open-knowledge-bases,https://arxiv.org/pdf/1902.00172v1.pdf
Noun Phrase Canonicalization,,# 1,Ambiguous dataset,99.8,CESI,-,Open Knowledge Graph Canonicalization,CESI: Canonicalizing Open Knowledge Bases using Embeddings and Side Information,/paper/cesi-canonicalizing-open-knowledge-bases,https://arxiv.org/pdf/1902.00172v1.pdf
Noun Phrase Canonicalization,,# 1,ReVerb45k,99.9,CESI,-,Open Knowledge Graph Canonicalization,CESI: Canonicalizing Open Knowledge Bases using Embeddings and Side Information,/paper/cesi-canonicalizing-open-knowledge-bases,https://arxiv.org/pdf/1902.00172v1.pdf
MNIST,,# 6,Accuracy,94.73,PixelGAN Autoencoders,-,Unsupervised MNIST,PixelGAN Autoencoders,/paper/pixelgan-autoencoders,https://arxiv.org/pdf/1706.00531v1.pdf
MNIST,,# 6,Accuracy,94.73,PixelGAN Autoencoders,-,Unsupervised image classification,PixelGAN Autoencoders,/paper/pixelgan-autoencoders,https://arxiv.org/pdf/1706.00531v1.pdf
Charades,,# 3,MAP,39.7,NL I3D + GCN + Resnet-101,-,Action Recognition In Videos,Videos as Space-Time Region Graphs,/paper/videos-as-space-time-region-graphs,https://arxiv.org/pdf/1806.01810v2.pdf
COCO,,# 6,Bounding Box AP,45.7,D-RFCN + SNIP,-,Object Detection,DSOD: Learning Deeply Supervised Object Detectors from Scratch,/paper/dsod-learning-deeply-supervised-object,https://arxiv.org/pdf/1708.01241v2.pdf
ADE20K,,# 7,Validation mIoU,21.64,SegNet,-,Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
CamVid,,# 7,Mean IoU,46.4%,SegNet,-,Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
CamVid,,# 6,mIoU,46.4%,SegNet,-,Real-Time Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
CamVid,,# 4,Time (ms),217,SegNet,-,Real-Time Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
CamVid,,# 4,Frame (fps),4.6,SegNet,-,Real-Time Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
Cityscapes,,# 18,Mean IoU,57.0%,SegNet,-,Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
Cityscapes,,# 11,mIoU,57.0%,SegNet,-,Real-Time Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
Cityscapes,,# 3,Time (ms),60,SegNet,-,Real-Time Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
Cityscapes,,# 5,Frame (fps),16.7,SegNet,-,Real-Time Semantic Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
SUN-RGBD,,# 2,Mean IoU,31.84,SegNet,-,Scene Segmentation,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,/paper/segnet-a-deep-convolutional-encoder-decoder,https://arxiv.org/pdf/1511.00561v3.pdf
Citeseer,,# 3,Accuracy,71.80%,MTGAE,-,Node Classification,Multi-Task Graph Autoencoders,/paper/multi-task-graph-autoencoders,https://arxiv.org/pdf/1811.02798v1.pdf
Citeseer,,# 1,Accuracy,94.90%,MTGAE,-,Link Prediction,Multi-Task Graph Autoencoders,/paper/multi-task-graph-autoencoders,https://arxiv.org/pdf/1811.02798v1.pdf
Cora,,# 6,Accuracy,79.00%,MTGAE,-,Node Classification,Multi-Task Graph Autoencoders,/paper/multi-task-graph-autoencoders,https://arxiv.org/pdf/1811.02798v1.pdf
Cora,,# 1,Accuracy,94.60%,MTGAE,-,Link Prediction,Multi-Task Graph Autoencoders,/paper/multi-task-graph-autoencoders,https://arxiv.org/pdf/1811.02798v1.pdf
Pubmed,,# 1,Accuracy,80.40%,MTGAE,-,Node Classification,Multi-Task Graph Autoencoders,/paper/multi-task-graph-autoencoders,https://arxiv.org/pdf/1811.02798v1.pdf
Pubmed,,# 2,Accuracy,94.40%,MTGAE,-,Link Prediction,Multi-Task Graph Autoencoders,/paper/multi-task-graph-autoencoders,https://arxiv.org/pdf/1811.02798v1.pdf
SQuAD1.1,,# 45,EM,78.234,MEMEN (single model),-,Question Answering,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,/paper/memen-multi-layer-embedding-with-memory,https://arxiv.org/pdf/1707.09098v1.pdf
SQuAD1.1,,# 49,F1,85.344,MEMEN (single model),-,Question Answering,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,/paper/memen-multi-layer-embedding-with-memory,https://arxiv.org/pdf/1707.09098v1.pdf
SQuAD1.1,,# 70,EM,75.37,MEMEN (ensemble),-,Question Answering,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,/paper/memen-multi-layer-embedding-with-memory,https://arxiv.org/pdf/1707.09098v1.pdf
SQuAD1.1,,# 76,F1,82.65799999999999,MEMEN (ensemble),-,Question Answering,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,/paper/memen-multi-layer-embedding-with-memory,https://arxiv.org/pdf/1707.09098v1.pdf
TriviaQA,,# 5,EM,43.16,MEMEN,-,Question Answering,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,/paper/memen-multi-layer-embedding-with-memory,https://arxiv.org/pdf/1707.09098v1.pdf
TriviaQA,,# 5,F1,46.9,MEMEN,-,Question Answering,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,/paper/memen-multi-layer-embedding-with-memory,https://arxiv.org/pdf/1707.09098v1.pdf
MSCOCO,,# 1,MAP,29.8,YOLACT,-,Real-time Instance Segmentation,YOLACT: Real-time Instance Segmentation,/paper/yolact-real-time-instance-segmentation,https://arxiv.org/pdf/1904.02689v1.pdf
WikiBio,,# 1,BLEU,44.89,Field-gating Seq2seq + dual attention,-,Table-to-text Generation,Table-to-text Generation by Structure-aware Seq2seq Learning,/paper/table-to-text-generation-by-structure-aware,https://arxiv.org/pdf/1711.09724v1.pdf
WikiBio,,# 2,ROUGE,41.21,Field-gating Seq2seq + dual attention,-,Table-to-text Generation,Table-to-text Generation by Structure-aware Seq2seq Learning,/paper/table-to-text-generation-by-structure-aware,https://arxiv.org/pdf/1711.09724v1.pdf
WikiBio,,# 2,BLEU,44.71,Field-gating Seq2seq + dual attention + beam search,-,Table-to-text Generation,Table-to-text Generation by Structure-aware Seq2seq Learning,/paper/table-to-text-generation-by-structure-aware,https://arxiv.org/pdf/1711.09724v1.pdf
WikiBio,,# 1,ROUGE,41.65,Field-gating Seq2seq + dual attention + beam search,-,Table-to-text Generation,Table-to-text Generation by Structure-aware Seq2seq Learning,/paper/table-to-text-generation-by-structure-aware,https://arxiv.org/pdf/1711.09724v1.pdf
Data3DâR2N2,,# 4,Avg F1,39.01,3D-R2N2,-,3D Object Reconstruction,3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction,/paper/3d-r2n2-a-unified-approach-for-single-and,https://arxiv.org/pdf/1604.00449v1.pdf
CoNLL-2014 A1,,# 5,F0.5,17.86,Bi-LSTM + LMcost (trained on FCE),-,Grammatical Error Detection,Semi-supervised Multitask Learning for Sequence Labeling,/paper/semi-supervised-multitask-learning-for,https://arxiv.org/pdf/1704.07156v1.pdf
CoNLL-2014 A2,,# 6,F0.5,25.88,Bi-LSTM + LMcost (trained on FCE),-,Grammatical Error Detection,Semi-supervised Multitask Learning for Sequence Labeling,/paper/semi-supervised-multitask-learning-for,https://arxiv.org/pdf/1704.07156v1.pdf
FCE,,# 3,F0.5,48.48,Bi-LSTM + LMcost,-,Grammatical Error Detection,Semi-supervised Multitask Learning for Sequence Labeling,/paper/semi-supervised-multitask-learning-for,https://arxiv.org/pdf/1704.07156v1.pdf
Penn Treebank,,# 8,Accuracy,97.43,Bi-LSTM + LMcost,-,Part-Of-Speech Tagging,Semi-supervised Multitask Learning for Sequence Labeling,/paper/semi-supervised-multitask-learning-for,https://arxiv.org/pdf/1704.07156v1.pdf
CIFAR-10,,# 1,Accuracy,61.7,IIC,-,Unsupervised image classification,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
CIFAR-20,,# 1,Accuracy,25.7,IIC,-,Unsupervised image classification,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
COCO-Stuff-15,,# 1,Accuracy,27.7,IIC,-,Unsupervised semantic segmentation,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
COCO-Stuff-3,,# 1,Accuracy,72.3,IIC,-,Unsupervised semantic segmentation,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
MNIST,,# 1,Accuracy,99.3,IIC,-,Unsupervised image classification,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
Potsdam,,# 1,Accuracy,65.1,IIC,-,Unsupervised semantic segmentation,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
Potsdam-3,,# 1,Accuracy,45.4,IIC,-,Unsupervised semantic segmentation,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
STL-10,,# 1,Percentage correct,88.8,IIC,-,Image Classification,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
STL-10,,# 1,Accuracy,88.8,IIC,-,Semi-Supervised Image Classification,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
STL-10,,# 1,Accuracy,61.00%,IIC,-,Unsupervised image classification,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-clustering-for,https://arxiv.org/pdf/1807.06653v3.pdf
MNIST,,# 1,Accuracy,99.3,IIC,-,Unsupervised MNIST,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,/paper/invariant-information-distillation-for,https://arxiv.org/pdf/1807.06653v3.pdf
Caltech,,# 10,Reasonable Miss Rate,8.18,F-DNN+SS,-,Pedestrian Detection,Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection,/paper/fused-dnn-a-deep-neural-network-fusion,https://arxiv.org/pdf/1610.03466v2.pdf
Citeseer,,# 1,Accuracy,73.0%,LGCN sub,-,Node Classification,Large-Scale Learnable Graph Convolutional Networks,/paper/large-scale-learnable-graph-convolutional,https://arxiv.org/pdf/1808.03965v1.pdf
Cora,,# 1,Accuracy,83.30%,LGCN sub,-,Node Classification,Large-Scale Learnable Graph Convolutional Networks,/paper/large-scale-learnable-graph-convolutional,https://arxiv.org/pdf/1808.03965v1.pdf
Cora,,# 2,Accuracy,83.3%,LGCN,-,Document Classification,Large-Scale Learnable Graph Convolutional Networks,/paper/large-scale-learnable-graph-convolutional,https://arxiv.org/pdf/1808.03965v1.pdf
Pubmed,,# 2,Accuracy,79.50%,LGCN sub,-,Node Classification,Large-Scale Learnable Graph Convolutional Networks,/paper/large-scale-learnable-graph-convolutional,https://arxiv.org/pdf/1808.03965v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 6,Accuracy,49.97%,Reptile + Transduction,-,Few-Shot Image Classification,On First-Order Meta-Learning Algorithms,/paper/on-first-order-meta-learning-algorithms,https://arxiv.org/pdf/1803.02999v3.pdf
Mini-ImageNet - 5-Shot Learning,,# 7,Accuracy,65.99%,Reptile + Transduction,-,Few-Shot Image Classification,On First-Order Meta-Learning Algorithms,/paper/on-first-order-meta-learning-algorithms,https://arxiv.org/pdf/1803.02999v3.pdf
OMNIGLOT - 1-Shot Learning,,# 5,Accuracy,97.68%,Reptile + Transduction,-,Few-Shot Image Classification,On First-Order Meta-Learning Algorithms,/paper/on-first-order-meta-learning-algorithms,https://arxiv.org/pdf/1803.02999v3.pdf
OMNIGLOT - 5-Shot Learning,,# 5,Accuracy,99.48%,Reptile + Transduction,-,Few-Shot Image Classification,On First-Order Meta-Learning Algorithms,/paper/on-first-order-meta-learning-algorithms,https://arxiv.org/pdf/1803.02999v3.pdf
iSEG 2017 Challenge,,# 1,Dice Score,0.9243,LiviaNet (SemiDenseNet),-,Infant Brain Mri Segmentation,Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation,/paper/deep-cnn-ensembles-and-suggestive-annotations,https://arxiv.org/pdf/1712.05319v2.pdf
MOSI,,# 3,Accuracy,77.1%,MARN,-,Multimodal Sentiment Analysis,Multi-attention Recurrent Network for Human Communication Comprehension,/paper/multi-attention-recurrent-network-for-human,https://arxiv.org/pdf/1802.00923v1.pdf
WMT2014 English-French,,# 13,BLEU score,39.2,Deep-Att + PosUnk,-,Machine Translation,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,/paper/deep-recurrent-models-with-fast-forward,https://arxiv.org/pdf/1606.04199v3.pdf
WMT2014 English-French,,# 19,BLEU score,35.9,Deep-Att,-,Machine Translation,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,/paper/deep-recurrent-models-with-fast-forward,https://arxiv.org/pdf/1606.04199v3.pdf
WMT2014 English-German,,# 16,BLEU score,20.7,Deep-Att,-,Machine Translation,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,/paper/deep-recurrent-models-with-fast-forward,https://arxiv.org/pdf/1606.04199v3.pdf
PASCAL VOC 2007,,# 17,MAP,68.5%,subCNN,-,Object Detection,Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection,/paper/subcategory-aware-convolutional-neural,https://arxiv.org/pdf/1604.04693v3.pdf
Caltech,,# 6,Reasonable Miss Rate,5.5,HyperLearner,-,Pedestrian Detection,What Can Help Pedestrian Detection?,/paper/what-can-help-pedestrian-detection,https://arxiv.org/pdf/1705.02757v1.pdf
Penn Treebank (Word Level),,# 18,Validation perplexity,75.7,Tied Variational LSTM + augmented loss,-,Language Modelling,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,/paper/tying-word-vectors-and-word-classifiers-a,https://arxiv.org/pdf/1611.01462v3.pdf
Penn Treebank (Word Level),,# 21,Test perplexity,73.2,Tied Variational LSTM + augmented loss,-,Language Modelling,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,/paper/tying-word-vectors-and-word-classifiers-a,https://arxiv.org/pdf/1611.01462v3.pdf
Penn Treebank (Word Level),,# 1,Params,24M,Tied Variational LSTM + augmented loss,-,Language Modelling,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,/paper/tying-word-vectors-and-word-classifiers-a,https://arxiv.org/pdf/1611.01462v3.pdf
CHiME real,,# 2,Percentage error,14.6,Li-GRU,-,Noisy Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 4,Percentage error,15.9,RNN + Dropout + BatchNorm + Monophone Reg,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 8,Percentage error,16.6,GRU,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 1,Percentage error,14.2,LiGRU + Dropout + BatchNorm + Monophone Reg,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 2,Percentage error,14.5,LSTM + Dropout + BatchNorm + Monophone Reg,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 3,Percentage error,14.9,GRU + Dropout + BatchNorm + Monophone Reg,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 6,Percentage error,16.3,Li-GRU,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 5,Percentage error,16.0,LSTM,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
TIMIT,,# 7,Percentage error,16.5,RNN,-,Speech Recognition,The PyTorch-Kaldi Speech Recognition Toolkit,/paper/the-pytorch-kaldi-speech-recognition-toolkit,https://arxiv.org/pdf/1811.07453v2.pdf
WikiQA,,# 5,MAP,0.701,PairwiseRank + Multi-Perspective CNN,-,Question Answering,Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency,/paper/noise-contrastive-estimation-and-negative,https://arxiv.org/pdf/1809.01812v1.pdf
WikiQA,,# 5,MRR,0.718,PairwiseRank + Multi-Perspective CNN,-,Question Answering,Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency,/paper/noise-contrastive-estimation-and-negative,https://arxiv.org/pdf/1809.01812v1.pdf
Yahoo Questions,,# 3,NLL,332.1,CNN-VAE,-,Text Generation,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,/paper/improved-variational-autoencoders-for-text,https://arxiv.org/pdf/1702.08139v2.pdf
Yahoo Questions,,# 1,KL,10.0,CNN-VAE,-,Text Generation,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,/paper/improved-variational-autoencoders-for-text,https://arxiv.org/pdf/1702.08139v2.pdf
Yahoo Questions,,# 3,Perplexity,63.9,CNN-VAE,-,Text Generation,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,/paper/improved-variational-autoencoders-for-text,https://arxiv.org/pdf/1702.08139v2.pdf
IEMOCAP,,# 3,F1,56.13%,CMN,-,Emotion Recognition in Conversation,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,/paper/conversational-memory-network-for-emotion,https://aclweb.org/anthology/N18-1193
Annotated Faces in the Wild,,# 5,AP,0.9917,LRN + RSA,-,Face Detection,Recurrent Scale Approximation for Object Detection in CNN,/paper/recurrent-scale-approximation-for-object,https://arxiv.org/pdf/1707.09531v2.pdf
Query Wellformedness,,# 1,Accuracy,70.7,"word-1, 2 POS-1, 2, 3",-,Query Wellformedness,Identifying Well-formed Natural Language Questions,/paper/identifying-well-formed-natural-language,https://arxiv.org/pdf/1808.09419v1.pdf
WMT 2014 EN-DE,,# 1,BLEU,28.9,universal transformer base,-,Machine Translation,Universal Transformers,/paper/universal-transformers,https://arxiv.org/pdf/1807.03819v3.pdf
WMT2014 English-German,,# 6,BLEU score,28.9,universal transformer base,-,Machine Translation,Universal Transformers,/paper/universal-transformers,https://arxiv.org/pdf/1807.03819v3.pdf
CNN / Daily Mail,,# 1,PPL,23.6,C2F + ALTERNATE,-,Document Summarization,Coarse-to-Fine Attention Models for Document Summarization,/paper/coarse-to-fine-attention-models-for-document,https://aclweb.org/anthology/W17-4505
CNN / Daily Mail,,# 3,ROUGE-1,31.1,C2F + ALTERNATE,-,Document Summarization,Coarse-to-Fine Attention Models for Document Summarization,/paper/coarse-to-fine-attention-models-for-document,https://aclweb.org/anthology/W17-4505
CNN / Daily Mail,,# 3,ROUGE-2,15.4,C2F + ALTERNATE,-,Document Summarization,Coarse-to-Fine Attention Models for Document Summarization,/paper/coarse-to-fine-attention-models-for-document,https://aclweb.org/anthology/W17-4505
CNN / Daily Mail,,# 3,ROUGE-L,28.8,C2F + ALTERNATE,-,Document Summarization,Coarse-to-Fine Attention Models for Document Summarization,/paper/coarse-to-fine-attention-models-for-document,https://aclweb.org/anthology/W17-4505
MPII Multi-Person,,# 5,AP,74.3%,Articulated Tracking,-,Multi-Person Pose Estimation,ArtTrack: Articulated Multi-person Tracking in the Wild,/paper/arttrack-articulated-multi-person-tracking-in,https://arxiv.org/pdf/1612.01465v3.pdf
Switchboard (300hr),,# 1,Word Error Rate (WER),9.3,End-to-end LF-MMI,-,Speech Recognition,End-to-end speech recognition using lattice-free MMI,/paper/end-to-end-speech-recognition-using-lattice,https://www.danielpovey.com/files/2018_interspeech_end2end.pdf
WSJ eval92,,# 2,Percentage error,3.0,End-to-end LF-MMI,-,Speech Recognition,End-to-end speech recognition using lattice-free MMI,/paper/end-to-end-speech-recognition-using-lattice,https://www.danielpovey.com/files/2018_interspeech_end2end.pdf
WSJ eval92,,# 1,Word Error Rate (WER),3.0,End-to-end LF-MMI,-,Speech Recognition,End-to-end speech recognition using lattice-free MMI,/paper/end-to-end-speech-recognition-using-lattice,https://www.danielpovey.com/files/2018_interspeech_end2end.pdf
Scan2CAD,,# 2,Average Accuracy,10.29%,3DMatch,-,3D Reconstruction,3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions,/paper/3dmatch-learning-local-geometric-descriptors,https://arxiv.org/pdf/1603.08182v3.pdf
Atari 2600 Freeway,,# 1,Score,34.0,TRPO-hash,-,Atari Games,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,/paper/exploration-a-study-of-count-based,https://arxiv.org/pdf/1611.04717v3.pdf
Atari 2600 Frostbite,,# 2,Score,5214.0,TRPO-hash,-,Atari Games,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,/paper/exploration-a-study-of-count-based,https://arxiv.org/pdf/1611.04717v3.pdf
Atari 2600 Montezuma's Revenge,,# 13,Score,75.0,TRPO-hash,-,Atari Games,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,/paper/exploration-a-study-of-count-based,https://arxiv.org/pdf/1611.04717v3.pdf
Atari 2600 Venture,,# 9,Score,445.0,TRPO-hash,-,Atari Games,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,/paper/exploration-a-study-of-count-based,https://arxiv.org/pdf/1611.04717v3.pdf
DukeMTMC-reID,,# 3,Rank-1,83.3,PCB (RPP),-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
DukeMTMC-reID,,# 3,MAP,69.2,PCB (RPP),-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
DukeMTMC-reID,,# 4,Rank-1,81.8,PCB (UP),-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
DukeMTMC-reID,,# 4,MAP,66.1,PCB (UP),-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
Market-1501,,# 3,Rank-1,93.8,PCB + RPP,-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
Market-1501,,# 3,MAP,81.6,PCB + RPP,-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
Market-1501,,# 4,Rank-1,92.3,PCB,-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
Market-1501,,# 4,MAP,77.4,PCB,-,Person Re-Identification,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),/paper/beyond-part-models-person-retrieval-with,https://arxiv.org/pdf/1711.09349v3.pdf
DukeMTMC-reID,,# 8,Rank-1,76.7,SVDNet,-,Person Re-Identification,SVDNet for Pedestrian Retrieval,/paper/svdnet-for-pedestrian-retrieval,https://arxiv.org/pdf/1703.05693v4.pdf
DukeMTMC-reID,,# 8,MAP,56.8,SVDNet,-,Person Re-Identification,SVDNet for Pedestrian Retrieval,/paper/svdnet-for-pedestrian-retrieval,https://arxiv.org/pdf/1703.05693v4.pdf
Market-1501,,# 16,Rank-1,82.3,SVDNet,-,Person Re-Identification,SVDNet for Pedestrian Retrieval,/paper/svdnet-for-pedestrian-retrieval,https://arxiv.org/pdf/1703.05693v4.pdf
Market-1501,,# 19,MAP,62.1,SVDNet,-,Person Re-Identification,SVDNet for Pedestrian Retrieval,/paper/svdnet-for-pedestrian-retrieval,https://arxiv.org/pdf/1703.05693v4.pdf
CUB-200-2011,,# 2,Accuracy,89.3%,WS-DAN,-,Fine-Grained Image Classification,See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification,/paper/see-better-before-looking-closer-weakly,https://arxiv.org/pdf/1901.09891v2.pdf
FGVC Aircraft,,# 1,Accuracy,93.0%,WS-DAN,-,Fine-Grained Image Classification,See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification,/paper/see-better-before-looking-closer-weakly,https://arxiv.org/pdf/1901.09891v2.pdf
Stanford Cars,,# 1,Accuracy,94.5%,WS-DAN,-,Fine-Grained Image Classification,See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification,/paper/see-better-before-looking-closer-weakly,https://arxiv.org/pdf/1901.09891v2.pdf
Stanford Dogs,,# 1,Accuracy,92.1,WS-DAN,-,Fine-Grained Image Classification,See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification,/paper/see-better-before-looking-closer-weakly,https://arxiv.org/pdf/1901.09891v2.pdf
WMT2014 English-French,,# 21,BLEU score,34.81,LSTM,-,Machine Translation,Sequence to Sequence Learning with Neural Networks,/paper/sequence-to-sequence-learning-with-neural,https://arxiv.org/pdf/1409.3215v3.pdf
WMT2014 English-French,,# 17,BLEU score,36.5,SMT+LSTM5,-,Machine Translation,Sequence to Sequence Learning with Neural Networks,/paper/sequence-to-sequence-learning-with-neural,https://arxiv.org/pdf/1409.3215v3.pdf
NYU Depth v2,,# 1,Mean IoU,32.3%,Dilated FCN-2s RGB,-,Semantic Segmentation,Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation,/paper/efficient-yet-deep-convolutional-neural,https://arxiv.org/pdf/1707.08254v3.pdf
PASCAL VOC 2012,,# 16,Mean IoU,69%,Dilated FCN-2s VGG19,-,Semantic Segmentation,Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation,/paper/efficient-yet-deep-convolutional-neural,https://arxiv.org/pdf/1707.08254v3.pdf
GigaWord,,# 1,ROUGE-1,37.27,FTSum_g,-,Text Summarization,Faithful to the Original: Fact Aware Neural Abstractive Summarization,/paper/faithful-to-the-original-fact-aware-neural,https://arxiv.org/pdf/1711.04434v1.pdf
GigaWord,,# 6,ROUGE-2,17.65,FTSum_g,-,Text Summarization,Faithful to the Original: Fact Aware Neural Abstractive Summarization,/paper/faithful-to-the-original-fact-aware-neural,https://arxiv.org/pdf/1711.04434v1.pdf
GigaWord,,# 3,ROUGE-L,34.24,FTSum_g,-,Text Summarization,Faithful to the Original: Fact Aware Neural Abstractive Summarization,/paper/faithful-to-the-original-fact-aware-neural,https://arxiv.org/pdf/1711.04434v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 12,Restaurant (Acc),80.98,AEN-GloVe,-,Aspect-Based Sentiment Analysis,Attentional Encoder Network for Targeted Sentiment Classification,/paper/attentional-encoder-network-for-targeted,https://arxiv.org/pdf/1902.09314v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 11,Laptop (Acc),73.51,AEN-GloVe,-,Aspect-Based Sentiment Analysis,Attentional Encoder Network for Targeted Sentiment Classification,/paper/attentional-encoder-network-for-targeted,https://arxiv.org/pdf/1902.09314v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 2,Restaurant (Acc),83.12,AEN-BERT,-,Aspect-Based Sentiment Analysis,Attentional Encoder Network for Targeted Sentiment Classification,/paper/attentional-encoder-network-for-targeted,https://arxiv.org/pdf/1902.09314v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 25,Laptop (Acc),79.93,AEN-BERT,-,Aspect-Based Sentiment Analysis,Attentional Encoder Network for Targeted Sentiment Classification,/paper/attentional-encoder-network-for-targeted,https://arxiv.org/pdf/1902.09314v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 1,Restaurant (Acc),84.46,BERT-SPC,-,Aspect-Based Sentiment Analysis,Attentional Encoder Network for Targeted Sentiment Classification,/paper/attentional-encoder-network-for-targeted,https://arxiv.org/pdf/1902.09314v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 24,Laptop (Acc),78.99,BERT-SPC,-,Aspect-Based Sentiment Analysis,Attentional Encoder Network for Targeted Sentiment Classification,/paper/attentional-encoder-network-for-targeted,https://arxiv.org/pdf/1902.09314v2.pdf
SVHN,,# 16,Percentage error,2.16,DCNN,-,Image Classification,Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks,/paper/multi-digit-number-recognition-from-street,https://arxiv.org/pdf/1312.6082v4.pdf
BSD100 - 4x upscaling,,# 16,PSNR,27.41,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
BSD100 - 4x upscaling,,# 20,SSIM,0.7297,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
Set14 - 4x upscaling,,# 19,PSNR,28.25,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
Set14 - 4x upscaling,,# 21,SSIM,0.773,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
Set5 - 4x upscaling,,# 14,PSNR,31.82,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
Set5 - 4x upscaling,,# 18,SSIM,0.8903,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
Urban100 - 4x upscaling,,# 18,PSNR,25.41,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
Urban100 - 4x upscaling,,# 17,SSIM,0.7632,IDN,-,Image Super-Resolution,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,/paper/fast-and-accurate-single-image-super,https://arxiv.org/pdf/1803.09454v1.pdf
IDHP,,# 3,Average Treatment Effect Error,0.31,OLS with separate regressors for each treatment,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 9,Average Treatment Effect Error,0.94,OLS with treatments as a feature,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 4,Average Treatment Effect Error,0.34,BART,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 10,Average Treatment Effect Error,0.96,Random Forest,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 5,Average Treatment Effect Error,0.4,Causal Forest,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 2,Average Treatment Effect Error,0.28,TARNet,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 6,Average Treatment Effect Error,0.42,Balancing Neural Network,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 7,Average Treatment Effect Error,0.79,k-NN,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 8,Average Treatment Effect Error,0.93,Balancing Linear Regression,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
IDHP,,# 1,Average Treatment Effect Error,0.27,Counterfactual Regression + WASS,-,Causal Inference,Estimating individual treatment effect: generalization bounds and algorithms,/paper/estimating-individual-treatment-effect,https://arxiv.org/pdf/1606.03976v5.pdf
Million Song Dataset,,# 1,[emailÂ protected],0.266,Mult-VAE PR,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Million Song Dataset,,# 1,[emailÂ protected],0.364,Mult-VAE PR,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Million Song Dataset,,# 1,[emailÂ protected],0.266,Mult-DAE,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Million Song Dataset,,# 2,[emailÂ protected],0.363,Mult-DAE,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
MovieLens 20M,,# 1,[emailÂ protected],0.395,Mult-VAE PR,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
MovieLens 20M,,# 1,[emailÂ protected],0.537,Mult-VAE PR,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
MovieLens 20M,,# 2,[emailÂ protected],0.387,Mult-DAE,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
MovieLens 20M,,# 2,[emailÂ protected],0.524,Mult-DAE,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Netflix,,# 1,[emailÂ protected],0.35100000000000003,Mult-VAE PR,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Netflix,,# 1,[emailÂ protected],0.444,Mult-VAE PR,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Netflix,,# 2,[emailÂ protected],0.344,Mult-DAE,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
Netflix,,# 2,[emailÂ protected],0.43799999999999994,Mult-DAE,-,Collaborative Filtering,Variational Autoencoders for Collaborative Filtering,/paper/variational-autoencoders-for-collaborative,https://arxiv.org/pdf/1802.05814v1.pdf
OCHuman,,# 1,AP,0.552,Pose2Seg (plus ground-truth keypoints),-,Human Instance Segmentation,Pose2Seg: Detection Free Human Instance Segmentation,/paper/pose2seg-detection-free-human-instance,https://arxiv.org/pdf/1803.10683v3.pdf
WIDER Face (Easy),,# 4,AP,0.95,FDNet,-,Face Detection,Face Detection Using Improved Faster RCNN,/paper/face-detection-using-improved-faster-rcnn,https://arxiv.org/pdf/1802.02142v1.pdf
WIDER Face (Hard),,# 5,AP,0.878,FDNet,-,Face Detection,Face Detection Using Improved Faster RCNN,/paper/face-detection-using-improved-faster-rcnn,https://arxiv.org/pdf/1802.02142v1.pdf
WIDER Face (Medium),,# 4,AP,0.9390000000000001,FDNet,-,Face Detection,Face Detection Using Improved Faster RCNN,/paper/face-detection-using-improved-faster-rcnn,https://arxiv.org/pdf/1802.02142v1.pdf
CHiME clean,,# 2,Percentage error,6.3,CNN + Bi-RNN + CTC (speech to letters),-,Noisy Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
CHiME real,,# 4,Percentage error,67.94,CNN + Bi-RNN + CTC (speech to letters),-,Noisy Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
swb_hub_500 WER fullSWBCH,,# 6,Percentage error,16.0,"CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trainedonlyon SWB",-,Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
Switchboard + Hub500,,# 21,Percentage error,20.0,Deep Speech,-,Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
Switchboard + Hub500,,# 15,Percentage error,12.6,Deep Speech + FSH,-,Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
Switchboard + Hub500,,# 15,Percentage error,12.6,"CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trainedonlyon SWB",-,Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
VoxForge American-Canadian,,# 2,Percentage error,15.01,Deep Speech,-,Accented Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
VoxForge Commonwealth,,# 2,Percentage error,28.46,Deep Speech,-,Accented Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
VoxForge European,,# 2,Percentage error,31.2,Deep Speech,-,Accented Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
VoxForge Indian,,# 2,Percentage error,45.35,Deep Speech,-,Accented Speech Recognition,Deep Speech: Scaling up end-to-end speech recognition,/paper/deep-speech-scaling-up-end-to-end-speech,https://arxiv.org/pdf/1412.5567v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 25,Restaurant (Acc),75.63,TD-LSTM,-,Aspect-Based Sentiment Analysis,Effective LSTMs for Target-Dependent Sentiment Classification,/paper/effective-lstms-for-target-dependent,https://arxiv.org/pdf/1512.01100v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 1,Laptop (Acc),68.13,TD-LSTM,-,Aspect-Based Sentiment Analysis,Effective LSTMs for Target-Dependent Sentiment Classification,/paper/effective-lstms-for-target-dependent,https://arxiv.org/pdf/1512.01100v2.pdf
Google Dataset,,# 1,F1,0.851,BiRNN + LM Evaluator,-,Sentence Compression,A Language Model based Evaluator for Sentence Compression,/paper/a-language-model-based-evaluator-for-sentence,https://aclweb.org/anthology/P18-2028
Google Dataset,,# 2,CR,0.39,BiRNN + LM Evaluator,-,Sentence Compression,A Language Model based Evaluator for Sentence Compression,/paper/a-language-model-based-evaluator-for-sentence,https://aclweb.org/anthology/P18-2028
CIFAR-100,,# 40,Percentage correct,63.2,Tree Priors,-,Image Classification,Discriminative Transfer Learning with Tree-based Priors,/paper/discriminative-transfer-learning-with-tree,https://papers.nips.cc/paper/5029-discriminative-transfer-learning-with-tree-based-priors.pdf
ImageNet 64x64,,# 2,Bits per byte,3.52,SPN,-,Image Generation,Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling,/paper/generating-high-fidelity-images-with-subscale,https://arxiv.org/pdf/1812.01608v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 4,Restaurant (Acc),81.6,SA-LSTM-P,-,Aspect-Based Sentiment Analysis,Learning Latent Opinions for Aspect-Level Sentiment Classification,/paper/learning-latent-opinions-for-aspect-level,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17327/16110
SemEval 2014 Task 4 Sub Task 2,,# 17,Laptop (Acc),75.1,SA-LSTM-P,-,Aspect-Based Sentiment Analysis,Learning Latent Opinions for Aspect-Level Sentiment Classification,/paper/learning-latent-opinions-for-aspect-level,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17327/16110
D&D,,# 1,Accuracy,76.27%,PSCN,-,Graph Classification,Learning Convolutional Neural Networks for Graphs,/paper/learning-convolutional-neural-networks-for,https://arxiv.org/pdf/1605.05273v4.pdf
IMDb-B,,# 4,Accuracy,71.00%,PSCN,-,Graph Classification,Learning Convolutional Neural Networks for Graphs,/paper/learning-convolutional-neural-networks-for,https://arxiv.org/pdf/1605.05273v4.pdf
MUTAG,,# 1,Accuracy,88.95%,PSCN,-,Graph Classification,Learning Convolutional Neural Networks for Graphs,/paper/learning-convolutional-neural-networks-for,https://arxiv.org/pdf/1605.05273v4.pdf
NCI1,,# 3,Accuracy,76.34%,PSCN,-,Graph Classification,Learning Convolutional Neural Networks for Graphs,/paper/learning-convolutional-neural-networks-for,https://arxiv.org/pdf/1605.05273v4.pdf
Cityscapes,,# 7,Mean IoU,80.3%,Smooth Network with Channel Attention Block,-,Semantic Segmentation,Learning a Discriminative Feature Network for Semantic Segmentation,/paper/learning-a-discriminative-feature-network-for,https://arxiv.org/pdf/1804.09337v1.pdf
PASCAL VOC 2012,,# 3,Mean IoU,86.2%,Smooth Network with Channel Attention Block,-,Semantic Segmentation,Learning a Discriminative Feature Network for Semantic Segmentation,/paper/learning-a-discriminative-feature-network-for,https://arxiv.org/pdf/1804.09337v1.pdf
TREC-6,,# 4,Error,4,TBCNN,-,Text Classification,Discriminative Neural Sentence Modeling by Tree-Based Convolution,/paper/discriminative-neural-sentence-modeling-by,https://arxiv.org/pdf/1504.01106v5.pdf
CUB-200 - 0-Shot Learning,,# 1,Accuracy,54.6%,Prototypical-Nets,-,Few-Shot Image Classification,Prototypical Networks for Few-shot Learning,/paper/prototypical-networks-for-few-shot-learning,https://arxiv.org/pdf/1703.05175v2.pdf
Mini-ImageNet - 1-Shot Learning,,# 7,Accuracy,49.42%,Prototypical-Nets + C64F feature extractor,-,Few-Shot Image Classification,Prototypical Networks for Few-shot Learning,/paper/prototypical-networks-for-few-shot-learning,https://arxiv.org/pdf/1703.05175v2.pdf
Mini-ImageNet - 5-Shot Learning,,# 4,Accuracy,68.20%,Prototypical-Nets + C64F feature extractor,-,Few-Shot Image Classification,Prototypical Networks for Few-shot Learning,/paper/prototypical-networks-for-few-shot-learning,https://arxiv.org/pdf/1703.05175v2.pdf
OMNIGLOT - 1-Shot Learning,,# 1,Accuracy,98.8%,Prototypical-Nets,-,Few-Shot Image Classification,Prototypical Networks for Few-shot Learning,/paper/prototypical-networks-for-few-shot-learning,https://arxiv.org/pdf/1703.05175v2.pdf
OMNIGLOT - 5-Shot Learning,,# 2,Accuracy,99.7%,Prototypical-Nets,-,Few-Shot Image Classification,Prototypical Networks for Few-shot Learning,/paper/prototypical-networks-for-few-shot-learning,https://arxiv.org/pdf/1703.05175v2.pdf
TREC Robust04,,# 9,MAP,0.272,POSIT-DRMM-MV,-,Ad-Hoc Information Retrieval,Deep Relevance Ranking Using Enhanced Document-Query Interactions,/paper/deep-relevance-ranking-using-enhanced,https://arxiv.org/pdf/1809.01682v2.pdf
TREC Robust04,,# 10,MAP,0.258,PACRR,-,Ad-Hoc Information Retrieval,Deep Relevance Ranking Using Enhanced Document-Query Interactions,/paper/deep-relevance-ranking-using-enhanced,https://arxiv.org/pdf/1809.01682v2.pdf
BSD100 - 4x upscaling,,# 3,PSNR,27.77,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
BSD100 - 4x upscaling,,# 5,SSIM,0.7436,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Manga109 - 4x upscaling,,# 2,PSNR,31.22,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Manga109 - 4x upscaling,,# 2,SSIM,0.9173,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Set14 - 4x upscaling,,# 4,PSNR,28.87,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Set14 - 4x upscaling,,# 6,SSIM,0.7889,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Set5 - 4x upscaling,,# 3,PSNR,32.63,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Set5 - 4x upscaling,,# 4,SSIM,0.9002,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Urban100 - 4x upscaling,,# 3,PSNR,26.82,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
Urban100 - 4x upscaling,,# 2,SSIM,0.8087,RCAN,-,Image Super-Resolution,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,/paper/image-super-resolution-using-very-deep,https://arxiv.org/pdf/1807.02758v2.pdf
BRATS-2015,,# 2,Dice Score,84%,AFN-6,-,Brain Tumor Segmentation,Autofocus Layer for Semantic Segmentation,/paper/autofocus-layer-for-semantic-segmentation,https://arxiv.org/pdf/1805.08403v3.pdf
Penn Treebank,,# 3,POS,97.3,Arc-hybrid,-,Dependency Parsing,Training with Exploration Improves a Greedy Stack-LSTM Parser,/paper/training-with-exploration-improves-a-greedy,https://arxiv.org/pdf/1603.03793v2.pdf
Penn Treebank,,# 8,UAS,93.56,Arc-hybrid,-,Dependency Parsing,Training with Exploration Improves a Greedy Stack-LSTM Parser,/paper/training-with-exploration-improves-a-greedy,https://arxiv.org/pdf/1603.03793v2.pdf
Penn Treebank,,# 8,LAS,91.42,Arc-hybrid,-,Dependency Parsing,Training with Exploration Improves a Greedy Stack-LSTM Parser,/paper/training-with-exploration-improves-a-greedy,https://arxiv.org/pdf/1603.03793v2.pdf
SST-5 Fine-grained classification,,# 9,Accuracy,49.6,Epic,-,Sentiment Analysis,"Less Grammar, More Features",/paper/less-grammar-more-features,https://aclweb.org/anthology/P14-1022
BIWI,,# 3,MAE,13.852,KEPLER,-,Head Pose Estimation,KEPLER: Keypoint and Pose Estimation of Unconstrained Faces by Learning Efficient H-CNN Regressors,/paper/kepler-keypoint-and-pose-estimation-of,https://arxiv.org/pdf/1702.05085v1.pdf
ImageNet,,# 6,Top 1 Accuracy,81.3%,PolyNet,-,Image Classification,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,/paper/polynet-a-pursuit-of-structural-diversity-in,https://arxiv.org/pdf/1611.05725v2.pdf
ImageNet,,# 4,Top 5 Accuracy,95.8%,PolyNet,-,Image Classification,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,/paper/polynet-a-pursuit-of-structural-diversity-in,https://arxiv.org/pdf/1611.05725v2.pdf
WMT2014 English-French,,# 1,BLEU score,45.6,Transformer Big + BT,-,Machine Translation,Understanding Back-Translation at Scale,/paper/understanding-back-translation-at-scale,https://arxiv.org/pdf/1808.09381v2.pdf
SR11Deep,,# 1,BLEU,0.6659999999999999,GCN + feat,-,Data-to-Text Generation,Deep Graph Convolutional Encoders for Structured Data to Text Generation,/paper/deep-graph-convolutional-encoders-for,https://arxiv.org/pdf/1810.09995v1.pdf
WebNLG,,# 1,BLEU,0.5589999999999999,GCN EC,-,Data-to-Text Generation,Deep Graph Convolutional Encoders for Structured Data to Text Generation,/paper/deep-graph-convolutional-encoders-for,https://arxiv.org/pdf/1810.09995v1.pdf
PASCAL VOC 2007,,# 9,MAP,43.7,Deep Self-Taught Learning,-,Weakly Supervised Object Detection,Deep Self-Taught Learning for Weakly Supervised Object Localization,/paper/deep-self-taught-learning-for-weakly,https://arxiv.org/pdf/1704.05188v2.pdf
PASCAL VOC 2012,,# 7,MAP,38.3,Deep Self-Taught Learning,-,Weakly Supervised Object Detection,Deep Self-Taught Learning for Weakly Supervised Object Localization,/paper/deep-self-taught-learning-for-weakly,https://arxiv.org/pdf/1704.05188v2.pdf
FLIC Elbows,,# 2,[emailÂ protected],97.59%,Convolutional Pose Machines,-,Pose Estimation,Convolutional Pose Machines,/paper/convolutional-pose-machines,https://arxiv.org/pdf/1602.00134v4.pdf
FLIC Wrists,,# 2,[emailÂ protected],95.03%,Convolutional Pose Machines,-,Pose Estimation,Convolutional Pose Machines,/paper/convolutional-pose-machines,https://arxiv.org/pdf/1602.00134v4.pdf
Leeds Sports Poses,,# 3,PCK,90.5%,Convolutional Pose Machines,-,Pose Estimation,Convolutional Pose Machines,/paper/convolutional-pose-machines,https://arxiv.org/pdf/1602.00134v4.pdf
MPII Human Pose,,# 8,PCKh-0.5,88.52%,Convolutional Pose Machines,-,Pose Estimation,Convolutional Pose Machines,/paper/convolutional-pose-machines,https://arxiv.org/pdf/1602.00134v4.pdf
ShapeNet-Part,,# 3,Class Average IoU,82.0,SPLATNet 3D,-,3D Part Segmentation,SPLATNet: Sparse Lattice Networks for Point Cloud Processing,/paper/splatnet-sparse-lattice-networks-for-point,https://arxiv.org/pdf/1802.08275v4.pdf
ShapeNet-Part,,# 6,Instance Average IoU,84.6,SPLATNet 3D,-,3D Part Segmentation,SPLATNet: Sparse Lattice Networks for Point Cloud Processing,/paper/splatnet-sparse-lattice-networks-for-point,https://arxiv.org/pdf/1802.08275v4.pdf
LineMOD,,# 1,Accuracy,97.5%,PoseCNN + DeepIM,-,6D Pose Estimation,DeepIM: Deep Iterative Matching for 6D Pose Estimation,/paper/deepim-deep-iterative-matching-for-6d-pose,https://arxiv.org/pdf/1804.00175v3.pdf
Children's Book Test,,# 1,Accuracy-CN,93.30%,GPT-2,-,Question Answering,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
Children's Book Test,,# 1,Accuracy-NE,89.05%,GPT-2,-,Question Answering,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
CNN / Daily Mail,,# 4,ROUGE-1,29.34,GPT-2,-,Document Summarization,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
CNN / Daily Mail,,# 4,ROUGE-2,8.27,GPT-2,-,Document Summarization,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
CNN / Daily Mail,,# 4,ROUGE-L,26.58,GPT-2,-,Document Summarization,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
enwiki8,,# 1,Bit per Character (BPC),0.93,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
enwiki8,,# 1,Number of params,1542M,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
One Billion Word,,# 13,PPL,42.16,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
One Billion Word,,# 1,Number of params,1.54B,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
Penn Treebank (Word Level),,# 1,Test perplexity,35.76,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
Penn Treebank (Word Level),,# 1,Params,1542M,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
Text8,,# 1,Bit per Character (BPC),0.98,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
WikiText-103,,# 1,Test perplexity,17.48,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
WikiText-103,,# 1,Number of params,1542M,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
WikiText-2,,# 1,Test perplexity,18.34,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
WikiText-2,,# 1,Number of params,1542M,GPT-2,-,Language Modelling,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
Winograd Schema Challenge,,# 1,Score,70.70,GPT-2,-,Common Sense Reasoning,Language Models are Unsupervised Multitask Learners,/paper/language-models-are-unsupervised-multitask,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
Cityscapes,,# 1,Maximum F-measure,71.3%,CASENet,-,Edge Detection,CASENet: Deep Category-Aware Semantic Edge Detection,/paper/casenet-deep-category-aware-semantic-edge,https://arxiv.org/pdf/1705.09759v1.pdf
Cityscapes,,# 1,AP,70.8%,CASENet,-,Edge Detection,CASENet: Deep Category-Aware Semantic Edge Detection,/paper/casenet-deep-category-aware-semantic-edge,https://arxiv.org/pdf/1705.09759v1.pdf
SBD,,# 1,Maximum F-measure,71.4%,CASENet,-,Edge Detection,CASENet: Deep Category-Aware Semantic Edge Detection,/paper/casenet-deep-category-aware-semantic-edge,https://arxiv.org/pdf/1705.09759v1.pdf
BSD100 - 4x upscaling,,# 23,PSNR,27.12,ZSSR,-,Image Super-Resolution,"""Zero-Shot"" Super-Resolution using Deep Internal Learning",/paper/zero-shot-super-resolution-using-deep,https://arxiv.org/pdf/1712.06087v1.pdf
BSD100 - 4x upscaling,,# 25,SSIM,0.7211,ZSSR,-,Image Super-Resolution,"""Zero-Shot"" Super-Resolution using Deep Internal Learning",/paper/zero-shot-super-resolution-using-deep,https://arxiv.org/pdf/1712.06087v1.pdf
Set14 - 4x upscaling,,# 24,PSNR,28.01,ZSSR,-,Image Super-Resolution,"""Zero-Shot"" Super-Resolution using Deep Internal Learning",/paper/zero-shot-super-resolution-using-deep,https://arxiv.org/pdf/1712.06087v1.pdf
Set14 - 4x upscaling,,# 25,SSIM,0.7651,ZSSR,-,Image Super-Resolution,"""Zero-Shot"" Super-Resolution using Deep Internal Learning",/paper/zero-shot-super-resolution-using-deep,https://arxiv.org/pdf/1712.06087v1.pdf
Set5 - 4x upscaling,,# 20,PSNR,31.13,ZSSR,-,Image Super-Resolution,"""Zero-Shot"" Super-Resolution using Deep Internal Learning",/paper/zero-shot-super-resolution-using-deep,https://arxiv.org/pdf/1712.06087v1.pdf
Set5 - 4x upscaling,,# 23,SSIM,0.8796,ZSSR,-,Image Super-Resolution,"""Zero-Shot"" Super-Resolution using Deep Internal Learning",/paper/zero-shot-super-resolution-using-deep,https://arxiv.org/pdf/1712.06087v1.pdf
WN18,,# 5,[emailÂ protected],0.9390000000000001,Gaifman,-,Link Prediction,Discriminative Gaifman Models,/paper/discriminative-gaifman-models,https://arxiv.org/pdf/1610.09369v1.pdf
WN18,,# 6,[emailÂ protected],0.7609999999999999,Gaifman,-,Link Prediction,Discriminative Gaifman Models,/paper/discriminative-gaifman-models,https://arxiv.org/pdf/1610.09369v1.pdf
WN18,,# 3,MR,352.0,Gaifman,-,Link Prediction,Discriminative Gaifman Models,/paper/discriminative-gaifman-models,https://arxiv.org/pdf/1610.09369v1.pdf
AG News,,# 16,Error,9.64,Seq2CNN with GWS(50),-,Text Classification,Abstractive Text Classification Using Sequence-to-convolution Neural Networks,/paper/abstractive-text-classification-using,https://arxiv.org/pdf/1805.07745v4.pdf
DBpedia,,# 17,Error,2.77,Seq2CNN(50),-,Text Classification,Abstractive Text Classification Using Sequence-to-convolution Neural Networks,/paper/abstractive-text-classification-using,https://arxiv.org/pdf/1805.07745v4.pdf
Yahoo! Answers,,# 6,Accuracy,55.39,Seq2CNN(50),-,Text Classification,Abstractive Text Classification Using Sequence-to-convolution Neural Networks,/paper/abstractive-text-classification-using,https://arxiv.org/pdf/1805.07745v4.pdf
Florence,,# 1,Average 3D Error,0.95,GANFit,-,3D Face Reconstruction,GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction,/paper/ganfit-generative-adversarial-network-fitting,https://arxiv.org/pdf/1902.05978v2.pdf
CIFAR-10,,# 55,Percentage correct,82.0,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,-,Image Classification,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,/paper/discriminative-unsupervised-feature-learning-1,https://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf
STL-10,,# 7,Percentage correct,72.8,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,-,Image Classification,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,/paper/discriminative-unsupervised-feature-learning-1,https://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf
Annotated Faces in the Wild,,# 8,AP,0.9721,DPM,-,Face Detection,A Fast and Accurate Unconstrained Face Detector,/paper/a-fast-and-accurate-unconstrained-face,https://arxiv.org/pdf/1408.1656v3.pdf
FDDB,,# 8,AP,0.8640000000000001,DPM,-,Face Detection,A Fast and Accurate Unconstrained Face Detector,/paper/a-fast-and-accurate-unconstrained-face,https://arxiv.org/pdf/1408.1656v3.pdf
PASCAL Face,,# 7,AP,0.9029,DPM,-,Face Detection,A Fast and Accurate Unconstrained Face Detector,/paper/a-fast-and-accurate-unconstrained-face,https://arxiv.org/pdf/1408.1656v3.pdf
IJB-C,,# 3,TAR @ FAR=0.01,66.50%,FaceNet,-,Face Verification,FaceNet: A Unified Embedding for Face Recognition and Clustering,/paper/facenet-a-unified-embedding-for-face,https://arxiv.org/pdf/1503.03832v3.pdf
Labeled Faces in the Wild,,# 3,Accuracy,99.63%,FaceNet,-,Face Verification,FaceNet: A Unified Embedding for Face Recognition and Clustering,/paper/facenet-a-unified-embedding-for-face,https://arxiv.org/pdf/1503.03832v3.pdf
MegaFace,,# 4,Accuracy,86.47%,FaceNet,-,Face Verification,FaceNet: A Unified Embedding for Face Recognition and Clustering,/paper/facenet-a-unified-embedding-for-face,https://arxiv.org/pdf/1503.03832v3.pdf
MegaFace,,# 6,Accuracy,70.49%,FaceNet,-,Face Identification,FaceNet: A Unified Embedding for Face Recognition and Clustering,/paper/facenet-a-unified-embedding-for-face,https://arxiv.org/pdf/1503.03832v3.pdf
YouTube Faces DB,,# 7,Accuracy,95.12%,FaceNet,-,Face Verification,FaceNet: A Unified Embedding for Face Recognition and Clustering,/paper/facenet-a-unified-embedding-for-face,https://arxiv.org/pdf/1503.03832v3.pdf
Event2Mind,,# 1,Dev,4.25,BiRNN 100d,-,Common Sense Reasoning,"Event2Mind: Commonsense Inference on Events, Intents, and Reactions",/paper/event2mind-commonsense-inference-on-events,https://arxiv.org/pdf/1805.06939v1.pdf
Event2Mind,,# 1,Test,4.22,BiRNN 100d,-,Common Sense Reasoning,"Event2Mind: Commonsense Inference on Events, Intents, and Reactions",/paper/event2mind-commonsense-inference-on-events,https://arxiv.org/pdf/1805.06939v1.pdf
Event2Mind,,# 2,Dev,4.44,ConvNet,-,Common Sense Reasoning,"Event2Mind: Commonsense Inference on Events, Intents, and Reactions",/paper/event2mind-commonsense-inference-on-events,https://arxiv.org/pdf/1805.06939v1.pdf
Event2Mind,,# 2,Test,4.4,ConvNet,-,Common Sense Reasoning,"Event2Mind: Commonsense Inference on Events, Intents, and Reactions",/paper/event2mind-commonsense-inference-on-events,https://arxiv.org/pdf/1805.06939v1.pdf
CUB-200 - 0-Shot Learning,,# 2,Accuracy,50.1%,SJE,-,Few-Shot Image Classification,Evaluation of Output Embeddings for Fine-Grained Image Classification,/paper/evaluation-of-output-embeddings-for-fine,https://arxiv.org/pdf/1409.8403v2.pdf
Data3DâR2N2,,# 2,Avg F1,59.72,Pixel2Mesh,-,3D Object Reconstruction,Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images,/paper/pixel2mesh-generating-3d-mesh-models-from,https://arxiv.org/pdf/1804.01654v2.pdf
RVL-CDIP,,# 1,Accuracy,92.21%,Transfer Learning from VGG16 trained on Imagenet,-,Document Image Classification,Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks,/paper/document-image-classification-with-intra,https://arxiv.org/pdf/1801.09321v3.pdf
Penn Treebank (Word Level),,# 14,Validation perplexity,58.9,AWD-LSTM 3-layer with Fraternal dropout,-,Language Modelling,Fraternal Dropout,/paper/fraternal-dropout,https://arxiv.org/pdf/1711.00066v4.pdf
Penn Treebank (Word Level),,# 16,Test perplexity,56.8,AWD-LSTM 3-layer with Fraternal dropout,-,Language Modelling,Fraternal Dropout,/paper/fraternal-dropout,https://arxiv.org/pdf/1711.00066v4.pdf
Penn Treebank (Word Level),,# 1,Params,24M,AWD-LSTM 3-layer with Fraternal dropout,-,Language Modelling,Fraternal Dropout,/paper/fraternal-dropout,https://arxiv.org/pdf/1711.00066v4.pdf
WikiText-2,,# 11,Validation perplexity,66.8,AWD-LSTM 3-layer with Fraternal dropout,-,Language Modelling,Fraternal Dropout,/paper/fraternal-dropout,https://arxiv.org/pdf/1711.00066v4.pdf
WikiText-2,,# 12,Test perplexity,64.1,AWD-LSTM 3-layer with Fraternal dropout,-,Language Modelling,Fraternal Dropout,/paper/fraternal-dropout,https://arxiv.org/pdf/1711.00066v4.pdf
WikiText-2,,# 1,Number of params,34M,AWD-LSTM 3-layer with Fraternal dropout,-,Language Modelling,Fraternal Dropout,/paper/fraternal-dropout,https://arxiv.org/pdf/1711.00066v4.pdf
KITTI Cars Easy,,# 2,AP,84.32%,PointRCNN,-,3D Object Detection,PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud,/paper/pointrcnn-3d-object-proposal-generation-and,https://arxiv.org/pdf/1812.04244v1.pdf
KITTI Cars Hard,,# 1,AP,67.86%,PointRCNN,-,3D Object Detection,PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud,/paper/pointrcnn-3d-object-proposal-generation-and,https://arxiv.org/pdf/1812.04244v1.pdf
KITTI Cars Moderate,,# 1,AP,75.42%,PointRCNN,-,3D Object Detection,PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud,/paper/pointrcnn-3d-object-proposal-generation-and,https://arxiv.org/pdf/1812.04244v1.pdf
Set14 - 4x upscaling,,# 30,PSNR,27.57,SPMC,-,Image Super-Resolution,Detail-revealing Deep Video Super-resolution,/paper/detail-revealing-deep-video-super-resolution,https://arxiv.org/pdf/1704.02738v1.pdf
Set14 - 4x upscaling,,# 27,SSIM,0.76,SPMC,-,Image Super-Resolution,Detail-revealing Deep Video Super-resolution,/paper/detail-revealing-deep-video-super-resolution,https://arxiv.org/pdf/1704.02738v1.pdf
Set5 - 4x upscaling,,# 23,PSNR,30.96,SPMC,-,Image Super-Resolution,Detail-revealing Deep Video Super-resolution,/paper/detail-revealing-deep-video-super-resolution,https://arxiv.org/pdf/1704.02738v1.pdf
Set5 - 4x upscaling,,# 26,SSIM,0.87,SPMC,-,Image Super-Resolution,Detail-revealing Deep Video Super-resolution,/paper/detail-revealing-deep-video-super-resolution,https://arxiv.org/pdf/1704.02738v1.pdf
Vid4 - 4x upscaling,,# 5,PSNR,25.88,DRDVSR,-,Video Super-Resolution,Detail-revealing Deep Video Super-resolution,/paper/detail-revealing-deep-video-super-resolution,https://arxiv.org/pdf/1704.02738v1.pdf
Vid4 - 4x upscaling,,# 8,SSIM,0.774,DRDVSR,-,Video Super-Resolution,Detail-revealing Deep Video Super-resolution,/paper/detail-revealing-deep-video-super-resolution,https://arxiv.org/pdf/1704.02738v1.pdf
MultiNLI,,# 1,Matched,86.7,MT-DNN,-,Natural Language Inference,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
MultiNLI,,# 1,Mismatched,86.0,MT-DNN,-,Natural Language Inference,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
Quora Question Pairs,,# 1,Accuracy,89.6,MT-DNN,-,Paraphrase Identification,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
SciTail,,# 1,Accuracy,94.1,MT-DNN,-,Natural Language Inference,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
SNLI,,# 2,% Test Accuracy,91.1,MT-DNN,-,Natural Language Inference,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
SNLI,,# 3,% Train Accuracy,96.8,MT-DNN,-,Natural Language Inference,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
SNLI,,# 1,Parameters,110m,MT-DNN,-,Natural Language Inference,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
SST-2 Binary classification,,# 1,Accuracy,95.6,MT-DNN,-,Sentiment Analysis,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural,https://arxiv.org/pdf/1901.11504v1.pdf
LineMOD,,# 5,Accuracy,70.2%,PoseCNN,-,6D Pose Estimation,PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes,/paper/posecnn-a-convolutional-neural-network-for-6d,https://arxiv.org/pdf/1711.00199v3.pdf
OccludedLINEMOD,,# 1,Accuracy,78.0%,PoseCNN + ICP,-,6D Pose Estimation,PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes,/paper/posecnn-a-convolutional-neural-network-for-6d,https://arxiv.org/pdf/1711.00199v3.pdf
YCB-Video,,# 2,Mean AUC,93.0%,PoseCNN + ICP,-,6D Pose Estimation,PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes,/paper/posecnn-a-convolutional-neural-network-for-6d,https://arxiv.org/pdf/1711.00199v3.pdf
Cityscapes,,# 10,Mean IoU,71.8%,FRRN,-,Semantic Segmentation,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,/paper/full-resolution-residual-networks-for,https://arxiv.org/pdf/1611.08323v2.pdf
Cityscapes,,# 4,mIoU,71.8%,FRRN,-,Real-Time Semantic Segmentation,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,/paper/full-resolution-residual-networks-for,https://arxiv.org/pdf/1611.08323v2.pdf
Cityscapes,,# 4,Time (ms),469,FRRN,-,Real-Time Semantic Segmentation,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,/paper/full-resolution-residual-networks-for,https://arxiv.org/pdf/1611.08323v2.pdf
Cityscapes,,# 6,Frame (fps),2.1,FRRN,-,Real-Time Semantic Segmentation,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,/paper/full-resolution-residual-networks-for,https://arxiv.org/pdf/1611.08323v2.pdf
SST-2 Binary classification,,# 18,Accuracy,85.4,RNTN,-,Sentiment Analysis,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,/paper/recursive-deep-models-for-semantic,https://aclweb.org/anthology/D13-1170
SST-5 Fine-grained classification,,# 13,Accuracy,45.7,RNTN,-,Sentiment Analysis,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,/paper/recursive-deep-models-for-semantic,https://aclweb.org/anthology/D13-1170
Set5-2x,,# 1,PSNR,37.82,FALSR-A,-,Super Resolution,"Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search",/paper/fast-accurate-and-lightweight-super-2,https://arxiv.org/pdf/1901.07261.pdf
DUC 2004 Task 1,,# 7,ROUGE-1,28.61,words-lvt5k-1sent,-,Text Summarization,Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,/paper/abstractive-text-summarization-using-sequence,https://arxiv.org/pdf/1602.06023v5.pdf
DUC 2004 Task 1,,# 6,ROUGE-2,9.42,words-lvt5k-1sent,-,Text Summarization,Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,/paper/abstractive-text-summarization-using-sequence,https://arxiv.org/pdf/1602.06023v5.pdf
DUC 2004 Task 1,,# 5,ROUGE-L,25.24,words-lvt5k-1sent,-,Text Summarization,Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,/paper/abstractive-text-summarization-using-sequence,https://arxiv.org/pdf/1602.06023v5.pdf
GigaWord,,# 3,ROUGE-1,36.4,words-lvt5k-1sent,-,Text Summarization,Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,/paper/abstractive-text-summarization-using-sequence,https://arxiv.org/pdf/1602.06023v5.pdf
GigaWord,,# 4,ROUGE-2,17.7,words-lvt5k-1sent,-,Text Summarization,Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,/paper/abstractive-text-summarization-using-sequence,https://arxiv.org/pdf/1602.06023v5.pdf
GigaWord,,# 6,ROUGE-L,33.71,words-lvt5k-1sent,-,Text Summarization,Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,/paper/abstractive-text-summarization-using-sequence,https://arxiv.org/pdf/1602.06023v5.pdf
RWTH-PHOENIX-Weather 2014,,# 1,Word Error Rate (WER),40.7,SubUNets,-,Sign Language Recognition,SubUNets: End-To-End Hand Shape and Continuous Sign Language Recognition,/paper/subunets-end-to-end-hand-shape-and-continuous,https://openaccess.thecvf.com/content_ICCV_2017/papers/Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper.pdf
PanoContext,,# 1,3DIoU,82.17%,HorizonNet,-,3D Room Layouts From A Single Rgb Panorama,HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation,/paper/horizonnet-learning-room-layout-with-1d,https://arxiv.org/pdf/1901.03861v2.pdf
Stanford 2D-3D,,# 1,3DIoU,79.79%,HorizonNet,-,3D Room Layouts From A Single Rgb Panorama,HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation,/paper/horizonnet-learning-room-layout-with-1d,https://arxiv.org/pdf/1901.03861v2.pdf
Children's Book Test,,# 4,Accuracy-CN,69.4%,AoA reader,-,Question Answering,Attention-over-Attention Neural Networks for Reading Comprehension,/paper/attention-over-attention-neural-networks-for,https://arxiv.org/pdf/1607.04423v4.pdf
Children's Book Test,,# 4,Accuracy-NE,72%,AoA reader,-,Question Answering,Attention-over-Attention Neural Networks for Reading Comprehension,/paper/attention-over-attention-neural-networks-for,https://arxiv.org/pdf/1607.04423v4.pdf
CNN / Daily Mail,,# 8,CNN,74.4,AoA Reader,-,Question Answering,Attention-over-Attention Neural Networks for Reading Comprehension,/paper/attention-over-attention-neural-networks-for,https://arxiv.org/pdf/1607.04423v4.pdf
MCTest-160,,# 1,Accuracy,75.27%,"syntax, frame, coreference, and word embedding features",-,Question Answering,A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,/paper/a-parallel-hierarchical-model-for-machine,https://arxiv.org/pdf/1603.08884v1.pdf
MCTest-500,,# 2,Accuracy,69.94%,"syntax, frame, coreference, and word embedding features",-,Question Answering,A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,/paper/a-parallel-hierarchical-model-for-machine,https://arxiv.org/pdf/1603.08884v1.pdf
MCTest-500,,# 1,Accuracy,71%,Parallel-Hierarchical,-,Question Answering,A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,/paper/a-parallel-hierarchical-model-for-machine,https://arxiv.org/pdf/1603.08884v1.pdf
IC15,,# 6,F-Measure,82.54%,R2CNN,-,Scene Text Detection,R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection,/paper/r2cnn-rotational-region-cnn-for-orientation,https://arxiv.org/pdf/1706.09579v2.pdf
Annotated Faces in the Wild,,# 4,AP,0.994,HyperFace-ResNet,-,Face Detection,"HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition",/paper/hyperface-a-deep-multi-task-learning,https://arxiv.org/pdf/1603.01249v3.pdf
FDDB,,# 7,AP,0.9009999999999999,HyperFace,-,Face Detection,"HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition",/paper/hyperface-a-deep-multi-task-learning,https://arxiv.org/pdf/1603.01249v3.pdf
PASCAL Face,,# 5,AP,0.962,HyperFace-ResNet,-,Face Detection,"HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition",/paper/hyperface-a-deep-multi-task-learning,https://arxiv.org/pdf/1603.01249v3.pdf
BSD68 sigma10,,# 1,PSNR,36.49,Residual Dense Network +,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
BSD68 sigma30,,# 1,PSNR,30.7,Residual Dense Network +,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
BSD68 sigma50,,# 3,PSNR,26.43,RDN+,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
BSD68 sigma70,,# 1,PSNR,26.88,Residual Dense Network +,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
Urban100 sigma30,,# 1,PSNR,31.78,Residual Dense Network +,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
Urban100 sigma50,,# 1,PSNR,29.38,Residual Dense Network +,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
Urban100 sigma70,,# 1,PSNR,27.74,Residual Dense Network +,-,Image Denoising,Residual Dense Network for Image Restoration,/paper/residual-dense-network-for-image-restoration,https://arxiv.org/pdf/1812.10477v1.pdf
IWSLT2015 German-English,,# 13,BLEU score,28.53,Bi-GRU (MLE+SLE),-,Machine Translation,Neural Machine Translation by Jointly Learning to Align and Translate,/paper/neural-machine-translation-by-jointly,https://arxiv.org/pdf/1409.0473v7.pdf
WMT2014 English-French,,# 18,BLEU score,36.15,RNN-search50*,-,Machine Translation,Neural Machine Translation by Jointly Learning to Align and Translate,/paper/neural-machine-translation-by-jointly,https://arxiv.org/pdf/1409.0473v7.pdf
QM9,,# 2,Error ratio,1.36,Gated Graph Sequence NN,-,Drug Discovery,Gated Graph Sequence Neural Networks,/paper/gated-graph-sequence-neural-networks,https://arxiv.org/pdf/1511.05493v4.pdf
WikiSQL,,# 2,BLEU-4,35.53,GGS-NN,-,SQL-to-Text,Gated Graph Sequence Neural Networks,/paper/gated-graph-sequence-neural-networks,https://arxiv.org/pdf/1511.05493v4.pdf
QM9,,# 3,Error ratio,2.59,Molecular Graph Convolutions,-,Drug Discovery,Molecular Graph Convolutions: Moving Beyond Fingerprints,/paper/molecular-graph-convolutions-moving-beyond,https://arxiv.org/pdf/1603.00856v3.pdf
IC15,,# 9,F-Measure,76.91%,SSTD,-,Scene Text Detection,Single Shot Text Detector with Regional Attention,/paper/single-shot-text-detector-with-regional,https://arxiv.org/pdf/1709.00138v1.pdf
CIFAR-10,,# 57,Percentage correct,79.7,Learning with Recursive Perceptual Representations,-,Image Classification,Learning with Recursive Perceptual Representations,/paper/learning-with-recursive-perceptual,https://papers.nips.cc/paper/4747-learning-with-recursive-perceptual-representations.pdf
Charades,,# 2,MAP,41.1,Timeception,-,Action Recognition In Videos,Timeception for Complex Action Recognition,/paper/timeception-for-complex-action-recognition,https://arxiv.org/pdf/1812.01289v2.pdf
CIFAR-10,,# 8,Percentage correct,96.62,CoPaNet-R-164,-,Image Classification,Deep Competitive Pathway Networks,/paper/deep-competitive-pathway-networks,https://arxiv.org/pdf/1709.10282v1.pdf
CIFAR-10,,# 8,Percentage error,3.38,CoPaNet-R-164,-,Image Classification,Deep Competitive Pathway Networks,/paper/deep-competitive-pathway-networks,https://arxiv.org/pdf/1709.10282v1.pdf
CIFAR-100,,# 9,Percentage correct,81.1,CoPaNet-R-164,-,Image Classification,Deep Competitive Pathway Networks,/paper/deep-competitive-pathway-networks,https://arxiv.org/pdf/1709.10282v1.pdf
CIFAR-100,,# 5,Percentage error,18.9,CoPaNet-R-164,-,Image Classification,Deep Competitive Pathway Networks,/paper/deep-competitive-pathway-networks,https://arxiv.org/pdf/1709.10282v1.pdf
SVHN,,# 3,Percentage error,1.58,CoPaNet-R-164,-,Image Classification,Deep Competitive Pathway Networks,/paper/deep-competitive-pathway-networks,https://arxiv.org/pdf/1709.10282v1.pdf
DukeMTMC-reID,,# 18,Rank-1,30.75,LOMO + XQDA,-,Person Re-Identification,Person Re-identification by Local Maximal Occurrence Representation and Metric Learning,/paper/person-re-identification-by-local-maximal,https://arxiv.org/pdf/1406.4216v2.pdf
DukeMTMC-reID,,# 18,MAP,17.04,LOMO + XQDA,-,Person Re-Identification,Person Re-identification by Local Maximal Occurrence Representation and Metric Learning,/paper/person-re-identification-by-local-maximal,https://arxiv.org/pdf/1406.4216v2.pdf
Market-1501,,# 24,Rank-1,43.79,LOMO + XQDA,-,Person Re-Identification,Person Re-identification by Local Maximal Occurrence Representation and Metric Learning,/paper/person-re-identification-by-local-maximal,https://arxiv.org/pdf/1406.4216v2.pdf
Market-1501,,# 24,MAP,22.22,LOMO + XQDA,-,Person Re-Identification,Person Re-identification by Local Maximal Occurrence Representation and Metric Learning,/paper/person-re-identification-by-local-maximal,https://arxiv.org/pdf/1406.4216v2.pdf
PASCAL VOC 2007,,# 8,MAP,46.1,VGPMIL,-,Weakly Supervised Object Detection,Variational Bayesian Multiple Instance Learning With Gaussian Processes,/paper/variational-bayesian-multiple-instance,https://openaccess.thecvf.com/content_cvpr_2017/papers/Haussmann_Variational_Bayesian_Multiple_CVPR_2017_paper.pdf
PASCAL VOC 2012,,# 9,MAP,37.8,LM-VGPMIL,-,Weakly Supervised Object Detection,Variational Bayesian Multiple Instance Learning With Gaussian Processes,/paper/variational-bayesian-multiple-instance,https://openaccess.thecvf.com/content_cvpr_2017/papers/Haussmann_Variational_Bayesian_Multiple_CVPR_2017_paper.pdf
COCO Visual Question Answering (VQA) real images 2.0 open ended,,# 4,Percentage correct,62.27,MCB,-,Visual Question Answering,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,/paper/making-the-v-in-vqa-matter-elevating-the-role,https://arxiv.org/pdf/1612.00837v3.pdf
COCO Visual Question Answering (VQA) real images 2.0 open ended,,# 5,Percentage correct,54.22,d-LSTM+nI,-,Visual Question Answering,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,/paper/making-the-v-in-vqa-matter-elevating-the-role,https://arxiv.org/pdf/1612.00837v3.pdf
VQA v2,,# 4,Accuracy,62.27%,MCB,-,Visual Question Answering,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,/paper/making-the-v-in-vqa-matter-elevating-the-role,https://arxiv.org/pdf/1612.00837v3.pdf
VQA v2,,# 5,Accuracy,54.22%,Deeper LSTM Q Norm,-,Visual Question Answering,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,/paper/making-the-v-in-vqa-matter-elevating-the-role,https://arxiv.org/pdf/1612.00837v3.pdf
VQA v2,,# 6,Accuracy,44.26%,LSTM Language Model only (blind model),-,Visual Question Answering,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,/paper/making-the-v-in-vqa-matter-elevating-the-role,https://arxiv.org/pdf/1612.00837v3.pdf
VQA v2,,# 7,Accuracy,25.98%,Prior (most common answer in training set),-,Visual Question Answering,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,/paper/making-the-v-in-vqa-matter-elevating-the-role,https://arxiv.org/pdf/1612.00837v3.pdf
SQuAD1.1,,# 97,EM,71.625,Dynamic Coattention Networks (ensemble),-,Question Answering,Dynamic Coattention Networks For Question Answering,/paper/dynamic-coattention-networks-for-question,https://arxiv.org/pdf/1611.01604v4.pdf
SQuAD1.1,,# 97,F1,80.383,Dynamic Coattention Networks (ensemble),-,Question Answering,Dynamic Coattention Networks For Question Answering,/paper/dynamic-coattention-networks-for-question,https://arxiv.org/pdf/1611.01604v4.pdf
SQuAD1.1,,# 124,EM,66.233,Dynamic Coattention Networks (single model),-,Question Answering,Dynamic Coattention Networks For Question Answering,/paper/dynamic-coattention-networks-for-question,https://arxiv.org/pdf/1611.01604v4.pdf
SQuAD1.1,,# 125,F1,75.896,Dynamic Coattention Networks (single model),-,Question Answering,Dynamic Coattention Networks For Question Answering,/paper/dynamic-coattention-networks-for-question,https://arxiv.org/pdf/1611.01604v4.pdf
CoNLL 2003 (English),,# 1,F1,93.5,CNN Large + fine-tune,-,Named Entity Recognition (NER),Cloze-driven Pretraining of Self-attention Networks,/paper/cloze-driven-pretraining-of-self-attention,https://arxiv.org/pdf/1903.07785v1.pdf
Penn Treebank,,# 1,F1 score,95.6,CNN Large + fine-tune,-,Constituency Parsing,Cloze-driven Pretraining of Self-attention Networks,/paper/cloze-driven-pretraining-of-self-attention,https://arxiv.org/pdf/1903.07785v1.pdf
SST-2 Binary classification,,# 2,Accuracy,94.6,CNN Large,-,Sentiment Analysis,Cloze-driven Pretraining of Self-attention Networks,/paper/cloze-driven-pretraining-of-self-attention,https://arxiv.org/pdf/1903.07785v1.pdf
CIFAR-10,,# 44,Percentage correct,89.7,APAC,-,Image Classification,APAC: Augmented PAttern Classification with Neural Networks,/paper/apac-augmented-pattern-classification-with,https://arxiv.org/pdf/1505.03229v1.pdf
MNIST,,# 2,Percentage error,0.2,APAC,-,Image Classification,APAC: Augmented PAttern Classification with Neural Networks,/paper/apac-augmented-pattern-classification-with,https://arxiv.org/pdf/1505.03229v1.pdf
SNLI,,# 27,% Test Accuracy,86.1,300D mLSTM word-by-word attention model,-,Natural Language Inference,Learning Natural Language Inference with LSTM,/paper/learning-natural-language-inference-with-lstm,https://arxiv.org/pdf/1512.08849v2.pdf
SNLI,,# 23,% Train Accuracy,92.0,300D mLSTM word-by-word attention model,-,Natural Language Inference,Learning Natural Language Inference with LSTM,/paper/learning-natural-language-inference-with-lstm,https://arxiv.org/pdf/1512.08849v2.pdf
SNLI,,# 1,Parameters,1.9m,300D mLSTM word-by-word attention model,-,Natural Language Inference,Learning Natural Language Inference with LSTM,/paper/learning-natural-language-inference-with-lstm,https://arxiv.org/pdf/1512.08849v2.pdf
PanoContext,,# 3,3DIoU,77.42%,DuLa-Net,-,3D Room Layouts From A Single Rgb Panorama,DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama,/paper/dula-net-a-dual-projection-network-for,https://arxiv.org/pdf/1811.11977v2.pdf
Realtor360,,# 1,3DIoU,77.20%,DuLa-Net,-,3D Room Layouts From A Single Rgb Panorama,DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama,/paper/dula-net-a-dual-projection-network-for,https://arxiv.org/pdf/1811.11977v2.pdf
Stanford 2D-3D,,# 2,3DIoU,79.36%,DuLa-Net,-,3D Room Layouts From A Single Rgb Panorama,DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama,/paper/dula-net-a-dual-projection-network-for,https://arxiv.org/pdf/1811.11977v2.pdf
IJB-A,,# 10,TAR @ FAR=0.01,88.60%,Synthesis as data augmentation,-,Face Verification,Do We Really Need to Collect Millions of Faces for Effective Face Recognition?,/paper/do-we-really-need-to-collect-millions-of,https://arxiv.org/pdf/1603.07057v2.pdf
SLAM 2018,,# 1,AUC,0.821,Context Based Model,-,Language Acquisition,Context Based Approach for Second Language Acquisition,/paper/context-based-approach-for-second-language,https://aclweb.org/anthology/W18-0524
ADE20K,,# 4,Validation mIoU,40.7,RefineNet,-,Semantic Segmentation,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,/paper/refinenet-multi-path-refinement-networks-for,https://arxiv.org/pdf/1611.06612v3.pdf
PASCAL Context,,# 5,mIoU,47.3,RefineNet,-,Semantic Segmentation,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,/paper/refinenet-multi-path-refinement-networks-for,https://arxiv.org/pdf/1611.06612v3.pdf
PASCAL VOC 2012,,# 7,Mean IoU,84.2%,Multipath-RefineNet,-,Semantic Segmentation,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,/paper/refinenet-multi-path-refinement-networks-for,https://arxiv.org/pdf/1611.06612v3.pdf
COCO,,# 14,Bounding Box AP,43.2,CornerNet-Saccade,-,Object Detection,CornerNet-Lite: Efficient Keypoint Based Object Detection,/paper/190408900,https://arxiv.org/pdf/1904.08900v1.pdf
COCO,,# 3,MAP,34.4%,CornerNet-Squeeze,-,Real-Time Object Detection,CornerNet-Lite: Efficient Keypoint Based Object Detection,/paper/190408900,https://arxiv.org/pdf/1904.08900v1.pdf
COCO,,# 4,FPS,30,CornerNet-Squeeze,-,Real-Time Object Detection,CornerNet-Lite: Efficient Keypoint Based Object Detection,/paper/190408900,https://arxiv.org/pdf/1904.08900v1.pdf
COCO,,# 32,Bounding Box AP,34.4,CornerNet-Squeeze,-,Object Detection,CornerNet-Lite: Efficient Keypoint Based Object Detection,/paper/190408900,https://arxiv.org/pdf/1904.08900v1.pdf
Amazon,,# 1,AUC,0.8871,DIN + Dice Activation,-,Click-Through Rate Prediction,Deep Interest Network for Click-Through Rate Prediction,/paper/deep-interest-network-for-click-through-rate,https://arxiv.org/pdf/1706.06978v4.pdf
Amazon,,# 2,AUC,0.8818,DIN,-,Click-Through Rate Prediction,Deep Interest Network for Click-Through Rate Prediction,/paper/deep-interest-network-for-click-through-rate,https://arxiv.org/pdf/1706.06978v4.pdf
MovieLens 20M,,# 2,AUC,0.7337,DIN,-,Click-Through Rate Prediction,Deep Interest Network for Click-Through Rate Prediction,/paper/deep-interest-network-for-click-through-rate,https://arxiv.org/pdf/1706.06978v4.pdf
MovieLens 20M,,# 1,AUC,0.7348,DIN + Dice Activation,-,Click-Through Rate Prediction,Deep Interest Network for Click-Through Rate Prediction,/paper/deep-interest-network-for-click-through-rate,https://arxiv.org/pdf/1706.06978v4.pdf
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 5,Percentage correct,64.2,FDA,-,Visual Question Answering,A Focused Dynamic Attention Model for Visual Question Answering,/paper/a-focused-dynamic-attention-model-for-visual,https://arxiv.org/pdf/1604.01485v1.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 6,Percentage correct,59.5,FDA,-,Visual Question Answering,A Focused Dynamic Attention Model for Visual Question Answering,/paper/a-focused-dynamic-attention-model-for-visual,https://arxiv.org/pdf/1604.01485v1.pdf
ACL-ARC,,# 2,F1,65.8,SciBERT,-,Citation Intent Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
ACL-ARC,,# 3,F1,65.71,SciBERT (SciVocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
ACL-ARC,,# 2,F1,65.79,SciBERT (Base Vocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
BC5CDR,,# 2,F1,88.11,SciBERT (Base Vocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
BC5CDR,,# 1,F1,88.94,SciBERT (SciVocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
ChemProt,,# 2,F1,73.7,SciBERT (Base Vocab),-,Relation Extraction,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
ChemProt,,# 1,F1,76.12,SciBERT (SciVocab),-,Relation Extraction,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
EBM-NLP,,# 1,F1,71.18,SciBERT (SciVocab),-,Participant Intervention Comparison Outcome Extraction,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
EBM-NLP,,# 2,F1,70.82,SciBERT (Base Vocab),-,Participant Intervention Comparison Outcome Extraction,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
GENIA - LAS,,# 2,F1,91.41,SciBERT (SciVocab),-,Dependency Parsing,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
GENIA - LAS,,# 3,F1,91.26,SciBERT (Base Vocab),-,Dependency Parsing,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
GENIA - UAS,,# 3,F1,92.32,SciBERT (Base Vocab),-,Dependency Parsing,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
GENIA - UAS,,# 2,F1,92.46,SciBERT (SciVocab),-,Dependency Parsing,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
JNLPBA,,# 3,F1,75.83,SciBERT (Base Vocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
JNLPBA,,# 2,F1,75.95,SciBERT (SciVocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
NCBI-disease,,# 2,F1,86.45,SciBERT (SciVocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
NCBI-disease,,# 1,F1,86.91,SciBERT (Base Vocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
Paper Field,,# 2,F1,64.02,SciBERT (Base Vocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
Paper Field,,# 1,F1,64.07,SciBERT (SciVocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
PubMed 20k RCT,,# 2,F1,86.81,SciBERT (SciVocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
PubMed 20k RCT,,# 3,F1,86.8,SciBERT (Base Vocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SciCite,,# 1,F1,84.9,SciBERT,-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SciCite,,# 1,F1,84.99,SciBERT,-,Citation Intent Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
ScienceCite,,# 1,F1,84.99,SciBERT (SciVocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
ScienceCite,,# 2,F1,84.43,SciBERT (Base Vocab),-,Sentence Classification,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SciERC,,# 1,F1,74.64,SciBERT (SciVocab),-,Relation Extraction,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SciERC,,# 1,F1,65.5,SciBERT (SciVocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SciERC,,# 2,F1,65.12,SciBERT (Base Vocab),-,Named Entity Recognition (NER),SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SciERC,,# 2,F1,74.42,SciBERT (Base Vocab),-,Relation Extraction,SciBERT: Pretrained Contextualized Embeddings for Scientific Text,/paper/scibert-pretrained-contextualized-embeddings,https://arxiv.org/pdf/1903.10676v1.pdf
SNLI,,# 31,% Test Accuracy,85.6,300D Directional self-attention network encoders,-,Natural Language Inference,DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding,/paper/disan-directional-self-attention-network-for,https://arxiv.org/pdf/1709.04696v3.pdf
SNLI,,# 27,% Train Accuracy,91.1,300D Directional self-attention network encoders,-,Natural Language Inference,DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding,/paper/disan-directional-self-attention-network-for,https://arxiv.org/pdf/1709.04696v3.pdf
SNLI,,# 1,Parameters,2.4m,300D Directional self-attention network encoders,-,Natural Language Inference,DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding,/paper/disan-directional-self-attention-network-for,https://arxiv.org/pdf/1709.04696v3.pdf
WebQuestions,,# 2,F1,39.2%,Subgraph embeddings,-,Question Answering,Question Answering with Subgraph Embeddings,/paper/question-answering-with-subgraph-embeddings,https://arxiv.org/pdf/1406.3676v3.pdf
Quora Question Pairs,,# 5,Accuracy,88.17,BiMPM,-,Paraphrase Identification,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
SNLI,,# 17,% Test Accuracy,87.5,BiMPM,-,Natural Language Inference,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
SNLI,,# 29,% Train Accuracy,90.9,BiMPM,-,Natural Language Inference,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
SNLI,,# 1,Parameters,1.6m,BiMPM,-,Natural Language Inference,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
SNLI,,# 10,% Test Accuracy,88.8,BiMPM Ensemble,-,Natural Language Inference,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
SNLI,,# 17,% Train Accuracy,93.2,BiMPM Ensemble,-,Natural Language Inference,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
SNLI,,# 1,Parameters,6.4m,BiMPM Ensemble,-,Natural Language Inference,Bilateral Multi-Perspective Matching for Natural Language Sentences,/paper/bilateral-multi-perspective-matching-for,https://arxiv.org/pdf/1702.03814v3.pdf
CIFAR-10,,# 4,Inception score,7.96,MSG-GAN,-,Image Generation,MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis,/paper/msg-gan-multi-scale-gradients-gan-for-more,https://arxiv.org/pdf/1903.06048v2.pdf
ACL-ARC,,# 3,F1,54.6,BiLSTM-Attention + ELMo,-,Citation Intent Classification,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
CoNLL 2003 (English),,# 7,F1,92.22,BiLSTM-CRF+ELMo,-,Named Entity Recognition (NER),Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
CoNLL 2012,,# 2,Avg F1,70.4,"(Lee et al., 2017)+ELMo",-,Coreference Resolution,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
OntoNotes,,# 5,F1,84.6,"(He et al., 2017) + ELMo",-,Semantic Role Labeling,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 11,% Test Accuracy,88.7,ESIM + ELMo,-,Natural Language Inference,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 24,% Train Accuracy,91.6,ESIM + ELMo,-,Natural Language Inference,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 1,Parameters,8.0m,ESIM + ELMo,-,Natural Language Inference,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 7,% Test Accuracy,89.3,ESIM + ELMo Ensemble,-,Natural Language Inference,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 22,% Train Accuracy,92.1,ESIM + ELMo Ensemble,-,Natural Language Inference,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 1,Parameters,40m,ESIM + ELMo Ensemble,-,Natural Language Inference,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SQuAD1.1,,# 22,EM,81.003,BiDAF + Self Attention + ELMo (ensemble),-,Question Answering,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SQuAD1.1,,# 24,F1,87.432,BiDAF + Self Attention + ELMo (ensemble),-,Question Answering,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SQuAD1.1,,# 41,EM,78.580,BiDAF + Self Attention + ELMo (single model),-,Question Answering,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SQuAD1.1,,# 41,F1,85.833,BiDAF + Self Attention + ELMo (single model),-,Question Answering,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SQuAD2.0,,# 82,EM,63.372,BiDAF + Self Attention + ELMo (single model),-,Question Answering,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SQuAD2.0,,# 88,F1,66.251,BiDAF + Self Attention + ELMo (single model),-,Question Answering,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SST-5 Fine-grained classification,,# 3,Accuracy,54.7,BCN+ELMo,-,Sentiment Analysis,Deep contextualized word representations,/paper/deep-contextualized-word-representations,https://arxiv.org/pdf/1802.05365v2.pdf
SNLI,,# 22,% Test Accuracy,86.73,CBS-1 + ESIM,-,Natural Language Inference,Parameter Re-Initialization through Cyclical Batch Size Schedules,/paper/parameter-re-initialization-through-cyclical,https://arxiv.org/pdf/1812.01216v1.pdf
Atari 2600 Alien,,# 14,Score,994.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Amidar,,# 21,Score,112.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Assault,,# 19,Score,1673.9,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Asterix,,# 20,Score,1440.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Asteroids,,# 11,Score,1562.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Atlantis,,# 1,Score,1267410.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Bank Heist,,# 20,Score,225.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Battle Zone,,# 17,Score,16600.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Beam Rider,,# 23,Score,744.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Berzerk,,# 15,Score,686.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Bowling,,# 21,Score,30.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Boxing,,# 18,Score,49.8,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Breakout,,# 21,Score,9.5,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Centipede,,# 6,Score,7783.9,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Chopper Command,,# 17,Score,3710.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Crazy Climber,,# 20,Score,26430.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Demon Attack,,# 20,Score,1166.5,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Double Dunk,,# 6,Score,0.2,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Enduro,,# 19,Score,95.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Fishing Derby,,# 20,Score,-49.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Freeway,,# 8,Score,31.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Frostbite,,# 20,Score,370.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Gopher,,# 22,Score,582.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Gravitar,,# 3,Score,805.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Ice Hockey,,# 15,Score,-4.1,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Kangaroo,,# 10,Score,11200.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Krull,,# 7,Score,8647.2,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Name This Game,,# 20,Score,4503.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Pong,,# 1,Score,21.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Private Eye,,# 21,Score,100.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Q*Bert,,# 26,Score,147.5,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 River Raid,,# 19,Score,5009.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Road Runner,,# 20,Score,16590.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Robotank,,# 20,Score,11.9,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Seaquest,,# 19,Score,1390.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Space Invaders,,# 21,Score,678.5,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Star Gunner,,# 19,Score,1470.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Tennis,,# 12,Score,-4.5,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Time Pilot,,# 16,Score,4970.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Tutankham,,# 12,Score,130.3,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Up and Down,,# 4,Score,67974.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Venture,,# 6,Score,760.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Video Pinball,,# 20,Score,22834.8,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Wizard of Wor,,# 16,Score,3480.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Atari 2600 Zaxxon,,# 15,Score,6380.0,ES FF (1 hour) noop,-,Atari Games,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,/paper/evolution-strategies-as-a-scalable,https://arxiv.org/pdf/1703.03864v2.pdf
Labeled Faces in the Wild,,# 2,Accuracy,99.73%,CosFace,-,Face Verification,CosFace: Large Margin Cosine Loss for Deep Face Recognition,/paper/cosface-large-margin-cosine-loss-for-deep,https://arxiv.org/pdf/1801.09414v2.pdf
MegaFace,,# 2,Accuracy,96.65%,CosFace,-,Face Verification,CosFace: Large Margin Cosine Loss for Deep Face Recognition,/paper/cosface-large-margin-cosine-loss-for-deep,https://arxiv.org/pdf/1801.09414v2.pdf
MegaFace,,# 2,Accuracy,82.72%,CosFace,-,Face Identification,CosFace: Large Margin Cosine Loss for Deep Face Recognition,/paper/cosface-large-margin-cosine-loss-for-deep,https://arxiv.org/pdf/1801.09414v2.pdf
YouTube Faces DB,,# 3,Accuracy,97.6%,CosFace,-,Face Verification,CosFace: Large Margin Cosine Loss for Deep Face Recognition,/paper/cosface-large-margin-cosine-loss-for-deep,https://arxiv.org/pdf/1801.09414v2.pdf
SNLI,,# 30,% Test Accuracy,85.7,300D LSTMN with deep attention fusion,-,Natural Language Inference,Long Short-Term Memory-Networks for Machine Reading,/paper/long-short-term-memory-networks-for-machine,https://arxiv.org/pdf/1601.06733v7.pdf
SNLI,,# 40,% Train Accuracy,87.3,300D LSTMN with deep attention fusion,-,Natural Language Inference,Long Short-Term Memory-Networks for Machine Reading,/paper/long-short-term-memory-networks-for-machine,https://arxiv.org/pdf/1601.06733v7.pdf
SNLI,,# 1,Parameters,1.7m,300D LSTMN with deep attention fusion,-,Natural Language Inference,Long Short-Term Memory-Networks for Machine Reading,/paper/long-short-term-memory-networks-for-machine,https://arxiv.org/pdf/1601.06733v7.pdf
SNLI,,# 26,% Test Accuracy,86.3,450D LSTMN with deep attention fusion,-,Natural Language Inference,Long Short-Term Memory-Networks for Machine Reading,/paper/long-short-term-memory-networks-for-machine,https://arxiv.org/pdf/1601.06733v7.pdf
SNLI,,# 39,% Train Accuracy,88.5,450D LSTMN with deep attention fusion,-,Natural Language Inference,Long Short-Term Memory-Networks for Machine Reading,/paper/long-short-term-memory-networks-for-machine,https://arxiv.org/pdf/1601.06733v7.pdf
SNLI,,# 1,Parameters,3.4m,450D LSTMN with deep attention fusion,-,Natural Language Inference,Long Short-Term Memory-Networks for Machine Reading,/paper/long-short-term-memory-networks-for-machine,https://arxiv.org/pdf/1601.06733v7.pdf
Children's Book Test,,# 4,Accuracy-NE,72%,AIA,-,Question Answering,Iterative Alternating Neural Attention for Machine Reading,/paper/iterative-alternating-neural-attention-for,https://arxiv.org/pdf/1606.02245v4.pdf
CNN / Daily Mail,,# 5,CNN,76.1,AIA,-,Question Answering,Iterative Alternating Neural Attention for Machine Reading,/paper/iterative-alternating-neural-attention-for,https://arxiv.org/pdf/1606.02245v4.pdf
COCO Visual Question Answering (VQA) real images 1.0 multiple choice,,# 7,Percentage correct,61.97,iBOWIMG baseline,-,Visual Question Answering,Simple Baseline for Visual Question Answering,/paper/simple-baseline-for-visual-question-answering,https://arxiv.org/pdf/1512.02167v2.pdf
COCO Visual Question Answering (VQA) real images 1.0 open ended,,# 10,Percentage correct,55.89,iBOWIMG baseline,-,Visual Question Answering,Simple Baseline for Visual Question Answering,/paper/simple-baseline-for-visual-question-answering,https://arxiv.org/pdf/1512.02167v2.pdf
OMNIGLOT - 1-Shot Learning,,# 4,Accuracy,98.1%,Neural Statistician,-,Few-Shot Image Classification,Towards a Neural Statistician,/paper/towards-a-neural-statistician,https://arxiv.org/pdf/1606.02185v2.pdf
OMNIGLOT - 5-Shot Learning,,# 4,Accuracy,99.5%,Neural Statistician,-,Few-Shot Image Classification,Towards a Neural Statistician,/paper/towards-a-neural-statistician,https://arxiv.org/pdf/1606.02185v2.pdf
BSD100 - 4x upscaling,,# 33,PSNR,24.95,Perceptual Loss,-,Image Super-Resolution,Perceptual Losses for Real-Time Style Transfer and Super-Resolution,/paper/perceptual-losses-for-real-time-style,https://arxiv.org/pdf/1603.08155v1.pdf
BSD100 - 4x upscaling,,# 32,SSIM,0.6317,Perceptual Loss,-,Image Super-Resolution,Perceptual Losses for Real-Time Style Transfer and Super-Resolution,/paper/perceptual-losses-for-real-time-style,https://arxiv.org/pdf/1603.08155v1.pdf
Set5 - 4x upscaling,,# 33,PSNR,27.09,Perceptual Loss,-,Image Super-Resolution,Perceptual Losses for Real-Time Style Transfer and Super-Resolution,/paper/perceptual-losses-for-real-time-style,https://arxiv.org/pdf/1603.08155v1.pdf
Set5 - 4x upscaling,,# 32,SSIM,0.768,Perceptual Loss,-,Image Super-Resolution,Perceptual Losses for Real-Time Style Transfer and Super-Resolution,/paper/perceptual-losses-for-real-time-style,https://arxiv.org/pdf/1603.08155v1.pdf
MMI,,# 1,Accuracy,98.63%,DeXpression,-,Facial Expression Recognition,DeXpression: Deep Convolutional Neural Network for Expression Recognition,/paper/dexpression-deep-convolutional-neural-network,https://arxiv.org/pdf/1509.05371v2.pdf
D&D,,# 4,Accuracy,"77,62%",GCAPS-CNN,-,Graph Classification,Graph Capsule Convolutional Neural Networks,/paper/graph-capsule-convolutional-neural-networks,https://arxiv.org/pdf/1805.08090v4.pdf
IMDb-B,,# 3,Accuracy,71.69%,GCAPS-CNN,-,Graph Classification,Graph Capsule Convolutional Neural Networks,/paper/graph-capsule-convolutional-neural-networks,https://arxiv.org/pdf/1805.08090v4.pdf
NCI1,,# 1,Accuracy,82.72%,GCAPS-CNN,-,Graph Classification,Graph Capsule Convolutional Neural Networks,/paper/graph-capsule-convolutional-neural-networks,https://arxiv.org/pdf/1805.08090v4.pdf
CIFAR-10,,# 10,Percentage correct,96.5,Fractional MP,-,Image Classification,Fractional Max-Pooling,/paper/fractional-max-pooling,https://arxiv.org/pdf/1412.6071v4.pdf
CIFAR-100,,# 16,Percentage correct,73.6,Fractional MP,-,Image Classification,Fractional Max-Pooling,/paper/fractional-max-pooling,https://arxiv.org/pdf/1412.6071v4.pdf
MNIST,,# 3,Percentage error,0.3,Fractional MP,-,Image Classification,Fractional Max-Pooling,/paper/fractional-max-pooling,https://arxiv.org/pdf/1412.6071v4.pdf
KITTI Cars Easy,,# 4,AP,81.94%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Cars Hard,,# 2,AP,66.38%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Cars Moderate,,# 6,AP,71.88%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Cyclists Easy,,# 3,AP,64.00%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Cyclists Hard,,# 3,AP,46.61%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Cyclists Moderate,,# 4,AP,52.18%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Pedestrians Easy,,# 3,AP,50.80%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Pedestrians Hard,,# 2,AP,40.88%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
KITTI Pedestrians Moderate,,# 4,AP,42.81%,AVOD + Feature Pyramid,-,3D Object Detection,Joint 3D Proposal Generation and Object Detection from View Aggregation,/paper/joint-3d-proposal-generation-and-object,https://arxiv.org/pdf/1712.02294v4.pdf
SNLI,,# 8,% Test Accuracy,89.1,SLRC,-,Natural Language Inference,I Know What You Want: Semantic Learning for Text Comprehension,/paper/i-know-what-you-want-semantic-learning-for,https://arxiv.org/pdf/1809.02794v2.pdf
SNLI,,# 37,% Train Accuracy,89.1,SLRC,-,Natural Language Inference,I Know What You Want: Semantic Learning for Text Comprehension,/paper/i-know-what-you-want-semantic-learning-for,https://arxiv.org/pdf/1809.02794v2.pdf
SNLI,,# 1,Parameters,6.1m,SLRC,-,Natural Language Inference,I Know What You Want: Semantic Learning for Text Comprehension,/paper/i-know-what-you-want-semantic-learning-for,https://arxiv.org/pdf/1809.02794v2.pdf
SNLI,,# 1,% Test Accuracy,91.3,SJRC (BERT-Large +SRL),-,Natural Language Inference,I Know What You Want: Semantic Learning for Text Comprehension,/paper/i-know-what-you-want-semantic-learning-for,https://arxiv.org/pdf/1809.02794v2.pdf
SNLI,,# 6,% Train Accuracy,95.7,SJRC (BERT-Large +SRL),-,Natural Language Inference,I Know What You Want: Semantic Learning for Text Comprehension,/paper/i-know-what-you-want-semantic-learning-for,https://arxiv.org/pdf/1809.02794v2.pdf
SNLI,,# 1,Parameters,308m,SJRC (BERT-Large +SRL),-,Natural Language Inference,I Know What You Want: Semantic Learning for Text Comprehension,/paper/i-know-what-you-want-semantic-learning-for,https://arxiv.org/pdf/1809.02794v2.pdf
ICVL Hands,,# 5,Average 3D Error,8.1,DeepPrior++,-,Hand Pose Estimation,DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation,/paper/deepprior-improving-fast-and-accurate-3d-hand,https://arxiv.org/pdf/1708.08325v1.pdf
MSRA Hands,,# 4,Average 3D Error,9.5,DeepPrior++,-,Hand Pose Estimation,DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation,/paper/deepprior-improving-fast-and-accurate-3d-hand,https://arxiv.org/pdf/1708.08325v1.pdf
NYU Hands,,# 4,Average 3D Error,12.3,DeepPrior++,-,Hand Pose Estimation,DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation,/paper/deepprior-improving-fast-and-accurate-3d-hand,https://arxiv.org/pdf/1708.08325v1.pdf
ICSI Meeting Recorder Dialog Act (MRDA) corpus,,# 1,Accuracy,91.7,CRF-ASN,-,Dialogue Act Classification,Dialogue Act Recognition via CRF-Attentive Structured Network,/paper/dialogue-act-recognition-via-crf-attentive,https://arxiv.org/pdf/1711.05568v1.pdf
Switchboard corpus,,# 1,Accuracy,81.3,CRF-ASN,-,Dialogue Act Classification,Dialogue Act Recognition via CRF-Attentive Structured Network,/paper/dialogue-act-recognition-via-crf-attentive,https://arxiv.org/pdf/1711.05568v1.pdf
CIFAR-10,,# 1,Model Entropy,4.5,NICE,-,Image Generation,NICE: Non-linear Independent Components Estimation,/paper/nice-non-linear-independent-components,https://arxiv.org/pdf/1410.8516v6.pdf
300W,,# 4,Mean Error Rate,3.49,LAB + Oracle + Inter-ocular Normalisation,-,Face Alignment,Look at Boundary: A Boundary-Aware Face Alignment Algorithm,/paper/look-at-boundary-a-boundary-aware-face,https://arxiv.org/pdf/1805.10483v1.pdf
DukeMTMC-reID,,# 19,Rank-1,25.13,BOW,-,Person Re-Identification,Scalable Person Re-Identification: A Benchmark,/paper/scalable-person-re-identification-a-benchmark,https://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf
DukeMTMC-reID,,# 19,MAP,12.17,BOW,-,Person Re-Identification,Scalable Person Re-Identification: A Benchmark,/paper/scalable-person-re-identification-a-benchmark,https://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf
Market-1501,,# 25,Rank-1,34.4,BOW,-,Person Re-Identification,Scalable Person Re-Identification: A Benchmark,/paper/scalable-person-re-identification-a-benchmark,https://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf
Market-1501,,# 25,MAP,14.09,BOW,-,Person Re-Identification,Scalable Person Re-Identification: A Benchmark,/paper/scalable-person-re-identification-a-benchmark,https://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf
Leeds Sports Poses,,# 2,PCK,92.6%,Multi-Context Attention,-,Pose Estimation,Multi-Context Attention for Human Pose Estimation,/paper/multi-context-attention-for-human-pose,https://arxiv.org/pdf/1702.07432v1.pdf
MPII Human Pose,,# 3,PCKh-0.5,91.5%,Multi-Context Attention,-,Pose Estimation,Multi-Context Attention for Human Pose Estimation,/paper/multi-context-attention-for-human-pose,https://arxiv.org/pdf/1702.07432v1.pdf
ADE20K Labels-to-Photos,,# 1,mIoU,38.5,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
ADE20K Labels-to-Photos,,# 1,Accuracy,79.9%,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
ADE20K Labels-to-Photos,,# 1,FID,33.9,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 1,mIoU,30.8,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 1,Accuracy,82.9%,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
ADE20K-Outdoor Labels-to-Photos,,# 1,FID,63.3,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
Cityscapes Labels-to-Photo,,# 1,Per-pixel Accuracy,81.9%,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
Cityscapes Labels-to-Photo,,# 1,mIoU,62.3,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
Cityscapes Labels-to-Photo,,# 2,FID,71.8,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
COCO-Stuff Labels-to-Photos,,# 1,mIoU,37.4,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
COCO-Stuff Labels-to-Photos,,# 1,Accuracy,67.9%,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
COCO-Stuff Labels-to-Photos,,# 1,FID,22.6,SPADE,-,Image-to-Image Translation,Semantic Image Synthesis with Spatially-Adaptive Normalization,/paper/semantic-image-synthesis-with-spatially,https://arxiv.org/pdf/1903.07291v1.pdf
Market-1501,,# 20,Rank-1,79.5,DLCE,-,Person Re-Identification,A Discriminatively Learned CNN Embedding for Person Re-identification,/paper/a-discriminatively-learned-cnn-embedding-for,https://arxiv.org/pdf/1611.05666v2.pdf
Market-1501,,# 20,MAP,59.9,DLCE,-,Person Re-Identification,A Discriminatively Learned CNN Embedding for Person Re-identification,/paper/a-discriminatively-learned-cnn-embedding-for,https://arxiv.org/pdf/1611.05666v2.pdf
MPII Human Pose,,# 5,PCKh-0.5,91.0%,Integral Regression,-,Pose Estimation,Integral Human Pose Regression,/paper/integral-human-pose-regression,https://arxiv.org/pdf/1711.08229v4.pdf
BSD100 - 4x upscaling,,# 13,PSNR,27.48,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
BSD100 - 4x upscaling,,# 18,SSIM,0.7306,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
BSD68 sigma15,,# 1,PSNR,31.88,NLRN,-,Image Denoising,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
BSD68 sigma25,,# 1,PSNR,29.41,NLRN,-,Image Denoising,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
BSD68 sigma50,,# 2,PSNR,26.47,NLRN,-,Image Denoising,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Set14 - 4x upscaling,,# 14,PSNR,28.36,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Set14 - 4x upscaling,,# 19,SSIM,0.7745,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Set5 - 4x upscaling,,# 12,PSNR,31.92,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Set5 - 4x upscaling,,# 15,SSIM,0.8916,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Urban100 - 4x upscaling,,# 12,PSNR,25.79,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Urban100 - 4x upscaling,,# 13,SSIM,0.7729,NLRN,-,Image Super-Resolution,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Urban100 sigma50,,# 2,PSNR,27.49,NLRN,-,Image Denoising,Non-Local Recurrent Network for Image Restoration,/paper/non-local-recurrent-network-for-image,https://arxiv.org/pdf/1806.02919v2.pdf
Aerial-to-Map,,# 2,Per-pixel Accuracy,42%,DualGAN,-,Image-to-Image Translation,DualGAN: Unsupervised Dual Learning for Image-to-Image Translation,/paper/dualgan-unsupervised-dual-learning-for-image,https://arxiv.org/pdf/1704.02510v4.pdf
Aerial-to-Map,,# 2,Per-class Accuracy,22%,DualGAN,-,Image-to-Image Translation,DualGAN: Unsupervised Dual Learning for Image-to-Image Translation,/paper/dualgan-unsupervised-dual-learning-for-image,https://arxiv.org/pdf/1704.02510v4.pdf
Aerial-to-Map,,# 2,Class IOU,0.09,DualGAN,-,Image-to-Image Translation,DualGAN: Unsupervised Dual Learning for Image-to-Image Translation,/paper/dualgan-unsupervised-dual-learning-for-image,https://arxiv.org/pdf/1704.02510v4.pdf
Caltech,,# 13,Reasonable Miss Rate,9.95,MS-CNN,-,Pedestrian Detection,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,/paper/a-unified-multi-scale-deep-convolutional,https://arxiv.org/pdf/1607.07155v1.pdf
WIDER Face (Hard),,# 9,AP,0.809,MSCNN,-,Face Detection,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,/paper/a-unified-multi-scale-deep-convolutional,https://arxiv.org/pdf/1607.07155v1.pdf
Sequential MNIST,,# 3,Unpermuted Accuracy,98.2%,LSTM,-,Sequential Image Classification,Unitary Evolution Recurrent Neural Networks,/paper/unitary-evolution-recurrent-neural-networks,https://arxiv.org/pdf/1511.06464v4.pdf
Sequential MNIST,,# 4,Permuted Accuracy,88%,LSTM,-,Sequential Image Classification,Unitary Evolution Recurrent Neural Networks,/paper/unitary-evolution-recurrent-neural-networks,https://arxiv.org/pdf/1511.06464v4.pdf
CIFAR-10,,# 6,FID,27.4,FOGAN,-,Image Generation,First Order Generative Adversarial Networks,/paper/first-order-generative-adversarial-networks,https://arxiv.org/pdf/1802.04591v2.pdf
LSUN Bedroom 256 x 256,,# 5,FID,11.4,FOGAN,-,Image Generation,First Order Generative Adversarial Networks,/paper/first-order-generative-adversarial-networks,https://arxiv.org/pdf/1802.04591v2.pdf
"CIFAR-10, 4000 Labels",,# 1,Accuracy,95,SWSA,-,Semi-Supervised Image Classification,There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average,/paper/there-are-many-consistent-explanations-of,https://arxiv.org/pdf/1806.05594v3.pdf
TACRED,,# 4,F1,65.1,PA-LSTM,-,Relation Extraction,Position-aware Attention and Supervised Data Improve Slot Filling,/paper/position-aware-attention-and-supervised-data,https://aclweb.org/anthology/D17-1004
Amazon Review Polarity,,# 1,Accuracy,79.5,EasyTL,-,Transfer Learning,Easy Transfer Learning By Exploiting Intra-domain Structures,/paper/easy-transfer-learning-by-exploiting-intra,https://arxiv.org/pdf/1904.01376v2.pdf
ImageCLEF-DA,,# 1,Accuracy,88.2,EasyTL,-,Transfer Learning,Easy Transfer Learning By Exploiting Intra-domain Structures,/paper/easy-transfer-learning-by-exploiting-intra,https://arxiv.org/pdf/1904.01376v2.pdf
Office-Home,,# 1,Accuracy,63.3,EasyTL,-,Transfer Learning,Easy Transfer Learning By Exploiting Intra-domain Structures,/paper/easy-transfer-learning-by-exploiting-intra,https://arxiv.org/pdf/1904.01376v2.pdf
CamVid,,# 1,Global Accuracy,91.5%,FC-DenseNet103,-,Semantic Segmentation,The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,/paper/the-one-hundred-layers-tiramisu-fully,https://arxiv.org/pdf/1611.09326v3.pdf
CamVid,,# 3,Mean IoU,66.9%,FC-DenseNet103,-,Semantic Segmentation,The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,/paper/the-one-hundred-layers-tiramisu-fully,https://arxiv.org/pdf/1611.09326v3.pdf
PASCAL VOC 2007,,# 6,MAP,47.3,MELM,-,Weakly Supervised Object Detection,Min-Entropy Latent Model for Weakly Supervised Object Detection,/paper/min-entropy-latent-model-for-weakly,https://arxiv.org/pdf/1902.06057v1.pdf
PASCAL VOC 2012,,# 6,MAP,42.4,MELM,-,Weakly Supervised Object Detection,Min-Entropy Latent Model for Weakly Supervised Object Detection,/paper/min-entropy-latent-model-for-weakly,https://arxiv.org/pdf/1902.06057v1.pdf
COCO,,# 6,Test AP,62.8,Pose-AE,-,Keypoint Detection,Associative Embedding: End-to-End Learning for Joint Detection and Grouping,/paper/associative-embedding-end-to-end-learning-for,https://arxiv.org/pdf/1611.05424v2.pdf
MPII Multi-Person,,# 3,AP,77.5%,Associative Embedding,-,Multi-Person Pose Estimation,Associative Embedding: End-to-End Learning for Joint Detection and Grouping,/paper/associative-embedding-end-to-end-learning-for,https://arxiv.org/pdf/1611.05424v2.pdf
MultiNLI,,# 5,Matched,71.4,GenSen,-,Natural Language Inference,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
MultiNLI,,# 5,Mismatched,71.3,GenSen,-,Natural Language Inference,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
Quora Question Pairs,,# 6,Accuracy,87.01,GenSen,-,Paraphrase Identification,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
SentEval,,# 1,MRPC,78.6/84.4,GenSen,-,Semantic Textual Similarity,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
SentEval,,# 1,SICK-R,0.888,GenSen,-,Semantic Textual Similarity,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
SentEval,,# 1,SICK-E,87.8,GenSen,-,Semantic Textual Similarity,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
SentEval,,# 1,STS,78.9/78.6,GenSen,-,Semantic Textual Similarity,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,/paper/learning-general-purpose-distributed-sentence,https://arxiv.org/pdf/1804.00079v1.pdf
Vid4 - 4x upscaling,,# 7,PSNR,25.35,VESPCN,-,Video Super-Resolution,Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,/paper/real-time-video-super-resolution-with-spatio,https://arxiv.org/pdf/1611.05250v2.pdf
Vid4 - 4x upscaling,,# 6,SSIM,0.7557,VESPCN,-,Video Super-Resolution,Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,/paper/real-time-video-super-resolution-with-spatio,https://arxiv.org/pdf/1611.05250v2.pdf
Vid4 - 4x upscaling,,# 2,MOVIE,5.82,VESPCN,-,Video Super-Resolution,Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,/paper/real-time-video-super-resolution-with-spatio,https://arxiv.org/pdf/1611.05250v2.pdf
Vid4 - 4x upscaling,,# 11,PSNR,23.82,bicubic,-,Video Super-Resolution,Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,/paper/real-time-video-super-resolution-with-spatio,https://arxiv.org/pdf/1611.05250v2.pdf
Vid4 - 4x upscaling,,# 1,SSIM,0.6548,bicubic,-,Video Super-Resolution,Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,/paper/real-time-video-super-resolution-with-spatio,https://arxiv.org/pdf/1611.05250v2.pdf
Vid4 - 4x upscaling,,# 5,MOVIE,9.31,bicubic,-,Video Super-Resolution,Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,/paper/real-time-video-super-resolution-with-spatio,https://arxiv.org/pdf/1611.05250v2.pdf
SemEvalCQA,,# 2,[emailÂ protected],0.755,AP-CNN,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
SemEvalCQA,,# 5,MAP,0.7709999999999999,AP-CNN,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
WikiQA,,# 6,MAP,0.6886,AP-CNN,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
WikiQA,,# 9,MRR,0.6957,AP-CNN,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
YahooCQA,,# 2,[emailÂ protected],0.568,AP-BiLSTM,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
YahooCQA,,# 2,MRR,0.731,AP-BiLSTM,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
YahooCQA,,# 3,[emailÂ protected],0.56,AP-CNN,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
YahooCQA,,# 3,MRR,0.726,AP-CNN,-,Question Answering,Attentive Pooling Networks,/paper/attentive-pooling-networks,https://arxiv.org/pdf/1602.03609v1.pdf
Real-World Affective Faces,,# 1,Accuracy,87.0%,Covariance Pooling,-,Facial Expression Recognition,Covariance Pooling For Facial Expression Recognition,/paper/covariance-pooling-for-facial-expression,https://arxiv.org/pdf/1805.04855v1.pdf
Static Facial Expressions in the Wild,,# 1,Accuracy,58.14%,Covariance Pooling,-,Facial Expression Recognition,Covariance Pooling For Facial Expression Recognition,/paper/covariance-pooling-for-facial-expression,https://arxiv.org/pdf/1805.04855v1.pdf
Charades,,# 1,MAP,42.5,LFB NL,-,Action Recognition In Videos,Long-Term Feature Banks for Detailed Video Understanding,/paper/long-term-feature-banks-for-detailed-video,https://arxiv.org/pdf/1812.05038v2.pdf
Caltech Lanes Cordova,,# 2,F1,0.866,Overfeat CNN detector + DBSCAN,-,Lane Detection,An Empirical Evaluation of Deep Learning on Highway Driving,/paper/an-empirical-evaluation-of-deep-learning-on,https://arxiv.org/pdf/1504.01716v3.pdf
Caltech Lanes Washington,,# 2,F1,0.861,Overfeat CNN detector + DBSCAN,-,Lane Detection,An Empirical Evaluation of Deep Learning on Highway Driving,/paper/an-empirical-evaluation-of-deep-learning-on,https://arxiv.org/pdf/1504.01716v3.pdf
SNLI,,# 12,% Test Accuracy,88.6,KIM,-,Natural Language Inference,Neural Natural Language Inference Models Enhanced with External Knowledge,/paper/neural-natural-language-inference-models,https://arxiv.org/pdf/1711.04289v3.pdf
SNLI,,# 13,% Train Accuracy,94.1,KIM,-,Natural Language Inference,Neural Natural Language Inference Models Enhanced with External Knowledge,/paper/neural-natural-language-inference-models,https://arxiv.org/pdf/1711.04289v3.pdf
SNLI,,# 1,Parameters,4.3m,KIM,-,Natural Language Inference,Neural Natural Language Inference Models Enhanced with External Knowledge,/paper/neural-natural-language-inference-models,https://arxiv.org/pdf/1711.04289v3.pdf
SNLI,,# 8,% Test Accuracy,89.1,KIM Ensemble,-,Natural Language Inference,Neural Natural Language Inference Models Enhanced with External Knowledge,/paper/neural-natural-language-inference-models,https://arxiv.org/pdf/1711.04289v3.pdf
SNLI,,# 14,% Train Accuracy,93.6,KIM Ensemble,-,Natural Language Inference,Neural Natural Language Inference Models Enhanced with External Knowledge,/paper/neural-natural-language-inference-models,https://arxiv.org/pdf/1711.04289v3.pdf
SNLI,,# 1,Parameters,43m,KIM Ensemble,-,Natural Language Inference,Neural Natural Language Inference Models Enhanced with External Knowledge,/paper/neural-natural-language-inference-models,https://arxiv.org/pdf/1711.04289v3.pdf
RotoWire,,# 2,BLEU,14.19,Encoder-decoder + conditional copy,-,Data-to-Text Generation,Challenges in Data-to-Document Generation,/paper/challenges-in-data-to-document-generation,https://arxiv.org/pdf/1707.08052v1.pdf
RotoWire (Content Ordering),,# 2,DLD,15.42%,Encoder-decoder + conditional copy,-,Data-to-Text Generation,Challenges in Data-to-Document Generation,/paper/challenges-in-data-to-document-generation,https://arxiv.org/pdf/1707.08052v1.pdf
Rotowire (Content Selection),,# 2,Precision,29.49%,Encoder-decoder + conditional copy,-,Data-to-Text Generation,Challenges in Data-to-Document Generation,/paper/challenges-in-data-to-document-generation,https://arxiv.org/pdf/1707.08052v1.pdf
Rotowire (Content Selection),,# 2,Recall,36.18%,Encoder-decoder + conditional copy,-,Data-to-Text Generation,Challenges in Data-to-Document Generation,/paper/challenges-in-data-to-document-generation,https://arxiv.org/pdf/1707.08052v1.pdf
RotoWire (Relation Generation),,# 2,count,23.72,Encoder-decoder + conditional copy,-,Data-to-Text Generation,Challenges in Data-to-Document Generation,/paper/challenges-in-data-to-document-generation,https://arxiv.org/pdf/1707.08052v1.pdf
RotoWire (Relation Generation),,# 2,Precision,74.80%,Encoder-decoder + conditional copy,-,Data-to-Text Generation,Challenges in Data-to-Document Generation,/paper/challenges-in-data-to-document-generation,https://arxiv.org/pdf/1707.08052v1.pdf
Caltech,,# 11,Reasonable Miss Rate,8.7,FasterRCNN,-,Pedestrian Detection,Is Faster R-CNN Doing Well for Pedestrian Detection?,/paper/is-faster-r-cnn-doing-well-for-pedestrian,https://arxiv.org/pdf/1607.07032v2.pdf
Caltech,,# 8,Reasonable Miss Rate,7.3,RPN+BF,-,Pedestrian Detection,Is Faster R-CNN Doing Well for Pedestrian Detection?,/paper/is-faster-r-cnn-doing-well-for-pedestrian,https://arxiv.org/pdf/1607.07032v2.pdf
Mini-ImageNet - 1-Shot Learning,,# 1,Accuracy,61.2%,MTL,-,Few-Shot Learning,Meta-Transfer Learning for Few-Shot Learning,/paper/meta-transfer-learning-for-few-shot-learning,https://arxiv.org/pdf/1812.02391v3.pdf
Mini-ImageNet - 1-Shot Learning,,# 2,Accuracy,61.2%,MTL,-,Few-Shot Image Classification,Meta-Transfer Learning for Few-Shot Learning,/paper/meta-transfer-learning-for-few-shot-learning,https://arxiv.org/pdf/1812.02391v3.pdf
Mini-ImageNet - 5-Shot Learning,,# 2,Accuracy,75.5%,MTL,-,Few-Shot Image Classification,Meta-Transfer Learning for Few-Shot Learning,/paper/meta-transfer-learning-for-few-shot-learning,https://arxiv.org/pdf/1812.02391v3.pdf
300W,,# 2,Mean Error Rate,2.83,SIR-LAN,-,Face Alignment,Facial Landmarks Detection by Self-Iterative Regression based Landmarks-Attention Network,/paper/facial-landmarks-detection-by-self-iterative,https://arxiv.org/pdf/1803.06598v1.pdf
NarrativeQA,,# 1,BLEU-1,44.35,DecaProp,-,Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
NarrativeQA,,# 1,BLEU-4,27.61,DecaProp,-,Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
NarrativeQA,,# 1,METEOR,21.8,DecaProp,-,Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
NarrativeQA,,# 2,Rouge-L,44.69,DecaProp,-,Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
NewsQA,,# 1,F1,66.3,DecaProp,-,Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
NewsQA,,# 1,EM,53.1,DecaProp,-,Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
Quasar,,# 2,EM (Quasar-T),38.6,DecaProp,-,Open-Domain Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
Quasar,,# 2,F1 (Quasar-T),46.9,DecaProp,-,Open-Domain Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
SearchQA,,# 1,Unigram Acc,62.2,DecaProp,-,Open-Domain Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
SearchQA,,# 1,N-gram F1,70.8,DecaProp,-,Open-Domain Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
SearchQA,,# 2,EM,56.8,DecaProp,-,Open-Domain Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
SearchQA,,# 2,F1,63.6,DecaProp,-,Open-Domain Question Answering,Densely Connected Attention Propagation for Reading Comprehension,/paper/densely-connected-attention-propagation-for,https://arxiv.org/pdf/1811.04210v2.pdf
VOT2017/18,,# 6,Expected Average Overlap (EAO),0.32299999999999995,LSART,-,Visual Object Tracking,Learning Spatial-Aware Regressions for Visual Tracking,/paper/learning-spatial-aware-regressions-for-visual,https://arxiv.org/pdf/1706.07457v2.pdf
bAbi,,# 2,Mean Error Rate,0.46%,RR,-,Question Answering,Recurrent Relational Networks,/paper/recurrent-relational-networks,https://arxiv.org/pdf/1711.08028v4.pdf
Atari 2600 Alien,,# 7,Score,3166.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Amidar,,# 6,Score,1735.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Assault,,# 8,Score,7203.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Asterix,,# 1,Score,406211.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Asteroids,,# 12,Score,1516.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Atlantis,,# 6,Score,841075.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Bank Heist,,# 10,Score,976.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Battle Zone,,# 10,Score,28742.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Beam Rider,,# 11,Score,14074.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Berzerk,,# 3,Score,1645.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Bowling,,# 3,Score,81.8,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Boxing,,# 5,Score,97.8,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Breakout,,# 3,Score,748.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Centipede,,# 3,Score,9646.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Chopper Command,,# 2,Score,15600.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Crazy Climber,,# 1,Score,179877.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Demon Attack,,# 1,Score,130955.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Double Dunk,,# 5,Score,2.5,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Enduro,,# 1,Score,3454.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Fishing Derby,,# 13,Score,8.9,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Freeway,,# 2,Score,33.9,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Frostbite,,# 7,Score,3965.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Gopher,,# 6,Score,33641.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Gravitar,,# 10,Score,440.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 HERO,,# 1,Score,38874.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Ice Hockey,,# 14,Score,-3.5,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 James Bond,,# 4,Score,1909.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Kangaroo,,# 7,Score,12853.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Krull,,# 5,Score,9735.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Kung-Fu Master,,# 3,Score,48192.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Ms. Pacman,,# 5,Score,3415.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Name This Game,,# 5,Score,12542.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Pong,,# 2,Score,20.9,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Private Eye,,# 1,Score,15095.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Q*Bert,,# 2,Score,23784.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 River Raid,,# 4,Score,17322.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Road Runner,,# 7,Score,55839.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Robotank,,# 13,Score,52.3,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Seaquest,,# 1,Score,266434.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Space Invaders,,# 8,Score,5747.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Star Gunner,,# 17,Score,49095.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Tennis,,# 2,Score,23.1,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Time Pilot,,# 8,Score,8329.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Tutankham,,# 2,Score,280.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Up and Down,,# 15,Score,15612.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Venture,,# 2,Score,1520.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Video Pinball,,# 1,Score,949604.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Wizard of Wor,,# 7,Score,9300.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
Atari 2600 Zaxxon,,# 9,Score,10513.0,C51 noop,-,Atari Games,A Distributional Perspective on Reinforcement Learning,/paper/a-distributional-perspective-on-reinforcement,https://arxiv.org/pdf/1707.06887v1.pdf
CUB-200-2011,,# 1,Top-1 Error Rate,53.36,SPG,-,Weakly-Supervised Object Localization,Self-produced Guidance for Weakly-supervised Object Localization,/paper/self-produced-guidance-for-weakly-supervised,https://arxiv.org/pdf/1807.08902v2.pdf
CUB-200-2011,,# 1,Top-5 Error,42.28,SPG,-,Weakly-Supervised Object Localization,Self-produced Guidance for Weakly-supervised Object Localization,/paper/self-produced-guidance-for-weakly-supervised,https://arxiv.org/pdf/1807.08902v2.pdf
ILSVRC 2015,,# 1,Top-1 Error Rate,51.4,SPG,-,Weakly-Supervised Object Localization,Self-produced Guidance for Weakly-supervised Object Localization,/paper/self-produced-guidance-for-weakly-supervised,https://arxiv.org/pdf/1807.08902v2.pdf
ILSVRC 2016,,# 1,Top-5 Error,40.0,SPG,-,Weakly-Supervised Object Localization,Self-produced Guidance for Weakly-supervised Object Localization,/paper/self-produced-guidance-for-weakly-supervised,https://arxiv.org/pdf/1807.08902v2.pdf
ImageNet,,# 11,Top 1 Accuracy,79.2%,JFT-300M Finetuning,-,Image Classification,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,/paper/revisiting-unreasonable-effectiveness-of-data,https://arxiv.org/pdf/1707.02968v2.pdf
ImageNet,,# 8,Top 5 Accuracy,94.7%,JFT-300M Finetuning,-,Image Classification,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,/paper/revisiting-unreasonable-effectiveness-of-data,https://arxiv.org/pdf/1707.02968v2.pdf
PASCAL VOC 2012,,# 12,Mean IoU,76.5%,ImageNet+JFT-300M Initialization,-,Semantic Segmentation,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,/paper/revisiting-unreasonable-effectiveness-of-data,https://arxiv.org/pdf/1707.02968v2.pdf
MHP v1.0,,# 1,AP 0.5,57.09%,NAN,-,Multi-Human Parsing,Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing,/paper/understanding-humans-in-crowded-scenes-deep,https://arxiv.org/pdf/1804.03287v3.pdf
MHP v2.0,,# 1,AP 0.5,25.14%,NAN,-,Multi-Human Parsing,Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing,/paper/understanding-humans-in-crowded-scenes-deep,https://arxiv.org/pdf/1804.03287v3.pdf
PASCAL-Person-Part,,# 1,AP 0.5,59.70%,NAN,-,Multi-Human Parsing,Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing,/paper/understanding-humans-in-crowded-scenes-deep,https://arxiv.org/pdf/1804.03287v3.pdf
Annotated Faces in the Wild,,# 3,AP,0.996,Anchor-based,-,Face Detection,Robust Face Detection via Learning Small Faces on Hard Images,/paper/robust-face-detection-via-learning-small,https://arxiv.org/pdf/1811.11662v1.pdf
FDDB,,# 4,AP,0.987,Anchor-based,-,Face Detection,Robust Face Detection via Learning Small Faces on Hard Images,/paper/robust-face-detection-via-learning-small,https://arxiv.org/pdf/1811.11662v1.pdf
PASCAL Face,,# 2,AP,0.99,Anchor-based,-,Face Detection,Robust Face Detection via Learning Small Faces on Hard Images,/paper/robust-face-detection-via-learning-small,https://arxiv.org/pdf/1811.11662v1.pdf
WIDER Face (Hard),,# 3,AP,0.889,Anchor-based,-,Face Detection,Robust Face Detection via Learning Small Faces on Hard Images,/paper/robust-face-detection-via-learning-small,https://arxiv.org/pdf/1811.11662v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 6,Restaurant (Acc),81.34,LCR-Rot,-,Aspect-Based Sentiment Analysis,Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention,/paper/left-center-right-separated-neural-network,https://arxiv.org/pdf/1802.00892v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 18,Laptop (Acc),75.24,LCR-Rot,-,Aspect-Based Sentiment Analysis,Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention,/paper/left-center-right-separated-neural-network,https://arxiv.org/pdf/1802.00892v1.pdf
MS MARCO,,# 1,Rouge-L,52.2,Masque Q&A Style,-,Question Answering,Multi-style Generative Reading Comprehension,/paper/multi-style-generative-reading-comprehension,https://arxiv.org/pdf/1901.02262v1.pdf
MS MARCO,,# 3,BLEU-1,43.77,Masque Q&A Style,-,Question Answering,Multi-style Generative Reading Comprehension,/paper/multi-style-generative-reading-comprehension,https://arxiv.org/pdf/1901.02262v1.pdf
CompCars,,# 1,Accuracy,95.4%,A3M,-,Fine-Grained Image Classification,Attribute-Aware Attention Model for Fine-grained Representation Learning,/paper/attribute-aware-attention-model-for-fine,https://arxiv.org/pdf/1901.00392v1.pdf
CUB-200-2011,,# 6,Accuracy,86.2%,A3M,-,Fine-Grained Image Classification,Attribute-Aware Attention Model for Fine-grained Representation Learning,/paper/attribute-aware-attention-model-for-fine,https://arxiv.org/pdf/1901.00392v1.pdf
Market-1501,,# 10,Rank-1,86.54,A3M,-,Person Re-Identification,Attribute-Aware Attention Model for Fine-grained Representation Learning,/paper/attribute-aware-attention-model-for-fine,https://arxiv.org/pdf/1901.00392v1.pdf
Market-1501,,# 10,MAP,68.97,A3M,-,Person Re-Identification,Attribute-Aware Attention Model for Fine-grained Representation Learning,/paper/attribute-aware-attention-model-for-fine,https://arxiv.org/pdf/1901.00392v1.pdf
MNIST,,# 8,Percentage error,0.8,Explaining and Harnessing Adversarial Examples,-,Image Classification,Explaining and Harnessing Adversarial Examples,/paper/explaining-and-harnessing-adversarial,https://arxiv.org/pdf/1412.6572v3.pdf
iSEG 2017 Challenge,,# 1,Dice Score,0.9257,HyperDenseNet,-,Medical Image Segmentation,HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation,/paper/hyperdense-net-a-hyper-densely-connected-cnn,https://arxiv.org/pdf/1804.02967v2.pdf
SQuAD2.0,,# 66,EM,71.417,Unet (ensemble),-,Question Answering,U-Net: Machine Reading Comprehension with Unanswerable Questions,/paper/u-net-machine-reading-comprehension-with,https://arxiv.org/pdf/1810.06638v1.pdf
SQuAD2.0,,# 66,F1,74.869,Unet (ensemble),-,Question Answering,U-Net: Machine Reading Comprehension with Unanswerable Questions,/paper/u-net-machine-reading-comprehension-with,https://arxiv.org/pdf/1810.06638v1.pdf
BSD100 - 4x upscaling,,# 27,PSNR,26.87,JMPF+,-,Image Super-Resolution,Joint Maximum Purity Forest with Application to Image Super-Resolution,/paper/joint-maximum-purity-forest-with-application,https://arxiv.org/pdf/1708.09200v1.pdf
Set14 - 4x upscaling,,# 33,PSNR,27.37,JMPF+,-,Image Super-Resolution,Joint Maximum Purity Forest with Application to Image Super-Resolution,/paper/joint-maximum-purity-forest-with-application,https://arxiv.org/pdf/1708.09200v1.pdf
Set5 - 4x upscaling,,# 28,PSNR,30.24,JMPF+,-,Image Super-Resolution,Joint Maximum Purity Forest with Application to Image Super-Resolution,/paper/joint-maximum-purity-forest-with-application,https://arxiv.org/pdf/1708.09200v1.pdf
ShapeNet-Part,,# 2,Class Average IoU,82.4,SpiderCNN,-,3D Part Segmentation,SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters,/paper/spidercnn-deep-learning-on-point-sets-with,https://arxiv.org/pdf/1803.11527v3.pdf
ShapeNet-Part,,# 3,Instance Average IoU,85.3,SpiderCNN,-,3D Part Segmentation,SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters,/paper/spidercnn-deep-learning-on-point-sets-with,https://arxiv.org/pdf/1803.11527v3.pdf
WMT2014 English-French,,# 27,BLEU score,26.22,SMT + iterative backtranslation (unsupervised),-,Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
WMT2014 English-German,,# 23,BLEU score,14.08,SMT + iterative backtranslation (unsupervised),-,Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
WMT2014 French-English,,# 1,BLEU score,25.87,SMT + iterative backtranslation (unsupervised),-,Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
WMT2014 French-English,,# 5,BLEU,25.9,SMT,-,Unsupervised Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
WMT2014 German-English,,# 3,BLEU score,17.43,SMT + iterative backtranslation (unsupervised),-,Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
WMT2016 English-German,,# 3,BLEU score,18.23,SMT + iterative backtranslation (unsupervised),-,Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
WMT2016 German-English,,# 3,BLEU score,23.05,SMT + iterative backtranslation (unsupervised),-,Machine Translation,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf
bAbi,,# 4,Mean Error Rate,6.4%,SDNC,-,Question Answering,Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,/paper/scaling-memory-augmented-neural-networks-with,https://arxiv.org/pdf/1610.09027v1.pdf
bAbi,,# 7,Accuracy (trained on 1k),49%,LSTM,-,Question Answering,Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,/paper/scaling-memory-augmented-neural-networks-with,https://arxiv.org/pdf/1610.09027v1.pdf
bAbi,,# 7,Mean Error Rate,28.7%,LSTM,-,Question Answering,Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,/paper/scaling-memory-augmented-neural-networks-with,https://arxiv.org/pdf/1610.09027v1.pdf
LibriSpeech test-other,,# 2,Word Error Rate (WER),7.63,tdnn + chain + rnnlm rescoring,-,Speech Recognition,Neural Network Language Modeling with Letter-based Features and Importance Sampling,/paper/neural-network-language-modeling-with-letter,https://www.cs.jhu.edu/~hxu/neural-network-language.pdf
General,,# 1,MAP,19.78,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
General,,# 1,MRR,36.1,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
General,,# 1,[emailÂ protected],19.03,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Medical domain,,# 1,MAP,34.05,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Medical domain,,# 1,MRR,54.64,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Medical domain,,# 1,[emailÂ protected],36.77,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Music domain,,# 1,MAP,40.97,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Music domain,,# 1,MRR,60.93,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Music domain,,# 1,[emailÂ protected],41.31,CRIM,-,Hypernym Discovery,CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery,/paper/crim-at-semeval-2018-task-9-a-hybrid-approach,https://aclweb.org/anthology/S18-1116
Edge-to-Handbags,,# 1,FID,0.069,PI-REC,-,Image Reconstruction,PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain,/paper/pi-rec-progressive-image-reconstruction-1,https://arxiv.org/pdf/1903.10146v1.pdf
Edge-to-Handbags,,# 1,LPIPS,0.168,PI-REC,-,Image Reconstruction,PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain,/paper/pi-rec-progressive-image-reconstruction-1,https://arxiv.org/pdf/1903.10146v1.pdf
Edge-to-Shoes,,# 1,FID,0.015,PI-REC,-,Image Reconstruction,PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain,/paper/pi-rec-progressive-image-reconstruction-1,https://arxiv.org/pdf/1903.10146v1.pdf
Edge-to-Shoes,,# 1,LPIPS,0.085,PI-REC,-,Image Reconstruction,PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain,/paper/pi-rec-progressive-image-reconstruction-1,https://arxiv.org/pdf/1903.10146v1.pdf
IC15,,# 5,F-Measure,84.14%,FTSN,-,Scene Text Detection,Fused Text Segmentation Networks for Multi-oriented Scene Text Detection,/paper/fused-text-segmentation-networks-for-multi,https://arxiv.org/pdf/1709.03272v4.pdf
Atari 2600 Alien,,# 6,Score,3213.5,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Amidar,,# 9,Score,782.5,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Assault,,# 5,Score,9011.6,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Asterix,,# 10,Score,18919.5,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Asteroids,,# 5,Score,2869.3,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Atlantis,,# 14,Score,340076.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Bank Heist,,# 6,Score,1103.3,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Battle Zone,,# 21,Score,8220.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Beam Rider,,# 17,Score,8299.4,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Berzerk,,# 8,Score,1199.6,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Bowling,,# 1,Score,102.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Boxing,,# 3,Score,99.3,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Breakout,,# 17,Score,344.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Centipede,,# 1,Score,49065.8,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Chopper Command,,# 21,Score,775.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Crazy Climber,,# 11,Score,119679.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Demon Attack,,# 11,Score,63644.9,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Double Dunk,,# 16,Score,-11.5,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Enduro,,# 8,Score,2002.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Fishing Derby,,# 2,Score,45.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Freeway,,# 4,Score,33.4,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Frostbite,,# 9,Score,3469.6,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Gopher,,# 4,Score,56218.2,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Gravitar,,# 8,Score,483.5,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 HERO,,# 18,Score,14225.2,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Ice Hockey,,# 15,Score,-4.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 James Bond,,# 17,Score,507.5,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Kangaroo,,# 5,Score,13150.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Krull,,# 4,Score,9745.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Kung-Fu Master,,# 8,Score,34393.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Montezuma's Revenge,,# 23,Score,0.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Ms. Pacman,,# 4,Score,4963.8,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Name This Game,,# 2,Score,15851.2,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Pong,,# 3,Score,20.6,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Private Eye,,# 12,Score,286.7,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Q*Bert,,# 20,Score,5236.8,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 River Raid,,# 10,Score,12530.8,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Road Runner,,# 11,Score,47770.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Robotank,,# 4,Score,64.3,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Seaquest,,# 9,Score,10932.3,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Space Invaders,,# 13,Score,2589.7,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Star Gunner,,# 21,Score,589.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Tennis,,# 4,Score,12.1,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Time Pilot,,# 18,Score,4870.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Tutankham,,# 9,Score,183.9,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Up and Down,,# 12,Score,22474.4,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Venture,,# 4,Score,1172.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Video Pinball,,# 18,Score,56287.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Wizard of Wor,,# 21,Score,483.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
Atari 2600 Zaxxon,,# 4,Score,14402.0,DDQN+Pop-Art noop,-,Atari Games,Learning values across many orders of magnitude,/paper/learning-values-across-many-orders-of,https://arxiv.org/pdf/1602.07714v2.pdf
SemEval 2014 Task 4 Sub Task 2,,# 20,Restaurant (Acc),78.79,JCI (hops),-,Aspect-Based Sentiment Analysis,Target-Sensitive Memory Networks for Aspect Sentiment Classification,/paper/target-sensitive-memory-networks-for-aspect,https://aclweb.org/anthology/P18-1088
SemEval 2014 Task 4 Sub Task 2,,# 6,Laptop (Acc),71.79,JCI (hops),-,Aspect-Based Sentiment Analysis,Target-Sensitive Memory Networks for Aspect Sentiment Classification,/paper/target-sensitive-memory-networks-for-aspect,https://aclweb.org/anthology/P18-1088
SemEval 2014 Task 4 Sub Task 2,,# 24,Restaurant (Acc),75.73,NP (hops),-,Aspect-Based Sentiment Analysis,Target-Sensitive Memory Networks for Aspect Sentiment Classification,/paper/target-sensitive-memory-networks-for-aspect,https://aclweb.org/anthology/P18-1088
SemEval 2014 Task 4 Sub Task 2,,# 10,Laptop (Acc),72.43,NP (hops),-,Aspect-Based Sentiment Analysis,Target-Sensitive Memory Networks for Aspect Sentiment Classification,/paper/target-sensitive-memory-networks-for-aspect,https://aclweb.org/anthology/P18-1088
IJB-A,,# 3,TAR @ FAR=0.01,97%,L2-constrained softmax loss,-,Face Verification,L2-constrained Softmax Loss for Discriminative Face Verification,/paper/l2-constrained-softmax-loss-for,https://arxiv.org/pdf/1703.09507v3.pdf
Pascal3D+,,# 4,Mean PCK,48.5,ConvNet,-,Keypoint Detection,Do Convnets Learn Correspondence?,/paper/do-convnets-learn-correspondence,https://arxiv.org/pdf/1411.1091v1.pdf
IWSLT2015 German-English,,# 4,BLEU score,33.1,Variational Attention,-,Machine Translation,Latent Alignment and Variational Attention,/paper/latent-alignment-and-variational-attention,https://arxiv.org/pdf/1807.03756v2.pdf
"PASCAL VOC 2012, 60 proposals per image",,# 1,Average Recall,0.8140000000000001,Recurrent Pixel Embedding,-,Object Proposal Generation,Recurrent Pixel Embedding for Instance Grouping,/paper/recurrent-pixel-embedding-for-instance,https://arxiv.org/pdf/1712.08273v1.pdf
SQuAD1.1,,# 32,EM,79.692,MAMCN+ (single model),-,Question Answering,A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension,/paper/a-multi-stage-memory-augmented-neural-network,https://aclweb.org/anthology/W18-2603
SQuAD1.1,,# 29,F1,86.727,MAMCN+ (single model),-,Question Answering,A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension,/paper/a-multi-stage-memory-augmented-neural-network,https://aclweb.org/anthology/W18-2603
enwiki8,,# 6,Bit per Character (BPC),1.11,12-layer Transformer,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
enwiki8,,# 1,Number of params,44M,12-layer Transformer,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
enwiki8,,# 5,Bit per Character (BPC),1.06,64-layer Transformer,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
enwiki8,,# 1,Number of params,235M,64-layer Transformer,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Hutter Prize,,# 3,Bit per Character (BPC),1.06,64-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Hutter Prize,,# 1,Number of params,235M,64-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Hutter Prize,,# 5,Bit per Character (BPC),1.11,12-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Hutter Prize,,# 1,Number of params,44M,12-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Text8,,# 4,Bit per Character (BPC),1.13,64-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Text8,,# 1,Number of params,235M,64-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Text8,,# 5,Bit per Character (BPC),1.18,12-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
Text8,,# 1,Number of params,44M,12-layer Character Transformer Model,-,Language Modelling,Character-Level Language Modeling with Deeper Self-Attention,/paper/character-level-language-modeling-with-deeper,https://arxiv.org/pdf/1808.04444v2.pdf
CIFAR-10,,# 2,Percentage correct,97.92,Proxyless-G + c/o,-,Image Classification,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,/paper/proxylessnas-direct-neural-architecture,https://arxiv.org/pdf/1812.00332v2.pdf
CIFAR-10,,# 2,Percentage error,2.08,Proxyless-G + c/o,-,Image Classification,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,/paper/proxylessnas-direct-neural-architecture,https://arxiv.org/pdf/1812.00332v2.pdf
CIFAR-10 Image Classification,,# 1,Percentage error,2.08,Proxyless-G + c/o,-,Architecture Search,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,/paper/proxylessnas-direct-neural-architecture,https://arxiv.org/pdf/1812.00332v2.pdf
CIFAR-10 Image Classification,,# 1,Params,5.7M,Proxyless-G + c/o,-,Architecture Search,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,/paper/proxylessnas-direct-neural-architecture,https://arxiv.org/pdf/1812.00332v2.pdf
CIFAR-10,,# 1,MAP,0.792,DTQ,-,Quantization,Deep Triplet Quantization,/paper/deep-triplet-quantization,https://arxiv.org/pdf/1902.00153v1.pdf
NUS-WIDE,,# 1,MAP,0.8009999999999999,DTQ,-,Image Retrieval,Deep Triplet Quantization,/paper/deep-triplet-quantization,https://arxiv.org/pdf/1902.00153v1.pdf
PASCAL VOC 2007,,# 1,MAP,53.6,Pred Net (Ens),-,Weakly Supervised Object Detection,Dissimilarity Coefficient based Weakly Supervised Object Detection,/paper/dissimilarity-coefficient-based-weakly,https://arxiv.org/pdf/1811.10016v1.pdf
PASCAL VOC 2012,,# 1,MAP,49.5,Pred Net (Ens),-,Weakly Supervised Object Detection,Dissimilarity Coefficient based Weakly Supervised Object Detection,/paper/dissimilarity-coefficient-based-weakly,https://arxiv.org/pdf/1811.10016v1.pdf
RWTH-PHOENIX-Weather 2014 T,,# 1,BLEU-4,19.26,Sign2Gloss2Text,-,Sign Language Translation,Neural Sign Language Translation,/paper/neural-sign-language-translation,https://openaccess.thecvf.com/content_cvpr_2018/papers/Camgoz_Neural_Sign_Language_CVPR_2018_paper.pdf
IC15,,# 3,F-Measure,84.5%,SLPR,-,Scene Text Detection,Sliding Line Point Regression for Shape Robust Scene Text Detection,/paper/sliding-line-point-regression-for-shape,https://arxiv.org/pdf/1801.09969v1.pdf
SCUT-CTW1500,,# 2,F-Measure,74.8%,SLPR,-,Curved Text Detection,Sliding Line Point Regression for Shape Robust Scene Text Detection,/paper/sliding-line-point-regression-for-shape,https://arxiv.org/pdf/1801.09969v1.pdf
IWSLT2015 English-German,,# 1,BLEU score,28.23,Transformer,-,Machine Translation,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
IWSLT2015 German-English,,# 1,BLEU score,34.44,Transformer,-,Machine Translation,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
Penn Treebank,,# 9,F1 score,92.7,Transformer,-,Constituency Parsing,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
WMT2014 English-French,,# 14,BLEU score,38.1,Transformer Base,-,Machine Translation,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
WMT2014 English-French,,# 8,BLEU score,41.0,Transformer Big,-,Machine Translation,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
WMT2014 English-German,,# 8,BLEU score,27.3,Transformer Base,-,Machine Translation,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
WMT2014 English-German,,# 7,BLEU score,28.4,Transformer Big,-,Machine Translation,Attention Is All You Need,/paper/attention-is-all-you-need,https://arxiv.org/pdf/1706.03762v5.pdf
CACDVS,,# 2,Accuracy,99.38%,AIM,-,Age-Invariant Face Recognition,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
CACDVS,,# 1,Accuracy,99.76%,AIM + CAFR,-,Age-Invariant Face Recognition,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
CAFR,,# 1,Accuracy,84.81%,AIM,-,Age-Invariant Face Recognition,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
FG-NET,,# 1,Accuracy,93.20%,AIM,-,Age-Invariant Face Recognition,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
IJB-C,,# 1,TAR @ FAR=0.01,93.50%,AIM,-,Face Verification,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
MORPH Album2,,# 1,Rank-1 Recognition Rate,99.65%,AIM + CAFR,-,Age-Invariant Face Recognition,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
MORPH Album2,,# 2,Rank-1 Recognition Rate,99.13%,AIM,-,Age-Invariant Face Recognition,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,/paper/look-across-elapse-disentangled,https://arxiv.org/pdf/1809.00338v2.pdf
CARS196,,# 1,[emailÂ protected],79.1,GoogleNet,-,Metric Learning,Hardness-Aware Deep Metric Learning,/paper/hardness-aware-deep-metric-learning,https://arxiv.org/pdf/1903.05503v1.pdf
MS MARCO,,# 2,MRR,0.253,Duet v2,-,Passage Re-Ranking,An Updated Duet Model for Passage Re-ranking,/paper/an-updated-duet-model-for-passage-re-ranking,https://arxiv.org/pdf/1903.07666v1.pdf
CIFAR-10,,# 49,Percentage correct,86.7,An Analysis of Unsupervised Pre-training in Light of Recent Advances,-,Image Classification,An Analysis of Unsupervised Pre-training in Light of Recent Advances,/paper/an-analysis-of-unsupervised-pre-training-in,https://arxiv.org/pdf/1412.6597v4.pdf
STL-10,,# 8,Percentage correct,70.2,An Analysis of Unsupervised Pre-training in Light of Recent Advances,-,Image Classification,An Analysis of Unsupervised Pre-training in Light of Recent Advances,/paper/an-analysis-of-unsupervised-pre-training-in,https://arxiv.org/pdf/1412.6597v4.pdf
BSD68 sigma25,,# 2,PSNR,29.3,N3Net,-,Image Denoising,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
BSD68 sigma50,,# 4,PSNR,26.39,N3Net,-,Image Denoising,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
BSD68 sigma70,,# 3,PSNR,25.14,N3Net,-,Image Denoising,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
Set5 - 4x upscaling,,# 18,PSNR,31.5,N3Net,-,Image Super-Resolution,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
Urban100 sigma30,,# 2,PSNR,30.19,N3Net,-,Image Denoising,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
Urban100 sigma50,,# 4,PSNR,26.82,N3Net,-,Image Denoising,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
Urban100 sigma70,,# 2,PSNR,25.15,N3Net,-,Image Denoising,Neural Nearest Neighbors Networks,/paper/neural-nearest-neighbors-networks,https://arxiv.org/pdf/1810.12575v1.pdf
BlogCatalog,,# 5,Accuracy,20.50%,LINE,-,Node Classification,LINE: Large-scale Information Network Embedding,/paper/line-large-scale-information-network,https://arxiv.org/pdf/1503.03578v1.pdf
BlogCatalog,,# 5,Macro-F1,0.192,LINE,-,Node Classification,LINE: Large-scale Information Network Embedding,/paper/line-large-scale-information-network,https://arxiv.org/pdf/1503.03578v1.pdf
Wikipedia,,# 5,Accuracy,17.50%,LINE,-,Node Classification,LINE: Large-scale Information Network Embedding,/paper/line-large-scale-information-network,https://arxiv.org/pdf/1503.03578v1.pdf
Wikipedia,,# 5,Macro-F1,0.164,LINE,-,Node Classification,LINE: Large-scale Information Network Embedding,/paper/line-large-scale-information-network,https://arxiv.org/pdf/1503.03578v1.pdf
WMT2014 English-French,,# 23,BLEU score,29.03,Regularized LSTM,-,Machine Translation,Recurrent Neural Network Regularization,/paper/recurrent-neural-network-regularization,https://arxiv.org/pdf/1409.2329v5.pdf
MS MARCO,,# 1,MRR,0.359,BERT + Small Training,-,Passage Re-Ranking,Passage Re-ranking with BERT,/paper/passage-re-ranking-with-bert,https://arxiv.org/pdf/1901.04085v4.pdf
Penn Treebank,,# 5,F1 score,94.32,LSTM Encoder-Decoder + LSTM-LM,-,Constituency Parsing,An Empirical Study of Building a Strong Baseline for Constituency Parsing,/paper/an-empirical-study-of-building-a-strong,https://aclweb.org/anthology/P18-2097
D&D,,# 3,Accuracy,71.51%,AWE,-,Graph Classification,Anonymous Walk Embeddings,/paper/anonymous-walk-embeddings,https://arxiv.org/pdf/1805.11921v3.pdf
IMDb-B,,# 1,Accuracy,74.45%,AWE,-,Graph Classification,Anonymous Walk Embeddings,/paper/anonymous-walk-embeddings,https://arxiv.org/pdf/1805.11921v3.pdf
MUTAG,,# 2,Accuracy,87.87%,AWE,-,Graph Classification,Anonymous Walk Embeddings,/paper/anonymous-walk-embeddings,https://arxiv.org/pdf/1805.11921v3.pdf
CoNLL 2003 (English),,# 16,F1,91.24,LM-LSTM-CRF,-,Named Entity Recognition (NER),Empower Sequence Labeling with Task-Aware Neural Language Model,/paper/empower-sequence-labeling-with-task-aware,https://arxiv.org/pdf/1709.04109v4.pdf
Penn Treebank,,# 6,Accuracy,97.53,LM-LSTM-CRF,-,Part-Of-Speech Tagging,Empower Sequence Labeling with Task-Aware Neural Language Model,/paper/empower-sequence-labeling-with-task-aware,https://arxiv.org/pdf/1709.04109v4.pdf
SQuAD1.1,,# 98,EM,71.415,smarnet (single model),-,Question Answering,Smarnet: Teaching Machines to Read and Comprehend Like Human,/paper/smarnet-teaching-machines-to-read-and,https://arxiv.org/pdf/1710.02772v1.pdf
SQuAD1.1,,# 99,F1,80.16,smarnet (single model),-,Question Answering,Smarnet: Teaching Machines to Read and Comprehend Like Human,/paper/smarnet-teaching-machines-to-read-and,https://arxiv.org/pdf/1710.02772v1.pdf
CoNLL 2005,,# 2,F1,87.7,Li et al.,-,Semantic Role Labeling,"Dependency or Span, End-to-End Uniform Semantic Role Labeling",/paper/dependency-or-span-end-to-end-uniform,https://arxiv.org/pdf/1901.05280v1.pdf
OntoNotes,,# 3,F1,86.0,Li et al.,-,Semantic Role Labeling,"Dependency or Span, End-to-End Uniform Semantic Role Labeling",/paper/dependency-or-span-end-to-end-uniform,https://arxiv.org/pdf/1901.05280v1.pdf
COCO,,# 1,MAP,56.6,MSLPD,-,Weakly Supervised Object Detection,Few-Example Object Detection with Model Communication,/paper/few-example-object-detection-with-model,https://arxiv.org/pdf/1706.08249v8.pdf
ImageNet,,# 3,MAP,13.9,MSLPD,-,Weakly Supervised Object Detection,Few-Example Object Detection with Model Communication,/paper/few-example-object-detection-with-model,https://arxiv.org/pdf/1706.08249v8.pdf
PASCAL VOC 2007,,# 11,MAP,41.7,MSLPD,-,Weakly Supervised Object Detection,Few-Example Object Detection with Model Communication,/paper/few-example-object-detection-with-model,https://arxiv.org/pdf/1706.08249v8.pdf
PASCAL VOC 2012,,# 10,MAP,35.4,MSLPD,-,Weakly Supervised Object Detection,Few-Example Object Detection with Model Communication,/paper/few-example-object-detection-with-model,https://arxiv.org/pdf/1706.08249v8.pdf
Restricted,,# 4,F0.5,55.8,Transformer,-,Grammatical Error Correction,Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task,/paper/approaching-neural-grammatical-error-1,https://aclweb.org/anthology/N18-1055
_Restricted_,,# 2,GLEU,59.9,Transformer,-,Grammatical Error Correction,Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task,/paper/approaching-neural-grammatical-error-1,https://aclweb.org/anthology/N18-1055
General,,# 3,MAP,9.37,NLP_HZ,-,Hypernym Discovery,NLP_HZ at SemEval-2018 Task 9: a Nearest Neighbor Approach,/paper/nlp_hz-at-semeval-2018-task-9-a-nearest,https://aclweb.org/anthology/S18-1148
General,,# 5,MRR,17.29,NLP_HZ,-,Hypernym Discovery,NLP_HZ at SemEval-2018 Task 9: a Nearest Neighbor Approach,/paper/nlp_hz-at-semeval-2018-task-9-a-nearest,https://aclweb.org/anthology/S18-1148
General,,# 3,[emailÂ protected],9.19,NLP_HZ,-,Hypernym Discovery,NLP_HZ at SemEval-2018 Task 9: a Nearest Neighbor Approach,/paper/nlp_hz-at-semeval-2018-task-9-a-nearest,https://aclweb.org/anthology/S18-1148
CNN / Daily Mail,,# 14,CNN,67.9,Classifier,-,Question Answering,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,/paper/a-thorough-examination-of-the-cnndaily-mail,https://arxiv.org/pdf/1606.02858v2.pdf
CNN / Daily Mail,,# 9,Daily Mail,68.3,Classifier,-,Question Answering,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,/paper/a-thorough-examination-of-the-cnndaily-mail,https://arxiv.org/pdf/1606.02858v2.pdf
CNN / Daily Mail,,# 3,CNN,77.6,Attentive + relabling + ensemble,-,Question Answering,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,/paper/a-thorough-examination-of-the-cnndaily-mail,https://arxiv.org/pdf/1606.02858v2.pdf
CNN / Daily Mail,,# 3,Daily Mail,79.2,Attentive + relabling + ensemble,-,Question Answering,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,/paper/a-thorough-examination-of-the-cnndaily-mail,https://arxiv.org/pdf/1606.02858v2.pdf
CNN / Daily Mail,,# 11,CNN,72.4,AttentiveReader + bilinear attention,-,Question Answering,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,/paper/a-thorough-examination-of-the-cnndaily-mail,https://arxiv.org/pdf/1606.02858v2.pdf
CNN / Daily Mail,,# 6,Daily Mail,75.8,AttentiveReader + bilinear attention,-,Question Answering,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,/paper/a-thorough-examination-of-the-cnndaily-mail,https://arxiv.org/pdf/1606.02858v2.pdf
COCO,,# 29,Bounding Box AP,36.2,Faster R-CNN + FPN,-,Object Detection,Feature Pyramid Networks for Object Detection,/paper/feature-pyramid-networks-for-object-detection,https://arxiv.org/pdf/1612.03144v2.pdf
TimeBank,,# 1,F1 score,0.511,Catena,-,Temporal Information Extraction,CATENA: CAusal and TEmporal relation extraction from NAtural language texts,/paper/catena-causal-and-temporal-relation,https://aclweb.org/anthology/C16-1007
CNN / Daily Mail,,# 1,CNN,78.6,GA+MAGE (32),-,Question Answering,Linguistic Knowledge as Memory for Recurrent Neural Networks,/paper/linguistic-knowledge-as-memory-for-recurrent,https://arxiv.org/pdf/1703.02620v1.pdf
CMU-SE,,# 1,BLEU-3,0.617,STWGAN-GP,-,Text Generation,Generating Text through Adversarial Training using Skip-Thought Vectors,/paper/generating-text-through-adversarial-training,https://arxiv.org/pdf/1808.08703v2.pdf
RVL-CDIP,,# 2,Accuracy,90.97%,"Transfer Learning from AlexNet, VGG-16, GoogLeNet and ResNet50",-,Document Image Classification,Cutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification,/paper/cutting-the-error-by-half-investigation-of,https://arxiv.org/pdf/1704.03557v1.pdf
Annotated Faces in the Wild,,# 9,AP,0.9597,Conv3D,-,Face Detection,Face Detection with End-to-End Integration of a ConvNet and a 3D Model,/paper/face-detection-with-end-to-end-integration-of,https://arxiv.org/pdf/1606.00850v3.pdf
SemEval 2007 Task 17,,# 4,F1,63.5,"LSTMLP (T:SemCor, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 17,,# 3,F1,63.7,"LSTMLP (T:SemCor, U:OMSTI)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 17,,# 6,F1,60.7,LSTM (T:OMSTI),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 17,,# 2,F1,64.2,LSTM (T:SemCor),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 17,,# 5,F1,63.3,"LSTMLP (T:OMSTI, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 7,,# 4,F1,83.3,"LSTMLP (T:OMSTI, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 7,,# 2,F1,84.3,"LSTMLP (T:SemCor, U:OMSTI)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 7,,# 5,F1,82.8,LSTM (T:SemCor),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 7,,# 6,F1,81.1,LSTM (T:OMSTI),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2007 Task 7,,# 3,F1,83.6,"LSTMLP (T:SemCor, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2013 Task 12,,# 3,F1,68.1,"LSTMLP (T:OMSTI, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2013 Task 12,,# 5,F1,67.3,LSTM (T:OMSTI),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2013 Task 12,,# 8,F1,67.0,LSTM (T:SemCor),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2013 Task 12,,# 4,F1,67.9,"LSTMLP (T:SemCor, U:OMSTI)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SemEval 2013 Task 12,,# 2,F1,69.5,"LSTMLP (T:SemCor, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 2,,# 2,F1,74.4,"LSTMLP (T:OMSTI, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 2,,# 5,F1,73.6,LSTM (T:SemCor),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 2,,# 6,F1,72.4,LSTM (T:OMSTI),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 2,,# 3,F1,73.9,"LSTMLP (T:SemCor, U:OMSTI)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 2,,# 4,F1,73.8,"LSTMLP (T:SemCor, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 3 Task 1,,# 9,F1,69.2,LSTM (T:SemCor),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 3 Task 1,,# 10,F1,64.3,LSTM (T:OMSTI),-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 3 Task 1,,# 2,F1,71.1,"LSTMLP (T:SemCor, U:OMSTI)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 3 Task 1,,# 1,F1,71.8,"LSTMLP (T:SemCor, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
SensEval 3 Task 1,,# 3,F1,71.0,"LSTMLP (T:OMSTI, U:1K)",-,Word Sense Disambiguation,Semi-supervised Word Sense Disambiguation with Neural Models,/paper/semi-supervised-word-sense-disambiguation-1,https://arxiv.org/pdf/1603.07012v2.pdf
Atari 2600 Montezuma's Revenge,,# 2,Score,6635,A2C+CoEX,-,Atari Games,Contingency-Aware Exploration in Reinforcement Learning,/paper/contingency-aware-exploration-in,https://arxiv.org/pdf/1811.01483v3.pdf
SNLI,,# 26,% Test Accuracy,86.3,Distance-based Self-Attention Network,-,Natural Language Inference,Distance-based Self-Attention Network for Natural Language Inference,/paper/distance-based-self-attention-network-for,https://arxiv.org/pdf/1712.02047v1.pdf
SNLI,,# 34,% Train Accuracy,89.6,Distance-based Self-Attention Network,-,Natural Language Inference,Distance-based Self-Attention Network for Natural Language Inference,/paper/distance-based-self-attention-network-for,https://arxiv.org/pdf/1712.02047v1.pdf
SNLI,,# 1,Parameters,4.7m,Distance-based Self-Attention Network,-,Natural Language Inference,Distance-based Self-Attention Network for Natural Language Inference,/paper/distance-based-self-attention-network-for,https://arxiv.org/pdf/1712.02047v1.pdf
Caltech,,# 5,Reasonable Miss Rate,5.0,RepLoss,-,Pedestrian Detection,Repulsion Loss: Detecting Pedestrians in a Crowd,/paper/repulsion-loss-detecting-pedestrians-in-a,https://arxiv.org/pdf/1711.07752v2.pdf
Caltech,,# 2,Reasonable Miss Rate,4.0,RepLoss + CityPersons dataset,-,Pedestrian Detection,Repulsion Loss: Detecting Pedestrians in a Crowd,/paper/repulsion-loss-detecting-pedestrians-in-a,https://arxiv.org/pdf/1711.07752v2.pdf
CityPersons,,# 4,Reasonable MR^-2,13.2,RepLoss,-,Pedestrian Detection,Repulsion Loss: Detecting Pedestrians in a Crowd,/paper/repulsion-loss-detecting-pedestrians-in-a,https://arxiv.org/pdf/1711.07752v2.pdf
CityPersons,,# 6,Heavy MR^-2,56.9,RepLoss,-,Pedestrian Detection,Repulsion Loss: Detecting Pedestrians in a Crowd,/paper/repulsion-loss-detecting-pedestrians-in-a,https://arxiv.org/pdf/1711.07752v2.pdf
CityPersons,,# 5,Partial MR^-2,16.8,RepLoss,-,Pedestrian Detection,Repulsion Loss: Detecting Pedestrians in a Crowd,/paper/repulsion-loss-detecting-pedestrians-in-a,https://arxiv.org/pdf/1711.07752v2.pdf
CityPersons,,# 3,Bare MR^-2,7.6,RepLoss,-,Pedestrian Detection,Repulsion Loss: Detecting Pedestrians in a Crowd,/paper/repulsion-loss-detecting-pedestrians-in-a,https://arxiv.org/pdf/1711.07752v2.pdf
Douban,,# 4,RMSE,0.738,Factorized Exchangeable Autoencoder,-,Collaborative Filtering,Deep Models of Interactions Across Sets,/paper/deep-models-of-interactions-across-sets,https://arxiv.org/pdf/1803.02879v2.pdf
Flixster,,# 1,RMSE,0.9079999999999999,Factorized Exchangeable Autoencoder,-,Collaborative Filtering,Deep Models of Interactions Across Sets,/paper/deep-models-of-interactions-across-sets,https://arxiv.org/pdf/1803.02879v2.pdf
MovieLens 100K,,# 2,RMSE,0.91,Self-Supervised Exchangeable Model,-,Collaborative Filtering,Deep Models of Interactions Across Sets,/paper/deep-models-of-interactions-across-sets,https://arxiv.org/pdf/1803.02879v2.pdf
YahooMusic,,# 1,RMSE,20.0,Factorized Exchangeable Autoencoder,-,Collaborative Filtering,Deep Models of Interactions Across Sets,/paper/deep-models-of-interactions-across-sets,https://arxiv.org/pdf/1803.02879v2.pdf
BSD100 - 4x upscaling,,# 25,PSNR,26.91,FAFR*,-,Image Super-Resolution,Image Super-resolution via Feature-augmented Random Forest,/paper/image-super-resolution-via-feature-augmented,https://arxiv.org/pdf/1712.05248v1.pdf
Set14 - 4x upscaling,,# 32,PSNR,27.48,FAFR*,-,Image Super-Resolution,Image Super-resolution via Feature-augmented Random Forest,/paper/image-super-resolution-via-feature-augmented,https://arxiv.org/pdf/1712.05248v1.pdf
Set5 - 4x upscaling,,# 27,PSNR,30.45,FARF*,-,Image Super-Resolution,Image Super-resolution via Feature-augmented Random Forest,/paper/image-super-resolution-via-feature-augmented,https://arxiv.org/pdf/1712.05248v1.pdf
SemEval 2014 Task 4 Sub Task 2,,# 7,Restaurant (Acc),81.3,BBLSTM-SL,-,Aspect-Based Sentiment Analysis,Aspect-Based Sentiment Analysis Using Bitmask Bidirectional Long Short Term Memory Networks,/paper/aspect-based-sentiment-analysis-using-bitmask,https://aaai.org/ocs/index.php/FLAIRS/FLAIRS18/paper/view/17646/16840
SemEval 2014 Task 4 Sub Task 2,,# 16,Laptop (Acc),74.9,BBLSTM-SL,-,Aspect-Based Sentiment Analysis,Aspect-Based Sentiment Analysis Using Bitmask Bidirectional Long Short Term Memory Networks,/paper/aspect-based-sentiment-analysis-using-bitmask,https://aaai.org/ocs/index.php/FLAIRS/FLAIRS18/paper/view/17646/16840
Amazon Review Full,,# 6,Accuracy,61.65,SRNN,-,Sentiment Analysis,Sliced Recurrent Neural Networks,/paper/sliced-recurrent-neural-networks,https://arxiv.org/pdf/1807.02291v1.pdf
Amazon Review Polarity,,# 6,Accuracy,95.26,SRNN,-,Sentiment Analysis,Sliced Recurrent Neural Networks,/paper/sliced-recurrent-neural-networks,https://arxiv.org/pdf/1807.02291v1.pdf
Yelp Binary classification,,# 10,Error,3.96,SRNN,-,Sentiment Analysis,Sliced Recurrent Neural Networks,/paper/sliced-recurrent-neural-networks,https://arxiv.org/pdf/1807.02291v1.pdf
HANDS 2017,,# 1,Average 3D Error,9.95,V2V-PoseNet,-,Hand Pose Estimation,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,/paper/v2v-posenet-voxel-to-voxel-prediction-network,https://arxiv.org/pdf/1711.07399v3.pdf
ICVL Hands,,# 1,Average 3D Error,6.28,V2V-PoseNet,-,Hand Pose Estimation,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,/paper/v2v-posenet-voxel-to-voxel-prediction-network,https://arxiv.org/pdf/1711.07399v3.pdf
ITOP front-view,,# 1,Mean mAP,88.74,V2V-PoseNet,-,Pose Estimation,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,/paper/v2v-posenet-voxel-to-voxel-prediction-network,https://arxiv.org/pdf/1711.07399v3.pdf
ITOP top-view,,# 1,Mean mAP,83.44,V2V-PoseNet,-,Pose Estimation,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,/paper/v2v-posenet-voxel-to-voxel-prediction-network,https://arxiv.org/pdf/1711.07399v3.pdf
MSRA Hands,,# 2,Average 3D Error,7.49,V2V-PoseNet,-,Hand Pose Estimation,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,/paper/v2v-posenet-voxel-to-voxel-prediction-network,https://arxiv.org/pdf/1711.07399v3.pdf
NYU Hands,,# 1,Average 3D Error,8.42,V2V-PoseNet,-,Hand Pose Estimation,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,/paper/v2v-posenet-voxel-to-voxel-prediction-network,https://arxiv.org/pdf/1711.07399v3.pdf
SST-5 Fine-grained classification,,# 8,Accuracy,50.4,Bi-LSTM+2+5,-,Sentiment Analysis,Leveraging Multi-grained Sentiment Lexicon Information for Neural Sequence Models,/paper/leveraging-multi-grained-sentiment-lexicon,https://arxiv.org/pdf/1812.01527v1.pdf
One Billion Word,,# 12,PPL,36.0,BIG G-LSTM-2,-,Language Modelling,Factorization tricks for LSTM networks,/paper/factorization-tricks-for-lstm-networks,https://arxiv.org/pdf/1703.10722v3.pdf
CIFAR-10,,# 2,Inception score,8.29,MMD-GAN-rep,-,Image Generation,Improving MMD-GAN Training with Repulsive Loss Function,/paper/improving-mmd-gan-training-with-repulsive,https://arxiv.org/pdf/1812.09916v4.pdf
CIFAR-10,,# 1,FID,16.21,MMD-GAN-rep,-,Image Generation,Improving MMD-GAN Training with Repulsive Loss Function,/paper/improving-mmd-gan-training-with-repulsive,https://arxiv.org/pdf/1812.09916v4.pdf
RaFD,,# 2,Classification Error,4.10%,DIA,-,Image-to-Image Translation,Deep Identity-aware Transfer of Facial Attributes,/paper/deep-identity-aware-transfer-of-facial,https://arxiv.org/pdf/1610.05586v2.pdf
CNN / Daily Mail,,# 7,CNN,74.7,ReasoNet,-,Question Answering,ReasoNet: Learning to Stop Reading in Machine Comprehension,/paper/reasonet-learning-to-stop-reading-in-machine,https://arxiv.org/pdf/1609.05284v3.pdf
CNN / Daily Mail,,# 5,Daily Mail,76.6,ReasoNet,-,Question Answering,ReasoNet: Learning to Stop Reading in Machine Comprehension,/paper/reasonet-learning-to-stop-reading-in-machine,https://arxiv.org/pdf/1609.05284v3.pdf
SQuAD1.1,,# 73,EM,75.03399999999999,ReasoNet (ensemble),-,Question Answering,ReasoNet: Learning to Stop Reading in Machine Comprehension,/paper/reasonet-learning-to-stop-reading-in-machine,https://arxiv.org/pdf/1609.05284v3.pdf
SQuAD1.1,,# 77,F1,82.552,ReasoNet (ensemble),-,Question Answering,ReasoNet: Learning to Stop Reading in Machine Comprehension,/paper/reasonet-learning-to-stop-reading-in-machine,https://arxiv.org/pdf/1609.05284v3.pdf
SQuAD1.1,,# 109,EM,70.555,ReasoNet (single model),-,Question Answering,ReasoNet: Learning to Stop Reading in Machine Comprehension,/paper/reasonet-learning-to-stop-reading-in-machine,https://arxiv.org/pdf/1609.05284v3.pdf
SQuAD1.1,,# 108,F1,79.36399999999999,ReasoNet (single model),-,Question Answering,ReasoNet: Learning to Stop Reading in Machine Comprehension,/paper/reasonet-learning-to-stop-reading-in-machine,https://arxiv.org/pdf/1609.05284v3.pdf
Atari 2600 Beam Rider,,# 19,Score,5184,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Atari 2600 Breakout,,# 20,Score,225,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Atari 2600 Enduro,,# 14,Score,661,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Atari 2600 Pong,,# 1,Score,21,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Atari 2600 Q*Bert,,# 21,Score,4500,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Atari 2600 Seaquest,,# 17,Score,1740,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Atari 2600 Space Invaders,,# 20,Score,1075,DQN best,-,Atari Games,Playing Atari with Deep Reinforcement Learning,/paper/playing-atari-with-deep-reinforcement,https://arxiv.org/pdf/1312.5602v1.pdf
Mini-ImageNet - 1-Shot Learning,,# 1,Accuracy,62.64%,MetaOptNet-SVM,-,Few-Shot Image Classification,Meta-Learning with Differentiable Convex Optimization,/paper/meta-learning-with-differentiable-convex,https://arxiv.org/pdf/1904.03758v2.pdf
Mini-ImageNet - 5-Shot Learning,,# 1,Accuracy,78.63%,MetaOptNet-SVM,-,Few-Shot Image Classification,Meta-Learning with Differentiable Convex Optimization,/paper/meta-learning-with-differentiable-convex,https://arxiv.org/pdf/1904.03758v2.pdf
swb_hub_500 WER fullSWBCH,,# 1,Percentage error,10.3,ResNet + BiLSTMs acoustic model,-,Speech Recognition,English Conversational Telephone Speech Recognition by Humans and Machines,/paper/english-conversational-telephone-speech,https://arxiv.org/pdf/1703.02136v1.pdf
Switchboard + Hub500,,# 1,Percentage error,5.5,ResNet + BiLSTMs acoustic model,-,Speech Recognition,English Conversational Telephone Speech Recognition by Humans and Machines,/paper/english-conversational-telephone-speech,https://arxiv.org/pdf/1703.02136v1.pdf
ImageCLEF-DA,,# 2,Accuracy,88.9,IAFN+ENT,-,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation: An Adaptive Feature Norm Approach,/paper/unsupervised-domain-adaptation-an-adaptive,https://arxiv.org/pdf/1811.07456v1.pdf
Office-31,,# 1,Accuracy,87.1,IAFN+ENT,-,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation: An Adaptive Feature Norm Approach,/paper/unsupervised-domain-adaptation-an-adaptive,https://arxiv.org/pdf/1811.07456v1.pdf
VisDA2017,,# 1,Accuracy,76.1,IAFN,-,Unsupervised Domain Adaptation,Unsupervised Domain Adaptation: An Adaptive Feature Norm Approach,/paper/unsupervised-domain-adaptation-an-adaptive,https://arxiv.org/pdf/1811.07456v1.pdf
CIFAR-10,,# 9,NLL Test,4.0,Deep GMM,-,Image Generation,Factoring Variations in Natural Images with Deep Gaussian Mixture Models,/paper/factoring-variations-in-natural-images-with,https://papers.nips.cc/paper/5227-factoring-variations-in-natural-images-with-deep-gaussian-mixture-models.pdf
CoNLL 2005,,# 4,F1,82.5,He et al. 2018,-,Semantic Role Labeling (predicted predicates),Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,/paper/jointly-predicting-predicates-and-arguments-1,https://aclweb.org/anthology/P18-2058
CoNLL 2005,,# 2,F1,86.0,He et al. 2018 + ELMo,-,Semantic Role Labeling (predicted predicates),Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,/paper/jointly-predicting-predicates-and-arguments-1,https://aclweb.org/anthology/P18-2058
CoNLL 2012,,# 4,F1,79.8,He et al. 2018,-,Semantic Role Labeling (predicted predicates),Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,/paper/jointly-predicting-predicates-and-arguments-1,https://aclweb.org/anthology/P18-2058
CoNLL 2012,,# 2,F1,82.9,He et al. 2018 + ELMo,-,Semantic Role Labeling (predicted predicates),Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,/paper/jointly-predicting-predicates-and-arguments-1,https://aclweb.org/anthology/P18-2058
OntoNotes,,# 4,F1,85.5,"He et al.,",-,Semantic Role Labeling,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,/paper/jointly-predicting-predicates-and-arguments-1,https://aclweb.org/anthology/P18-2058
OntoNotes,,# 7,F1,82.1,He et al.,-,Semantic Role Labeling,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,/paper/jointly-predicting-predicates-and-arguments-1,https://aclweb.org/anthology/P18-2058
Labeled Faces in the Wild,,# 1,Accuracy,99.83%,"ArcFace + MS1MV2 + R100,",-,Face Verification,ArcFace: Additive Angular Margin Loss for Deep Face Recognition,/paper/arcface-additive-angular-margin-loss-for-deep,https://arxiv.org/pdf/1801.07698v3.pdf
MegaFace,,# 1,Accuracy,98.48%,ArcFace + MS1MV2 + R100 + R,-,Face Verification,ArcFace: Additive Angular Margin Loss for Deep Face Recognition,/paper/arcface-additive-angular-margin-loss-for-deep,https://arxiv.org/pdf/1801.07698v3.pdf
MegaFace,,# 1,Accuracy,98.35%,ArcFace + MS1MV2 + R100 + R,-,Face Identification,ArcFace: Additive Angular Margin Loss for Deep Face Recognition,/paper/arcface-additive-angular-margin-loss-for-deep,https://arxiv.org/pdf/1801.07698v3.pdf
YouTube Faces DB,,# 2,Accuracy,98.02%,"ArcFace + MS1MV2 + R100,",-,Face Verification,ArcFace: Additive Angular Margin Loss for Deep Face Recognition,/paper/arcface-additive-angular-margin-loss-for-deep,https://arxiv.org/pdf/1801.07698v3.pdf
Penn Treebank (Character Level),,# 2,Bit per Character (BPC),1.169,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
Penn Treebank (Character Level),,# 1,Number of params,13.8M,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
Penn Treebank (Word Level),,# 2,Validation perplexity,48.0,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
Penn Treebank (Word Level),,# 4,Test perplexity,47.3,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
Penn Treebank (Word Level),,# 1,Params,22M,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
WikiText-2,,# 2,Validation perplexity,42.0,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
WikiText-2,,# 3,Test perplexity,40.3,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
WikiText-2,,# 1,Number of params,35M,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.,-,Language Modelling,Improved Language Modeling by Decoding the Past,/paper/improved-language-modeling-by-decoding-the,https://arxiv.org/pdf/1808.05908v4.pdf
Caltech,,# 16,Reasonable Miss Rate,17.1,Checkerboards+,-,Pedestrian Detection,Filtered Channel Features for Pedestrian Detection,/paper/filtered-channel-features-for-pedestrian,https://arxiv.org/pdf/1501.05759v1.pdf
TACRED,,# 3,F1,66.4,C-GCN,-,Relation Extraction,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,/paper/graph-convolution-over-pruned-dependency,https://arxiv.org/pdf/1809.10185v1.pdf
AG News,,# 13,Error,8.67,VDCN,-,Text Classification,Very Deep Convolutional Networks for Text Classification,/paper/very-deep-convolutional-networks-for-text,https://arxiv.org/pdf/1606.01781v2.pdf
DBpedia,,# 13,Error,1.29,VDCN,-,Text Classification,Very Deep Convolutional Networks for Text Classification,/paper/very-deep-convolutional-networks-for-text,https://arxiv.org/pdf/1606.01781v2.pdf
Leeds Sports Poses,,# 3,PCK,90.5%,Soft-argmax + contextual information,-,Pose Estimation,Human Pose Regression by Combining Indirect Part Detection and Contextual Information,/paper/human-pose-regression-by-combining-indirect,https://arxiv.org/pdf/1710.02322v1.pdf
CoNLL 2003 (English),,# 3,F1,92.8,BERT Large,-,Named Entity Recognition (NER),BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoNLL 2003 (English),,# 5,F1,92.4,BERT Base,-,Named Entity Recognition (NER),BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoQA,,# 3,In-domain,79.8,BERT-base finetune (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoQA,,# 3,Out-of-domain,74.1,BERT-base finetune (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoQA,,# 3,Overall,78.1,BERT-base finetune (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoQA,,# 1,In-domain,82.5,BERT Large Augmented (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoQA,,# 1,Out-of-domain,77.6,BERT Large Augmented (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
CoQA,,# 1,Overall,81.1,BERT Large Augmented (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
Quora Question Pairs,,# 1,Accuracy,72.1%,BERT (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SciCite,,# 2,F1,84.4,BERT,-,Sentence Classification,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SciTail,,# 2,Accuracy,92.0,BERT,-,Natural Language Inference,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SQuAD1.1,,# 1,EM,87.433,BERT (ensemble),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SQuAD1.1,,# 1,F1,93.160,BERT (ensemble),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SQuAD1.1,,# 4,EM,85.083,BERT (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SQuAD1.1,,# 3,F1,91.835,BERT (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SQuAD2.0,,# 41,EM,80.005,BERT (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SQuAD2.0,,# 43,F1,83.061,BERT (single model),-,Question Answering,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SWAG,,# 1,Dev,86.6,BERT Large,-,Common Sense Reasoning,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SWAG,,# 1,Test,86.3,BERT Large,-,Common Sense Reasoning,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SWAG,,# 2,Dev,81.6,BERT Base,-,Common Sense Reasoning,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
SWAG,,# 4,Test,-,BERT Base,-,Common Sense Reasoning,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
XNLI Zero-Shot English-to-German,,# 2,Accuracy,70.5%,BERT,-,Cross-Lingual Natural Language Inference,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
XNLI Zero-Shot English-to-Spanish,,# 1,Accuracy,74.3%,BERT,-,Cross-Lingual Natural Language Inference,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional,https://arxiv.org/pdf/1810.04805v1.pdf
COCO,,# 4,Bounding Box AP,46.1,SNIPER,-,Object Detection,SNIPER: Efficient Multi-Scale Training,/paper/sniper-efficient-multi-scale-training,https://arxiv.org/pdf/1805.09300v3.pdf
PASCAL VOC 2007,,# 1,MAP,86.9%,SNIPER,-,Object Detection,SNIPER: Efficient Multi-Scale Training,/paper/sniper-efficient-multi-scale-training,https://arxiv.org/pdf/1805.09300v3.pdf
SST-2 Binary classification,,# 23,Accuracy,54.72,Joined Model Multi-tasking,-,Sentiment Analysis,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,/paper/exploring-joint-neural-model-for-sentence,https://aclweb.org/anthology/W17-5535
SST-5 Fine-grained classification,,# 15,Accuracy,44.82,Joined Model Multi-tasking,-,Sentiment Analysis,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,/paper/exploring-joint-neural-model-for-sentence,https://aclweb.org/anthology/W17-5535
20NEWS,,# 1,Accuracy,87.91,RMDL,-,Document Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
20NEWS,,# 1,Accuracy,91.21,RMDL,-,Multi-Label Text Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
CIFAR-10,,# 37,Percentage correct,91.21,RMDL,-,Image Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
CIFAR-10,,# 16,Percentage error,8.79,RMDL,-,Image Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
IMDb,,# 1,Accuracy,90.79,RMDL,-,Document Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
MNIST,,# 1,Percentage error,0.18,RMDL,-,Image Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
Olivetti Faces 5 Image,,# 1,Accuracy,95.0,RMDL,-,Face Recognition,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
Reuters-21578,,# 1,Accuracy,90.69,RMDL,-,Document Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
WOS-11967,,# 1,Accuracy,91.59,RMDL,-,Document Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
WOS-46985,,# 1,Accuracy,90.69,RMDL,-,Document Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
WOS-5736,,# 1,Accuracy,93.57,RMDL,-,Document Classification,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for,https://arxiv.org/pdf/1805.01890v2.pdf
DukeMTMC-reID,,# 13,Rank-1,71.59,PAN,-,Person Re-Identification,Pedestrian Alignment Network for Large-scale Person Re-identification,/paper/pedestrian-alignment-network-for-large-scale,https://arxiv.org/pdf/1707.00408v1.pdf
DukeMTMC-reID,,# 14,MAP,51.51,PAN,-,Person Re-Identification,Pedestrian Alignment Network for Large-scale Person Re-identification,/paper/pedestrian-alignment-network-for-large-scale,https://arxiv.org/pdf/1707.00408v1.pdf
CIFAR-100,,# 33,Percentage correct,67.4,HD-CNN,-,Image Classification,HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition,/paper/hd-cnn-hierarchical-deep-convolutional-neural,https://arxiv.org/pdf/1410.0736v4.pdf
CIFAR-100,,# 10,Percentage error,32.62,HD-CNN,-,Image Classification,HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition,/paper/hd-cnn-hierarchical-deep-convolutional-neural,https://arxiv.org/pdf/1410.0736v4.pdf
Chinese License Plates,,# 1,Accuracy,95.0%,LPRNet basic,-,License Plate Recognition,LPRNet: License Plate Recognition via Deep Neural Networks,/paper/lprnet-license-plate-recognition-via-deep,https://arxiv.org/pdf/1806.10447v1.pdf
Chinese License Plates,,# 1,GFLOPs,0.34,LPRNet basic,-,License Plate Recognition,LPRNet: License Plate Recognition via Deep Neural Networks,/paper/lprnet-license-plate-recognition-via-deep,https://arxiv.org/pdf/1806.10447v1.pdf
DukeMTMC-reID,,# 5,Rank-1,80.0,Incremental Learning,-,Person Re-Identification,Incremental Learning in Person Re-Identification,/paper/incremental-learning-in-person-re,https://arxiv.org/pdf/1808.06281v3.pdf
DukeMTMC-reID,,# 6,MAP,60.2,Incremental Learning,-,Person Re-Identification,Incremental Learning in Person Re-Identification,/paper/incremental-learning-in-person-re,https://arxiv.org/pdf/1808.06281v3.pdf
Market-1501,,# 7,Rank-1,89.3,Incremental Learning,-,Person Re-Identification,Incremental Learning in Person Re-Identification,/paper/incremental-learning-in-person-re,https://arxiv.org/pdf/1808.06281v3.pdf
Market-1501,,# 6,MAP,71.8,Incremental Learning,-,Person Re-Identification,Incremental Learning in Person Re-Identification,/paper/incremental-learning-in-person-re,https://arxiv.org/pdf/1808.06281v3.pdf
Librispeech,,# 1,Word Error Rate (WER),11.1,VoiceFilter: bi-LSTM,-,Speech Recognition,VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking,/paper/voicefilter-targeted-voice-separation-by,https://arxiv.org/pdf/1810.04826v4.pdf
Citeseer,,# 7,Accuracy,64.7%,Planetoid*,-,Node Classification,Revisiting Semi-Supervised Learning with Graph Embeddings,/paper/revisiting-semi-supervised-learning-with,https://arxiv.org/pdf/1603.08861v2.pdf
Cora,,# 8,Accuracy,75.7%,Planetoid*,-,Node Classification,Revisiting Semi-Supervised Learning with Graph Embeddings,/paper/revisiting-semi-supervised-learning-with,https://arxiv.org/pdf/1603.08861v2.pdf
Cora,,# 6,Accuracy,75.7%,Planetoid*,-,Document Classification,Revisiting Semi-Supervised Learning with Graph Embeddings,/paper/revisiting-semi-supervised-learning-with,https://arxiv.org/pdf/1603.08861v2.pdf
NELL,,# 2,Accuracy,61.9%,Planetoid*,-,Node Classification,Revisiting Semi-Supervised Learning with Graph Embeddings,/paper/revisiting-semi-supervised-learning-with,https://arxiv.org/pdf/1603.08861v2.pdf
Pubmed,,# 5,Accuracy,77.2%,Planetoid*,-,Node Classification,Revisiting Semi-Supervised Learning with Graph Embeddings,/paper/revisiting-semi-supervised-learning-with,https://arxiv.org/pdf/1603.08861v2.pdf
