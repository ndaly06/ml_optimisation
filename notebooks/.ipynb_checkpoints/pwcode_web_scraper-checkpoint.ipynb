{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports the required packages\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return_headings method\n",
    "\n",
    "\"\"\"\n",
    "def return_headings(url, heading_tag):\n",
    "    \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    # searches for all h4 headings\n",
    "    search_headings = soup.findAll(heading_tag)\n",
    "    \n",
    "    # empty area headings list\n",
    "    headings = []\n",
    "    \n",
    "    # appends each area heading to the area_headings list\n",
    "    for div in search_headings:\n",
    "        headings.append(div.text)\n",
    "\n",
    "    # removes the white space from the headings list\n",
    "    headings = list(map(str.strip, headings))\n",
    "\n",
    "    # list comphrehension for lower casing each string in the area_headings list\n",
    "    headings = [x.lower() for x in headings]\n",
    "\n",
    "    # list comprehension that replaces the white space with an dash in the area_headings list\n",
    "    headings = [x.replace(\" \", \"-\") for x in headings]\n",
    "    \n",
    "    # returns the headings array\n",
    "    return headings\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "return_dataset method\n",
    "\n",
    "\"\"\"\n",
    "def return_dataset(url, dataset_tag):\n",
    "    \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    #\n",
    "    search_datasets = soup.findAll('div', attrs = {'class': dataset_tag})\n",
    "    \n",
    "    # empty area headings list\n",
    "    task_datasets = []\n",
    "    \n",
    "    # appends each task dataset to the task_datasets list\n",
    "    for div in search_datasets:\n",
    "        task_datasets.append(div.text)\n",
    "\n",
    "    # removes the white space from the task_datasets list\n",
    "    task_datasets = list(map(str.strip, task_datasets))\n",
    "\n",
    "    # list comphrension for lower casing each string in the task_datasets list\n",
    "    task_datasets = [x.lower() for x in task_datasets]\n",
    "\n",
    "    # list comphrension for replacing the white space with an dash in the task_datasets list\n",
    "    task_datasets = [x.replace(\" \", \"-\") for x in task_datasets]\n",
    "    \n",
    "    # removes the brace in the task_datasets list\n",
    "    task_datasets = [x.replace(\"(\", \"\") for x in task_datasets]\n",
    "    task_datasets = [x.replace(\")\", \"\") for x in task_datasets]\n",
    "    \n",
    "    # returns the task_datasets list\n",
    "    return task_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer-vision', 'natural-language-processing', 'medical', 'methodology', 'miscellaneous', 'speech', 'playing-games', 'graphs', 'time-series', 'audio', 'robots', 'music', 'computer-code', 'reasoning', 'knowledge-base', 'adversarial']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "url = 'https://paperswithcode.com/sota'\n",
    "heading_tag = 'h4'\n",
    "\n",
    "# invokes the return_headings function to return each of the area headings\n",
    "area_headings = return_headings(url, heading_tag)\n",
    "\n",
    "print(area_headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "# initialises an empty task headings list\n",
    "task_headings = []\n",
    "    \n",
    "# iterates through each of the area headings\n",
    "for i in range(len(area_headings)):\n",
    "    \n",
    "    url = 'https://paperswithcode.com/area/' + area_headings[i]\n",
    "    heading_tag = 'h4'\n",
    "    \n",
    "    # invokes the return_headings function to return \n",
    "    # and append each of the task headings to the task_headings list\n",
    "    task_headings.append(return_headings(url, heading_tag))\n",
    "    \n",
    "# converts the resulting 2d array into a 1d array using list comprehension\n",
    "# task_headings = [s for S in task_headings for s in S]\n",
    "\n",
    "# prints the first 10 elements in the task_headings list\n",
    "print(task_headings[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts the sub-tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialises an empty subtask headings list\n",
    "subtask_headings = []\n",
    "    \n",
    "# iterates through each of the area headings\n",
    "for i in range(len(area_headings)):\n",
    "    \n",
    "    # iterates through each of the corresponding subtask headings\n",
    "    for j in range(len(task_headings[i])):\n",
    "        \n",
    "        url = 'https://paperswithcode.com/area/' + area_headings[i] + '/' + task_headings[i][j] \n",
    "        heading_tag = 'h1'\n",
    "        \n",
    "        # invokes the return_headings function to return \n",
    "        # and append each of the subtask headings to the subtask_headings list\n",
    "        subtask_headings.append(return_headings(url, heading_tag))\n",
    "        \n",
    "# converts the resulting 2d list into a 1d list using list comprehension\n",
    "subtask_headings = [s for S in subtask_headings for s in S]\n",
    "\n",
    "# list comprehension for removing duplicate subtask headings\n",
    "subtask_headings = [ x for x in subtask_headings if \"-subtasks\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic-segmentation',\n",
       " 'real-time-semantic-segmentation',\n",
       " 'scene-segmentation',\n",
       " '3d-part-segmentation',\n",
       " 'weakly-supervised-semantic-segmentation']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the first 5 elements in the subtask_headings list\n",
    "subtask_headings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the dataset relating to each sub-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "subtask_datasets = []\n",
    "\n",
    "# iterates through each \n",
    "for i in range(len(subtask_headings)):\n",
    "    \n",
    "    #\n",
    "    url = \"https://paperswithcode.com/task/\" + subtask_headings[i]\n",
    "    html = requests.get(url).content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     print(url)\n",
    "\n",
    "    # nested for loop\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"/sota/\" + subtask_headings[i])}):\n",
    "        \n",
    "        # \n",
    "        subtask_datasets.append(link.get('href'))\n",
    "        \n",
    "        # remove duplicates from the subtasks_dataset list\n",
    "        subtask_datasets  = list(set(subtask_datasets))\n",
    "\n",
    "# sorts the subtask_datasets list\n",
    "subtask_datasets = sorted(subtask_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialises an empty data list\n",
    "data = []\n",
    "\n",
    "# iterates through element in the subtask_datasets list\n",
    "for i in range(len(subtask_datasets)):\n",
    "    \n",
    "    #\n",
    "    url = \"https://paperswithcode.com\" + subtask_datasets[i]  \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    # extracts the json data from the evaluation table on each page \n",
    "    # data.append(subtask_datasets[i])\n",
    "    data.append(json.loads(soup.find('script', id = 'evaluation-table-data').text))\n",
    "    \n",
    "#     print(url)\n",
    "\n",
    "# list comprehension that converts the resulting 2d array into a 1d array\n",
    "data = [s for S in data for s in S]\n",
    "\n",
    "# normalizes the json in the data array and creates a pandas dataframe\n",
    "papers = json_normalize(data)\n",
    "\n",
    "# converts the papers.url column to a list\n",
    "papers = papers['paper.url'].tolist()\n",
    "\n",
    "# removes None items from the papers list\n",
    "papers = list(filter(None.__ne__, papers))\n",
    "\n",
    "# remove duplicates from the papers list\n",
    "papers  = list(set(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/paper/detecting-oriented-text-in-natural-images-by',\n",
       " '/paper/neural-semantic-encoders',\n",
       " '/paper/pythia-v01-the-winning-entry-to-the-vqa',\n",
       " '/paper/large-scale-gan-training-for-high-fidelity',\n",
       " '/paper/margin-based-parallel-corpus-mining-with',\n",
       " '/paper/probabilistic-model-agnostic-meta-learning',\n",
       " '/paper/strong-baselines-for-neural-semi-supervised',\n",
       " '/paper/esrgan-enhanced-super-resolution-generative',\n",
       " '/paper/semi-supervised-sequence-modeling-with-cross',\n",
       " '/paper/mixing-context-granularities-for-improved']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the 1st 10 paper paths\n",
    "papers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts the models, corresponding paper titles & urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialises an empty tables list\n",
    "tables = []\n",
    "\n",
    "# initialises an empty paper titles list\n",
    "paper_titles = []\n",
    "\n",
    "# initialises an empty paper urls list\n",
    "paper_urls = []\n",
    "\n",
    "# iterates through each item in the \n",
    "for i in range(len(papers)):\n",
    "    \n",
    "    # prints the url and count\n",
    "#     print(url, i)\n",
    "    \n",
    "    #\n",
    "    url = \"https://paperswithcode.com\" + papers[i]\n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    # appends each table to the tables list\n",
    "    tables.append(soup.findAll('table')[0])\n",
    "    \n",
    "    # searches for the url of each research paper\n",
    "    paper_url_search = soup.findAll(href=re.compile(\"\\.pdf\"))\n",
    "    \n",
    "    # searches for the title of each research paper\n",
    "    paper_title_search = soup.findAll('div', attrs = {'class': 'paper-title'})\n",
    "    \n",
    "    # appends each paper title to the paper_titles list\n",
    "    for t in paper_title_search:\n",
    "        paper_title = t.h1\n",
    "        paper_titles.append(paper_title.text)\n",
    "    \n",
    "    # appends each paper url to the paper_urls list\n",
    "    for p in paper_url_search:\n",
    "        \n",
    "        # if url is found append url, else return none\n",
    "        paper_url = p.get('href')\n",
    "    \n",
    "        if paper_url is not None:\n",
    "            paper_urls.append(paper_url)\n",
    "        \n",
    "        else:\n",
    "            paper_urls.append('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 1319, 1212, 1319)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks that each list is of equal length\n",
    "len(tables), len(paper_titles), len(paper_urls), len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the tables html list to a text file\n",
    "with open('/Users/nialdaly/Documents/ml_optimisation/data/tables.txt', 'w') as f:\n",
    "    for item in tables:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# saves the papers list to a text file\n",
    "with open('/Users/nialdaly/Documents/ml_optimisation/data/papers.txt', 'wb') as fp:\n",
    "    pickle.dump(papers, fp)\n",
    "\n",
    "# saves the papers_titles list to a text file\n",
    "with open('/Users/nialdaly/Documents/ml_optimisation/data/paper_titles.txt', 'wb') as fp:\n",
    "    pickle.dump(paper_titles, fp)\n",
    "    \n",
    "# saves the papers_urls list to a text file\n",
    "with open('/Users/nialdaly/Documents/ml_optimisation/data/paper_urls.txt', 'wb') as fp:\n",
    "    pickle.dump(paper_urls, fp)\n",
    "    \n",
    "# saves the papers list to a text file\n",
    "with open('/Users/nialdaly/Documents/ml_optimisation/data/paper_paths.txt', 'wb') as fp:\n",
    "    pickle.dump(papers, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
