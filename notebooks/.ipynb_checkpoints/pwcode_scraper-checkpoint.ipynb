{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports the required packages\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.request\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return_headings method\n",
    "\n",
    "\"\"\"\n",
    "def return_headings(url, heading_tag):\n",
    "    \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    # searches for all h4 headings\n",
    "    search_headings = soup.findAll(heading_tag)\n",
    "    \n",
    "    # empty area headings list\n",
    "    headings = []\n",
    "    \n",
    "    # appends each area heading to the area_headings list\n",
    "    for div in search_headings:\n",
    "        headings.append(div.text)\n",
    "\n",
    "    # removes the white space from the headings list\n",
    "    headings = list(map(str.strip, headings))\n",
    "\n",
    "    # list comphrension for lower casing each string in the area_headings list\n",
    "    headings = [x.lower() for x in headings]\n",
    "\n",
    "    # replaces the white space with an dash in the area_headings list\n",
    "    headings = [x.replace(\" \", \"-\") for x in headings]\n",
    "    \n",
    "    # returns the headings array\n",
    "    return headings\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "return_dataset method\n",
    "\n",
    "\"\"\"\n",
    "def return_dataset(url, dataset_tag):\n",
    "    \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    #\n",
    "    search_datasets = soup.findAll('div', attrs = {'class': dataset_tag})\n",
    "    \n",
    "    # empty area headings list\n",
    "    task_datasets = []\n",
    "    \n",
    "    # appends each task dataset to the task_datasets list\n",
    "    for div in search_datasets:\n",
    "        task_datasets.append(div.text)\n",
    "\n",
    "    # removes the white space from the task_datasets list\n",
    "    task_datasets = list(map(str.strip, task_datasets))\n",
    "\n",
    "    # list comphrension for lower casing each string in the task_datasets list\n",
    "    task_datasets = [x.lower() for x in task_datasets]\n",
    "\n",
    "    # replaces the white space with an dash in the task_datasets list\n",
    "    task_datasets = [x.replace(\" \", \"-\") for x in task_datasets]\n",
    "    \n",
    "    # removes the brace in the task_datasets list\n",
    "    task_datasets = [x.replace(\"(\", \"\") for x in task_datasets]\n",
    "    task_datasets = [x.replace(\")\", \"\") for x in task_datasets]\n",
    "    \n",
    "    # returns the task_datasets list\n",
    "    return task_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer-vision', 'natural-language-processing', 'medical', 'methodology', 'miscellaneous', 'speech', 'playing-games', 'graphs', 'time-series', 'audio', 'robots', 'music', 'computer-code', 'reasoning', 'knowledge-base', 'adversarial']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "url = 'https://paperswithcode.com/sota'\n",
    "heading_tag = 'h4'\n",
    "\n",
    "# invokes the return_headings function to return each of the area headings\n",
    "area_headings = return_headings(url, heading_tag)\n",
    "\n",
    "print(area_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialises an empty task headings list\n",
    "task_headings = []\n",
    "    \n",
    "# iterates through each of the area headings\n",
    "for i in range(len(area_headings)):\n",
    "    \n",
    "    url = 'https://paperswithcode.com/area/' + area_headings[i]\n",
    "    \n",
    "    # invokes the return_headings function to return \n",
    "    # and append each of the task headings to the task_headings array\n",
    "    task_headings.append(return_headings(url, heading_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the resulting 2d array into a 1d array using list comprehension\n",
    "task_headings = [s for S in task_headings for s in S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "# print(len(task_headings))\n",
    "print(task_headings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialises the Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# iterates through task in the task_headings list\n",
    "for i in range(len(task_headings)):\n",
    "\n",
    "    # directs the driver to the paperswithcode webpage\n",
    "    driver.get(\"https://paperswithcode.com/task/\"+ task_headings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://paperswithcode.com/area/computer-vision/object-detection\n"
     ]
    }
   ],
   "source": [
    "# sota\n",
    "# areas - computer vision\n",
    "# tasks - \n",
    "#\n",
    "d = \"https://paperswithcode.com/area/\"+ area_headings[0] + \"/\" + task_headings[2]\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "url = 'https://paperswithcode.com/task/\"+ task_headings[0]'\n",
    "\n",
    "dataset_tag = 'table-striped'\n",
    "# for row in soup.find_all('div',attrs={\"class\" : \"reviewText\"}):\n",
    "#     print row.text\n",
    "# dataset black-links\n",
    "\n",
    "# invokes the return_dataset function to return each of the area headings\n",
    "task_datasets = return_dataset(url, dataset_tag)\n",
    "\n",
    "print(task_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cifar-10', 'mnist', 'cifar-100', 'imagenet', 'svhn', 'stl-10', 'msrc-21-per-class', 'msrc-21-per-pixel', 'cinic-10', 'inaturalist', 'multimnist', 'fashion-mnist']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "url = 'https://paperswithcode.com/task/' + task_headings[1]\n",
    "dataset_tag = 'dataset black-links'\n",
    "\n",
    "# invokes the return_dataset function\n",
    "print(return_dataset(url, dataset_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
