{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_paper_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8R1ul2twl6ct","colab_type":"code","outputId":"f6fd4baf-9da0-4fd1-d4d5-ad7237defba9","executionInfo":{"status":"ok","timestamp":1557504788623,"user_tz":-60,"elapsed":2862,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from fastai.collab import *\n","from fastai.tabular import *\n","from google.colab import drive\n","from google.colab import files\n","import sys\n","\n","#\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","root_dir = \"/content/gdrive/My Drive/ml_projects/ml_optimisation/bert_nlp\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sN0WrSrgLcM6","colab_type":"code","colab":{}},"source":["# installs the necessary modules\n","!pip install pytorch-pretrained-bert\n","\n","!pip install spacy ftfy==4.4.3\n","!python -m spacy download en"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tk5hG5Y3TYmJ","colab_type":"code","colab":{}},"source":["# !pip uninstall apex\n","# !rm -rf apex\n","\n","# #\n","# !git clone https://github.com/NVIDIA/apex.git\n","# %cd apex\n","# !python setup.py install --cuda_ext --cpp_ext"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fRWGGVfgI2g","colab_type":"code","colab":{}},"source":["#\n","!git clone https://github.com/NVIDIA/apex.git\n","%cd apex\n","!python setup.py install --cuda_ext --cpp_ext\n","\n","#\n","!pip install -e \"/content/apex\" --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1nYTfMpLYJ9","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","# import argparse\n","# import parser\n","import collections\n","import json\n","import logging\n","import math\n","import os\n","import random\n","import sys\n","from io import open\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n","from pytorch_pretrained_bert.modeling import BertForQuestionAnswering, BertConfig\n","from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n","from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n","                                                  BertTokenizer,\n","                                                  whitespace_tokenize)\n","\n","import apex\n","import pickle\n","\n","logger = logging.getLogger(__name__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HxoZIiHoGnlx","colab_type":"code","colab":{}},"source":["# enables inline plotting\n","%reload_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n96fC2Q-JN3X","colab_type":"code","colab":{}},"source":["#\n","sys.path.append('/content/gdrive/My Drive/ml_projects/ml_optimisation/bert_nlp/squad_support')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mFbnm56JuwG","colab_type":"code","colab":{}},"source":["# imports the squad_helper helper functions\n","import squad_helper\n","from squad_helper import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEf2sfeXUT9P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5f11088d-3340-4581-8eec-bc83e20dffe8","executionInfo":{"status":"ok","timestamp":1557504963650,"user_tz":-60,"elapsed":519,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["# defines the squad_dir directory\n","squad_dir = Path(root_dir + '/squad_dir/')\n","squad_dir"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/content/gdrive/My Drive/ml_projects/ml_optimisation/bert_nlp/squad_dir')"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"OzKE88uYUlVl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c7f91953-f77a-41b6-c9eb-7b635518b931","executionInfo":{"status":"ok","timestamp":1557504964427,"user_tz":-60,"elapsed":242,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["# defines the output_dir directory\n","output_dir = Path(root_dir + '/output_dir/')\n","output_dir"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/content/gdrive/My Drive/ml_projects/ml_optimisation/bert_nlp/output_dir')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"vqo65-b9TKnU","colab_type":"text"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"tsGh2rNhTKCi","colab_type":"code","colab":{}},"source":["server_ip = ''\n","\n","server_port = ''\n","\n","brt_model = 'bert-base-uncased'\n","\n","do_train = False\n","\n","do_predict = True\n","\n","lower_case = True\n","\n","train_file = squad_dir/'train-v1.1.json'\n","predict_file = squad_dir/'dev-v1.1.json'\n","\n","train_batch_size = 12\n","\n","learning_rate = 3e-5\n","\n","num_train_epochs = 2.0\n","\n","max_seq_length = 384\n","\n","doc_stride = 128\n","\n","local_rank = -1\n","\n","seed = 42\n","\n","gradient_accumulation_steps = 1\n","\n","output_dir = output_dir\n","\n","fp16 = True\n","\n","no_cuda = False\n","\n","loss_scale = 0\n","\n","warmup_proportion = 0.1\n","\n","# max_query_length = 64\n","max_query_length = 64\n","\n","version_2_with_negative = True\n","\n","predict_batch_size = 8\n","\n","# default\n","n_best_size = 20\n","\n","# default\n","do_lower_case = True\n","\n","# threshold\n","null_score_diff_threshold = 0\n","max_answer_length = 30 \n","\n","verbose_logging = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJdbdzomVD_q","colab_type":"text"},"source":["##"]},{"cell_type":"code","metadata":{"id":"g3-4lFkmSOOZ","colab_type":"code","colab":{}},"source":["if server_ip and server_port:\n","\n","  # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","  \n","  import ptvsd\n","  \n","  print(\"Waiting for debugger attach\")\n","  \n","  ptvsd.enable_attach(address=(server_ip, server_port), redirect_output=True)\n","  \n","  ptvsd.wait_for_attach()\n","\n","    \n","    \n","if local_rank == -1 or no_cuda:\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n","  \n","  n_gpu = torch.cuda.device_count()\n","  \n","else:\n","\n","  torch.cuda.set_device(local_rank)\n","  \n","  device = torch.device(\"cuda\", local_rank)\n","  \n","  n_gpu = 1\n","  \n","  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","  \n","  torch.distributed.init_process_group(backend='nccl')\n","\n","  \n","  logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                        datefmt = '%m/%d/%Y %H:%M:%S',\n","                        level = logging.INFO if local_rank in [-1, 0] else logging.WARN)\n","\n","    \n","    \n","  logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n","  \n","      device, n_gpu, bool(local_rank != -1), fp16))\n","\n","    \n","    \n","  if gradient_accumulation_steps < 1:\n","    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(gradient_accumulation_steps))\n","\n","    \n","  train_batch_size = train_batch_size // gradient_accumulation_steps\n","\n","    \n","  random.seed(seed)\n","  \n","  np.random.seed(seed)\n","    \n","  torch.manual_seed(seed)\n","    \n","    \n","  if n_gpu > 0:\n","        torch.cuda.manual_seed_all(seed)\n","\n","    \n","  if not do_train and not do_predict:\n","        raise ValueError(\"At least one of `do_train` or `do_predict` must be True.\")\n","\n","  if do_train:\n","    if not train_file:\n","      raise ValueError(\n","      \n","          \"If `do_train` is True, then `train_file` must be specified.\")\n","    \n","  if do_predict:  \n","      if not predict_file:\n","            raise ValueError(\n","                \"If `do_predict` is True, then `predict_file` must be specified.\")\n","\n","  if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n","        raise ValueError(\"Output directory () already exists and is not empty.\")\n","  \n","  if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9Aw175PVNf4","colab_type":"text"},"source":["##"]},{"cell_type":"code","metadata":{"id":"Gnpbp6xXMl61","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c33ae759-7912-406e-9a43-e8bc78b9098d","executionInfo":{"status":"ok","timestamp":1557504970523,"user_tz":-60,"elapsed":1081,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["#\n","tokenizer = BertTokenizer.from_pretrained(brt_model, do_lower_case = lower_case)\n","\n","train_examples = None\n","\n","num_train_optimization_steps = None\n","\n","#\n","if do_train:\n","  train_examples = read_squad_examples(\n","      \n","      input_file = train_file, \n","      is_training = True,\n","      version_2_with_negative = False\n","  )\n","        \n","  # \n","  num_train_optimization_steps = int(\n","            len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n","        \n","  if local_rank != -1:\n","    num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 5819757.69B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MafOV4FfYOtS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"600414c0-4447-4f10-dcc9-ab248879851f","executionInfo":{"status":"ok","timestamp":1557504973230,"user_tz":-60,"elapsed":416,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE),\n","                                                                 'distributed_{}'.format(\n","                                                                            local_rank)\n","                                                                        )\n","                                                 \n","cache_dir"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/root/.pytorch_pretrained_bert/distributed_-1'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"PmH29IAHVcMt","colab_type":"text"},"source":["## Model preparation"]},{"cell_type":"code","metadata":{"id":"LMiiUYGqVO5d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e27362ca-2279-423d-9d78-f5ce2642eff5","executionInfo":{"status":"ok","timestamp":1557505000496,"user_tz":-60,"elapsed":25126,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["#\n","model = BertForQuestionAnswering.from_pretrained(brt_model)\n","# model = BertForQuestionAnswering.from_pretrained(model,\n","#                                                  cache_dir\n","#                                                  )\n","\n","#\n","if fp16:\n","    model.half()\n","    model.to(device)\n","\n","    #\n","    if local_rank != -1:\n","\n","        try:\n","            from apex.parallel import DistributedDataParallel as DDP\n","\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","        model = DDP(model)\n","\n","    elif n_gpu > 1:\n","\n","        model = torch.nn.DataParallel(model)\n","\n","    # Prepare optimizer\n","    if do_train:\n","\n","        param_optimizer = list(model.named_parameters())\n","\n","        # hack to remove pooler, which is not used\n","        # thus it produce None grad that break apex\n","        param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n","\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        \n","        #\n","        optimizer_grouped_parameters = [\n","               {'params': [p for n, p in param_optimizer if not any(\n","                    nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","                {'params': [p for n, p in param_optimizer if any(\n","                    nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","               ]\n","\n","        if fp16:    \n","            try:\n","                \n","                from apex.optimizers import FP16_Optimizer\n","                \n","                from apex.optimizers import FusedAdam\n","                \n","                \n","            except ImportError:\n","              \n","              raise ImportError(\n","                  \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","                \n","            optimizer = FusedAdam(optimizer_grouped_parameters,\n","                                      lr = learning_rate,\n","                                      bias_correction = False,\n","                                      max_grad_norm = 1.0\n","                                 )\n","                \n","              \n","            if loss_scale == 0:\n","                \n","                #\n","                optimizer = FP16_Optimizer(\n","                        optimizer, dynamic_loss_scale = True)\n","                \n","            else:    \n","                optimizer = FP16_Optimizer(\n","                    optimizer,\n","                    static_loss_scale = loss_scale)\n","                \n","            warmup_linear = WarmupLinearSchedule(warmup = warmup_proportion,\n","                                                     t_total = num_train_optimization_steps)\n","            \n","        else:  \n","          optimizer = BertAdam(optimizer_grouped_parameters,\n","                                     lr = learning_rate,\n","                                     warmup = warmup_proportion,\n","                                     t_total = num_train_optimization_steps\n","                                    )\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100%|██████████| 407873900/407873900 [00:06<00:00, 65070243.47B/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lr1jXnieVjlj","colab_type":"text"},"source":["##"]},{"cell_type":"code","metadata":{"id":"eMcpxq9EVO2K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"de27f380-ac78-44c4-adba-5c9dd9eaedbb","executionInfo":{"status":"ok","timestamp":1557505006687,"user_tz":-60,"elapsed":557,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["cached_train_features_file = str(train_file) + '_{0}_{1}_{2}_{3}'.format(\n","        \n","        list(filter(None, brt_model.split('/'))).pop(), str(max_seq_length), str(doc_stride), str(max_query_length))\n","\n","cached_train_features_file"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/My Drive/ml_projects/ml_optimisation/bert_nlp/squad_dir/train-v1.1.json_bert-base-uncased_384_128_64'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"e_uXnh53Vun3","colab_type":"text"},"source":["##"]},{"cell_type":"code","metadata":{"id":"oqc5vHCQVOy5","colab_type":"code","colab":{}},"source":["global_step = 0\n","\n","if do_train:\n","  \n","    cached_train_features_file = str(train_file) + '_{0}_{1}_{2}_{3}'.format(\n","    \n","#     cached_train_features_file = train_file + '_{0}_{1}_{2}_{3}'.format(\n","        \n","        list(filter(None, brt_model.split('/'))).pop(), str(max_seq_length), str(doc_stride), str(max_query_length))\n","    \n","     \n","    train_features = None\n","        \n","    try:\n","    \n","      with open(cached_train_features_file, \"rb\") as reader:\n","                train_features = pickle.load(reader)\n","            \n","    except:\n","            \n","        train_features = convert_examples_to_features(\n","                \n","            examples = train_examples,\n","                    \n","            tokenizer = tokenizer,\n","                    \n","            max_seq_length = max_seq_length,\n","                    \n","            doc_stride = doc_stride,\n","                \n","            max_query_length = max_query_length,\n","                    \n","            is_training = True\n","            \n","        )\n","            \n","            \n","        if local_rank == -1 or torch.distributed.get_rank() == 0:\n","        \n","          logger.info(\"  Saving train features into cached file %s\", cached_train_features_file)\n","                \n","          with open(cached_train_features_file, \"wb\") as writer:\n","                  \n","              pickle.dump(train_features, writer)\n","              \n","        \n","    logger.info(\"***** Running training *****\")\n","    \n","    logger.info(\"  Num orig examples = %d\", len(train_examples))\n","    \n","    logger.info(\"  Num split examples = %d\", len(train_features))\n","    \n","    logger.info(\"  Batch size = %d\", train_batch_size)\n","    \n","    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n","    \n","    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","    \n","    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","    \n","    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","    \n","    all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n","    \n","    all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n","    \n","    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n","                                   all_start_positions, all_end_positions)\n","    \n","    if local_rank == -1:\n","            \n","        train_sampler = RandomSampler(train_data)\n","        \n","    else:\n","        \n","        train_sampler = DistributedSampler(train_data)\n","        \n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = train_batch_size)\n","    \n","    \n","    #\n","    model.train()\n","    \n","    for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n","        \n","    \n","      for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\", disable = local_rank not in [-1, 0])):\n","            \n","                if n_gpu == 1:\n","                    batch = tuple(t.to(device) for t in batch) # multi-gpu does scattering it-self\n","                  \n","                input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n","                \n","                loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n","                \n","                if n_gpu > 1:\n","                  \n","                    loss = loss.mean() # mean() to average on multi-gpu.\n","                    \n","                if gradient_accumulation_steps > 1:\n","                    loss = loss / gradient_accumulation_steps\n","\n","                if fp16:\n","                    optimizer.backward(loss)\n","                    \n","                else:\n","                    loss.backward()\n","                    \n","                if (step + 1) % gradient_accumulation_steps == 0:\n","                    if fp16:\n","                        # modify learning rate with special warm up BERT uses\n","                        # if args.fp16 is False, BertAdam is used and handles this automatically\n","                        lr_this_step = learning_rate * warmup_linear.get_lr(global_step / num_train_optimization_steps,\n","                                                                                 warmup_proportion)\n","                        \n","                        for param_group in optimizer.param_groups:\n","                          \n","                            param_group['lr'] = lr_this_step\n","                            \n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    global_step += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlVHOc2_VxHG","colab_type":"text"},"source":["##"]},{"cell_type":"code","metadata":{"id":"Bf5c4qP2VylD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bdcfd674-beb2-4d5c-8214-f00d890337d9","executionInfo":{"status":"ok","timestamp":1557505463473,"user_tz":-60,"elapsed":306410,"user":{"displayName":"Nial Daly","photoUrl":"","userId":"17909903908714183967"}}},"source":["if do_train and (local_rank == -1 or torch.distributed.get_rank() == 0):\n","        \n","    # Save a trained model, configuration and tokenizer\n","    \n","    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","\n","    \n","    # If we save using the predefined names, we can load using `from_pretrained`\n","    \n","    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n","    \n","    output_config_file = os.path.join(output_dir, CONFIG_NAME)\n","\n","    \n","    torch.save(model_to_save.state_dict(), output_model_file)\n","    \n","    model_to_save.config.to_json_file(output_config_file)\n","    \n","    tokenizer.save_vocabulary(output_dir)\n","\n","    \n","    # Load a trained model and vocabulary that you have fine-tuned\n","    \n","    model = BertForQuestionAnswering.from_pretrained(output_dir)\n","    \n","    tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case = do_lower_case)\n","    \n","else:\n","        \n","    model = BertForQuestionAnswering.from_pretrained(brt_model)\n","\n","    model.to(device)\n","\n","    if do_predict and (local_rank == -1 or torch.distributed.get_rank() == 0):\n","      \n","        eval_examples = read_squad_examples(\n","            \n","            input_file = predict_file, is_training = False, version_2_with_negative = version_2_with_negative)\n","        \n","        eval_features = convert_examples_to_features(\n","            examples=eval_examples,\n","            tokenizer=tokenizer,\n","            max_seq_length = max_seq_length,\n","            doc_stride = doc_stride,\n","            max_query_length = max_query_length,\n","            is_training=False)\n","\n","        logger.info(\"***** Running predictions *****\")\n","        \n","        logger.info(\"  Num orig examples = %d\", len(eval_examples))\n","        \n","        logger.info(\"  Num split examples = %d\", len(eval_features))\n","        \n","        logger.info(\"  Batch size = %d\", predict_batch_size)\n","        \n","\n","        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n","        \n","        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n","        \n","        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n","        \n","        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n","        \n","        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n","        \n","        # Run prediction for full data\n","        eval_sampler = SequentialSampler(eval_data)\n","        \n","        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size = predict_batch_size)\n","\n","        model.eval()\n","        \n","        all_results = []\n","        \n","        logger.info(\"Start evaluating\")\n","        \n","        for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\", disable = local_rank not in [-1, 0]):\n","            \n","            if len(all_results) % 1000 == 0:\n","            \n","              logger.info(\"Processing example: %d\" % (len(all_results)))\n","            \n","            input_ids = input_ids.to(device)\n","            \n","            input_mask = input_mask.to(device)\n","            \n","            segment_ids = segment_ids.to(device)\n","            \n","            with torch.no_grad():\n","              \n","                batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n","            \n","            for i, example_index in enumerate(example_indices):\n","                start_logits = batch_start_logits[i].detach().cpu().tolist()\n","                \n","                end_logits = batch_end_logits[i].detach().cpu().tolist()\n","                \n","                eval_feature = eval_features[example_index.item()]\n","                \n","                unique_id = int(eval_feature.unique_id)\n","                \n","                all_results.append(RawResult(unique_id = unique_id,\n","                                             start_logits = start_logits,\n","                                             end_logits = end_logits))\n","        \n","        output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n","        \n","        output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n","        \n","        output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n","        \n","        write_predictions(eval_examples, eval_features, all_results,\n","                          \n","                          n_best_size, max_answer_length,\n","                          \n","                          do_lower_case, output_prediction_file,\n","                          \n","                          output_nbest_file, output_null_log_odds_file, verbose_logging,\n","                          \n","                          \n","                          version_2_with_negative, null_score_diff_threshold\n","                         )"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Evaluating: 100%|██████████| 1355/1355 [05:11<00:00,  4.35it/s]\n"],"name":"stderr"}]}]}